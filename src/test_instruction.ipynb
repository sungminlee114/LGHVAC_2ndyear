{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.db.manager import DBManager\n",
    "from src.input_to_instructions.load_and_execute import *\n",
    "from src.input_to_instructions.types import *\n",
    "from src.plot_graph.execute import *\n",
    "from src.operation.execute import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:db.instance:Connected to the database PerSite_DB\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\"\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from src.plot_graph.execute import plot_graph_plotly\n",
    "matplotlib.rcParams['font.family'] = 'NanumGothicCoding'\n",
    "def run_query(user_input, metadata, instructions):\n",
    "    variables = {\n",
    "        \"Metadata\": metadata,\n",
    "    }\n",
    "    for instruction in instructions:\n",
    "        # logger.debug(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        # print(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        \n",
    "        if type(instruction) == InstructionQ:\n",
    "            # Execute query\n",
    "            result_df = DBManager.structured_query_data_t(metadata, instruction.args)\n",
    "            if result_df is None:\n",
    "                print(\"죄송합니다, 관련 데이터를 찾을 수 없습니다.\", \"response\")\n",
    "                return\n",
    "\n",
    "            # For demo, drop rows where any value is -1\n",
    "            result_df = result_df.loc[(result_df != -1).all(axis=1)]\n",
    "           \n",
    "            #pd.set_option('display.max_rows', 10000)        \n",
    "            #pd.set_option('display.max_columns', 1000)\n",
    "            #pd.set_option('display.width', 1000)\n",
    "            #pd.set_option('display.max_colwidth', 1000)\n",
    "            #print(f\"QueryResult: {result_df}\")\n",
    "\n",
    "            variables[instruction.result_name] = result_df\n",
    "        \n",
    "        elif type(instruction) == InstructionO:\n",
    "            # Execute operation\n",
    "\n",
    "            result_dict = OperationExecutor.execute(variables, instruction.scripts, instruction.returns)\n",
    "            variables.update(result_dict)\n",
    "            pass\n",
    "        elif type(instruction) == InstructionG:\n",
    "\n",
    "            # fig = plot_graph(instruction, variables)\n",
    "            # plt.show(fig)\n",
    "            \n",
    "            fig = plot_graph_plotly(instruction, variables)\n",
    "            #plt.show(fig)\n",
    "            fig.show()\n",
    "            #print(type(fig))\n",
    "            # print(fig, \"graph\")\n",
    "        elif type(instruction) == InstructionR:\n",
    "            pass\n",
    "            # # Execute response generation\n",
    "            # variables_to_report = {k: v for k, v in variables.items() if k not in [\"Metadata\"]}\n",
    "            # print(f\"Variables: {variables_to_report}\")\n",
    "            # response, required_variables = ResponseGeneration.execute(instruction, variables, user_input, metadata)\n",
    "            # print(f\"Required variables: {required_variables}\")\n",
    "            \n",
    "            # print(response, \"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_groundtruth(dateset_name):\n",
    "    def read(path):\n",
    "        data = read_json(path)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = directory.name\n",
    "            if \"v7\" in dateset_name:\n",
    "                data[i][\"Metadata\"] = metadata\n",
    "        return data\n",
    "\n",
    "    ds_ts = []\n",
    "    ds_tr = []\n",
    "    base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dateset_name}\")\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir() and \"scenario1\" in directory.name:\n",
    "            if \"v7\" in dateset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "                \n",
    "            \n",
    "            # ds_ts.extend(read(f\"{directory}/onlyq_ts.json\"))\n",
    "            #ds_tr.extend(read(f\"{directory}/onlyq_tr.json\"))\n",
    "            ds_tr.extend(read(f\"{directory}/graph_temp.json\"))\n",
    "    \n",
    "    ds = ds_ts + ds_tr\n",
    "    \n",
    "    # if \"v7\" in dateset_name:\n",
    "    #     db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "    # else:\n",
    "    #     db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "    #     metadata = None\n",
    "    \n",
    "    # with open(db_gt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        # f.write(\"[\")\n",
    "    # with tqdm(total=len(ds)) as pbar:\n",
    "    #count=0\n",
    "    for d in ds:\n",
    "        #count += 1\n",
    "        #if count < 10:\n",
    "        #    continue  # 10보다 작은 값은 건너뛰기\n",
    "        #if count > 40:\n",
    "        #    break     # 20을 초과하면 반복 종료\n",
    "        # pbar.set_description(f\"Processing {d['Input']}\")\n",
    "        # print(\"--\")\n",
    "        \n",
    "        instructions = InputToInstruction.postprocess(d['Response'])\n",
    "        user_input, tags, metadata, scenario = d[\"Input\"], d[\"Tags\"], d[\"Metadata\"], d[\"Scenario\"]\n",
    "        \n",
    "        run_query(user_input, metadata, instructions)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "build_query_groundtruth(\"v7-250309-reduceinputanddatefunctioncall\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
