{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "from unsloth import FastLanguageModel, unsloth_train\n",
    "\n",
    "from transformers import TrainerCallback, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# from src.db.manager import DBManager\n",
    "from src.input_to_instructions.types import InstructionQ\n",
    "\n",
    "# 시드 설정\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    transformers.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# 시드값 설정\n",
    "seed = 3407\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 0     # Unsloth auto supports RoPE Scaling internally!\n",
    "# dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "device = f\"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'sh2orc/Llama-3.1-Korean-8B-Instruct'\n",
    "# model_id = 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = 'Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B'\n",
    "# model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "# model_id = 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct'\n",
    "\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download(repo_id=model_id, local_dir=\"70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizer initialization\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     cache_dir=f\"{model_dir}/cache\",\n",
    "#     # attn_implementation=attn_implementation,\n",
    "#     local_files_only=True,\n",
    "#     device_map=\"cuda\"\n",
    "# )\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\n",
    "# #     model_id,\n",
    "# #     cache_dir=f\"{model_dir}/cache\",\n",
    "# #     local_files_only=True\n",
    "# # )\n",
    "# # if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "# pretrained_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed8e11144f34f43acbeb3158a8f803a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh2orc/Llama-3.1-Korean-8B-Instruct does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n",
      "Pad Token id: 128004 and Pad Token: <|finetune_right_pad_id|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer initialization\n",
    "\n",
    "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = torch_dtype,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     # load_in_8bit=True,\n",
    "    #     # llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    # ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # trust_remote_code=True\n",
    "    # local_files_only=True\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "\n",
    "# if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "#     pretrained_model.save_pretrained(model_dir)\n",
    "#     tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_type: 0SL\n"
     ]
    }
   ],
   "source": [
    "train_type = [\n",
    "    \"BASE\", # 0\n",
    "\n",
    "    \"5SL\", # 1\n",
    "    \"Finetuning\", # 2\n",
    "\n",
    "    \"rawSQL+LM\", # 3\n",
    "    \"QM+script\", # 4\n",
    "\n",
    "    \"NoExp\", # 5\n",
    "    \"ours\" # 6\n",
    "][-1]\n",
    "print(f\"train_type: {train_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DATASET_DIR: ../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall\n",
      "[PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/prompt_v2.txt'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/refine.ipynb'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/summary.ipynb'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/prompt.txt')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# BASE_DATASET_DIR = Path(\"../dataset/v5-250228-multimetadata\")\n",
    "# dataset_name = \"v6-250306-optimizetoken\"\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "BASE_DATASET_DIR = Path(f\"../finetuning/dataset/{dataset_name}\")\n",
    "print(f\"BASE_DATASET_DIR: {BASE_DATASET_DIR}\")\n",
    "print(list(BASE_DATASET_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너는 유저의 HVAC 관련 질문에 답변하는 Agent의 계획을 설계하는 정확하고 훌룡한 인공지능이다. \n",
      "사용자의 질문(Input)을 받아 Agent의 Instructions를 출력해 주어야 한다.\n",
      "Thinking에서는 Metadata를 바탕으로 Input을 low-level term으로 수정하고, 유저의 의도를 추측하며, 답변 계획을 세워야함.\n",
      "Expectation에서는 유저가 기대할만한 답변을 추측해야함.\n",
      "\n",
      "너는 type=q의 instruction으로 쿼리를, type=o로 python 기반 data operation을 계획할 수 있다.\n",
      "type=q에서는 필요한 모든 data들을 temporal, spatial, modality(columns) 필터를 거쳐 조회해야한다.\n",
      "시간과 관련된 필터링은 type=o가 아닌 type=q에서 수행한다.\n",
      "\n",
      "expectation의 명시된 모든 variable이 type=o에서 계산되야 한다.\n",
      "\n",
      "json형식으로 출력하며, eval() 함수를 사용할 수 있도록 괄호들과 따옴표들의 순서와 닫힘을 매우 신경써서 출력해야한다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# current_metadata = json.load(open(\"metadata.json\", \"r\"))\n",
    "\n",
    "\n",
    "if train_type not in [\"ours\", \"woall\"]:\n",
    "    common_prompt = open(BASE_DATASET_DIR / \"prompt_v2.txt\", \"r\").read()\n",
    "    common_prompt = re.sub(fr\"\\n?<\\|{train_type}\\|>(.|\\n)*?<\\|{train_type}\\|>\", \"\", common_prompt)\n",
    "else:\n",
    "    common_prompt = open(BASE_DATASET_DIR / f\"prompt.txt\", \"r\").read()\n",
    "\n",
    "    if train_type in [\"woall\"]:\n",
    "        # search <|FI|>~~<|FI|> and remove between them\n",
    "        common_prompt = re.sub(r\"\\n?<\\|Ours\\|>(.|\\n)*?<\\|Ours\\|>\", \"\", common_prompt)\n",
    "\n",
    "\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "print(common_prompt)\n",
    "\n",
    "# print(common_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1')]\n"
     ]
    }
   ],
   "source": [
    "scenario_dirs = [d for d in BASE_DATASET_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]\n",
    "print(scenario_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '어제 우리반과 옆반의 설정온도 차이 알려줘', 'Response': '{\"Thinking\": \"어제 우리반과 옆반의 설정온도를 쿼리한 후 각 반의 평균 설정온도 및 그 차이를 계산하여 반환하면 됨.\", \"Expectations\": [\"어제 우리반의 설정온도({{어제_우리반_설정온도_평균}}℃)는 옆반의 설정온도({{어제_옆반_설정온도_평균}}℃)보다 {{어제_설정온도_평균의_차이}}℃ 높습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"어제\": \"[DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'1 day\\'), DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"옆반\": \"01_IB7\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_어제_우리반_설정온도_df = data(t=\\'어제\\',s=\\'우리반\\',m=\\'설정온도\\')\", \"v_어제_옆반_설정온도_df = data(t=\\'어제\\',s=\\'옆반\\',m=\\'설정온도\\')\", \"v_어제_우리반_설정온도_평균 = v_어제_우리반_설정온도_df[\\'설정온도\\'].mean()\", \"v_어제_옆반_설정온도_평균 = v_어제_옆반_설정온도_df[\\'설정온도\\'].mean()\", \"v_어제_설정온도_평균의_차이 = v_어제_우리반_설정온도_평균 - v_어제_옆반_설정온도_평균\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '오늘 우리반과 옆반의 평균 온도차이 알려줘', 'Response': '{\"Thinking\": \"사용자는 오늘 하루 평균 우리반과 옆반의 실내온도의 차이를 알고 싶어함. 사용자는 차이 결과 뿐만 아니라 계산에 사용된 각 반의 평균 실내온도 값도 궁금할 것임. 오늘 하루 우리반과 옆반의 실내온도를 쿼리한 후 각 반의 평균 실내온도 및 그 차이를 계산하여 반환하면 됨.\", \"Expectations\": [\"우리반({{오늘_우리반_실내온도_평균}}℃)이 옆반({{오늘_옆반_실내온도_평균}}℃)보다 {{오늘_실내온도_평균의_차이}}℃ 높습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"오늘\": \"[DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\'), DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' + INTERVAL \\'1 day\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"옆반\": \"01_IB7\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_오늘_우리반_실내온도_df = data(t=\\'오늘\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_오늘_옆반_실내온도_df = data(t=\\'오늘\\',s=\\'옆반\\',m=\\'실내온도\\')\", \"v_오늘_우리반_실내온도_평균 = v_오늘_우리반_실내온도_df[\\'실내온도\\'].mean()\", \"v_오늘_옆반_실내온도_평균 = v_오늘_옆반_실내온도_df[\\'실내온도\\'].mean()\", \"v_오늘_실내온도_평균의_차이 = v_오늘_우리반_실내온도_평균 - v_오늘_옆반_실내온도_평균\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '작년 겨울 우리반 평균온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 우리반의 1년 전 12월 ~ 올해 2월의 평균 실내온도를 알고 싶어함. 우리반의 1년 전 겨울(12월 ~ 2월) 실내온도를 쿼리한 후 평균값을 계산하여 반환하면 됨.\", \"Expectations\": [\"작년 겨울(2021-12 ~ 2022-02) 우리반의 평균 온도는 {{작년겨울_우리반_실내온도_평균}}℃ 입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"작년겨울\": \"[DATE \\'CURRENT_YEAR-12-01\\' - INTERVAL \\'1 year\\', DATE \\'CURRENT_YEAR-03-01\\')\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_작년겨울_우리반_실내온도_df = data(t=\\'작년겨울\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_작년겨울_우리반_실내온도_평균 = v_작년겨울_우리반_실내온도_df[\\'실내온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '올해 여름 앞반 평균온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 올해 6월 ~ 8월 앞반의 평균 실내온도를 알고 싶어함. 올해 여름(6월 ~ 8월) 앞반의 실내온도를 쿼리한 후 평균값을 계산하여 반환하면 됨.\", \"Expectations\": [\"올해 여름(6월 ~ 8월) 앞반의 평균 온도는 {{올해여름_앞반_실내온도_평균}}℃ 입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"올해여름\": \"[DATE \\'CURRENT_YEAR-06-01\\', DATE \\'CURRENT_YEAR-09-01\\')\"}, \"spatials\": {\"앞반\": \"02_I81\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_올해여름_앞반_실내온도_df = data(t=\\'올해여름\\',s=\\'앞반\\',m=\\'실내온도\\')\", \"v_올해여름_앞반_실내온도_평균 = v_올해여름_앞반_실내온도_df[\\'실내온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '올해 봄 옆반 제일 추웠던 날 알려줘', 'Response': '{\"Thinking\": \"사용자는 올해 3월 ~ 5월 옆반의 실내온도가 제일 낮았던 날짜를 알고 싶어함. 올해 3월 ~ 5월 옆반의 실내온도를 쿼리한 후 실내온도가 가장 낮은 날을 찾아 그 날짜와 온도를 반환하면 됨.\", \"Expectations\": [\"올해 봄(3월 ~ 5월) 옆반은 {{올해봄_옆반_실내온도_최소_날짜}}에 {{올해봄_옆반_실내온도_최소}}℃로 제일 추웠습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"올해봄\": \"[DATE \\'CURRENT_YEAR-03-01\\', DATE \\'CURRENT_YEAR-06-01\\')\"}, \"spatials\": {\"옆반\": \"01_IB7\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_올해봄_옆반_실내온도_df = data(t=\\'올해봄\\',s=\\'옆반\\',m=\\'실내온도\\')\", \"v_올해봄_옆반_실내온도_최소 = v_올해봄_옆반_실내온도_df[\\'실내온도\\'].min()\", \"v_올해봄_옆반_실내온도_최소_df = v_올해봄_옆반_실내온도_df[v_올해봄_옆반_실내온도_df[\\'실내온도\\'] == v_올해봄_옆반_실내온도_최소]\", \"v_올해봄_옆반_실내온도_최소_날짜 = get_time(v_올해봄_옆반_실내온도_최소_df, fmt=\\'date\\')\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '4월 앞반 평균온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 4월 한달간 앞반의 평균 실내온도를 알고 싶어함. 4월 한달간 앞반의 실내온도를 쿼리한 후 평균값을 계산하여 반환하면 됨.\", \"Expectations\": [\"4월 앞반의 평균 온도는 {{4월_앞반_평균_실내온도}}℃ 입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"4월\": \"[DATE \\'CURRENT_YEAR-04-01\\', DATE \\'CURRENT_YEAR-05-01\\')\"}, \"spatials\": {\"앞반\": \"02_I81\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_4월_앞반_실내온도_df = data(t=\\'4월\\',s=\\'앞반\\',m=\\'실내온도\\')\", \"v_4월_앞반_평균_실내온도 = v_4월_앞반_실내온도_df[\\'실내온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '이번달 중 우리반 온도가 가장 덜 더운날이 언제야?', 'Response': '{\"Thinking\": \"사용자는 이번달 우리반의 일평균 실내온도가 가장 낮았던 날짜를 알고 싶어함. 이번달 우리반의 실내온도를 쿼리한 후 일평균 실내온도가 가장 낮은 날을 찾아 그 날짜와 온도를 반환하면 됨.\", \"Expectations\": [\"이번달 우리반은 {{이번달_우리반_최소실내온도_날짜}}에 {{이번달_우리반_최소실내온도}}℃로 가장 덜 더웠습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"이번달\": \"[DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\'), DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\' + INTERVAL \\'1 month\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_이번달_우리반_실내온도_df = data(t=\\'이번달\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_이번달_우리반_실내온도_최소 = v_이번달_우리반_실내온도_df[\\'실내온도\\'].min()\", \"v_이번달_우리반_실내온도_최소_날짜 = get_time(v_이번달_우리반_실내온도_df[v_이번달_우리반_실내온도_df[\\'실내온도\\'] == v_이번달_우리반_실내온도_최소], fmt=\\'date\\')\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '2주전 우리반과 옆반 합쳐서 설정온도가 가장 낮은날이 언제야?', 'Response': '{\"Thinking\": \"2주전은 정확히 말하면 2주전 한주 (월~일)동안임. 사용자는 2주전 월요일부터 일요일까지 우리반과 옆반의 설정온도가 가장 낮았던 날짜를 알고 싶어함. 2주전 우리반과 옆반의 설정온도를 쿼리한 후 설정온도가 가장 낮은 날을 찾아 그 날짜와 온도를 반환하면 됨.\", \"Expectations\": [\"2주전 우리반과 옆반 합쳐서 설정온도가 가장 낮은 날은 {{2주전_우리반과옆반_설정온도_최소_날짜}}로 {{2주전_우리반과옆반_설정온도_최소}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"2주전\": \"[DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'2 week\\'), DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'1 week\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"옆반\": \"01_IB7\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_2주전_우리반과옆반_설정온도_df = data(t=\\'2주전\\',s=[\\'우리반\\',\\'옆반\\'],m=\\'설정온도\\')\", \"v_2주전_우리반과옆반_설정온도_최소 = v_2주전_우리반과옆반_설정온도_df[\\'설정온도\\'].min()\", \"v_2주전_우리반과옆반_설정온도_최소_날짜 = get_time(v_2주전_우리반과옆반_설정온도_df[v_2주전_우리반과옆반_설정온도_df[\\'설정온도\\'] == v_2주전_우리반과옆반_설정온도_최소], fmt=\\'date\\')\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '이번달 중 뒷반 온도가 가장 더운날이 언제야?', 'Response': '{\"Thinking\": \"사용자는 이번달 뒷반의 실내온도가 가장 더운날이 언제인지 물어봄. 이번달 뒷반의 실내온도를 쿼리한 후 실내온도가 가장 높은 날을 찾아 그 날짜와 온도를 반환하면 됨.\", \"Expectations\": [\"이번달 뒷반은 {{이번달_뒷반_최대실내온도_날짜}}에 {{이번달_뒷반_최대실내온도}}℃로 가장 더웠습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"이번달\": \"[DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\'), DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\' + INTERVAL \\'1 month\\'))\"}, \"spatials\": {\"뒷반\": \"Unknown\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '우리반의 현재 설정 온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 우리반의 현재 설정온도를 알고 싶어함. 현재 우리반의 설정온도를 쿼리한 후 반환하면 됨.\", \"Expectations\": [\"우리반의 현재 설정온도는 {{현재_우리반_설정온도}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_현재_우리반_설정온도_df = data(t=\\'현재\\',s=\\'우리반\\',m=\\'설정온도\\')\", \"v_현재_우리반_설정온도 = v_현재_우리반_설정온도_df[\\'설정온도\\'].values[0]\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '8일전 설정온도는?', 'Response': '{\"Thinking\": \"공간 정보가 없으므로 우리반으로 추측함. 사용자는 8일 전 우리반의 설정온도를 알고 싶어함. 8일 전 설정온도의 하루 평균값을 반환하면 됨.\", \"Expectations\": [\"8일전 우리반의 평균 설정온도는 {{8일전_우리반_평균_설정온도}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"8일전\": \"[DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'8 day\\'), DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'7 day\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_8일전_우리반_설정온도_df = data(t=\\'8일전\\',s=\\'우리반\\',m=\\'설정온도\\')\", \"v_8일전_우리반_평균_설정온도 = v_8일전_우리반_설정온도_df[\\'설정온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '10년 전 오늘 우리반 온도는?', 'Response': '{\"Thinking\": \"사용자는 10년 전 오늘 우리반의 실내온도를 알고 싶어함. 10년 전 오늘 우리반의 실내온도의 하루 평균값을 반환하면 됨.\", \"Expectations\": [\"10년 전 오늘 우리반의 실내온도는 {{10년전오늘_우리반_실내온도_평균}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"10년전오늘\": \"[DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'10 year\\'), DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'10 year\\' + INTERVAL \\'1 day\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_10년전오늘_우리반_실내온도_df = data(t=\\'10년전오늘\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_10년전오늘_우리반_실내온도_평균 = v_10년전오늘_우리반_실내온도_df[\\'실내온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '롯데캐슬의 현재 온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 롯데캐슬의 현재 실내온도를 알고 싶어함. 롯데캐슬의 현재 실내온도값을 반환하면 됨.\", \"Expectations\": [\"롯데캐슬의 현재 실내온도는 {{현재_롯데캐슬_실내온도}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"롯데캐슬\": \"Unknown\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '1층 평균 실내온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 현재 1층의 공간 평균 실내온도를 알고 싶어함. 현재 1층에 속한 모든 공간의 실내온도의 평균값을 반환하면 됨.\", \"Expectations\": [\"1층의 평균 실내온도는 {{현재_1층_실내온도_평균}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"1층\": \"Unknown\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '우리반, 옆반, 앞반 중 가장 추운 방은?', 'Response': '{\"Thinking\": \"사용자는 현재 우리반, 옆반, 앞반 중 실내온도가 최저인 방을 알고싶어함.현재 세 방의 실내온도를 쿼리한 후 실내온도가 최저인 방과 그 온도를 반환하면 됨.\", \"Expectations\": [\"현재 세 방 중 {{현재_실내온도_최저_방}}이(가) {{현재_실내온도_최저}}℃로 가장 추워요.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"옆반\": \"01_IB7\", \"앞반\": \"02_I81\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_현재_방별_실내온도_df = data(t=\\'현재\\',s=[\\'우리반\\',\\'옆반\\',\\'앞반\\'],m=\\'실내온도\\')\", \"v_현재_실내온도_최저 = v_현재_방별_실내온도_df[\\'실내온도\\'].min()\", \"v_현재_실내온도_최저_df = v_현재_방별_실내온도_df[v_현재_방별_실내온도_df[\\'실내온도\\'] == v_현재_실내온도_최저]\", \"v_현재_실내온도_최저_방 = get_spatials(v_현재_실내온도_최저_df)\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '지금 에너지 사용량 알려줘', 'Response': '{\"Thinking\": \"사용자는 현재 우리반의 에너지 사용량을 알고싶어함. 현재 우리반의 에너지 사용량을 쿼리한 후 반환하면 됨.\", \"Expectations\": [\"현재 우리반의 에너지 사용량은 {{현재_우리반_에너지_사용량}}kWh입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"에너지 사용량\": \"Unknown\"}}}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '지난달 오늘 오후 2시에 옆반의 설정온도는 어땠어?', 'Response': '{\"Thinking\": \"사용자는 지난달 오늘 오후 2시에 옆반의 설정온도를 알고 싶어합니다. 해당 시간대의 설정온도 데이터를 쿼리한 후 반환하면 됩니다.\", \"Expectations\": [\"지난달 오늘 오후 2시 옆반의 설정온도는 {{지난달오늘오후2시_옆반_설정온도}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"지난달오늘오후2시\": \"DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'1 month\\') + INTERVAL \\'14 hours\\'\"}, \"spatials\": {\"옆반\": \"01_IB7\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_지난달오늘오후2시_옆반_설정온도_df = data(t=\\'지난달오늘오후2시\\',s=\\'옆반\\',m=\\'설정온도\\')\", \"v_지난달오늘오후2시_옆반_설정온도 = v_지난달오늘오후2시_옆반_설정온도_df[\\'설정온도\\']\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '옆반의 현재 온도랑 설정온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 옆반의 현재 실내온도와 설정온도를 알고 싶어함. 현재 실내온도와 설정온도를 쿼리한 후 반환하면 됨.\", \"Expectations\": [\"옆반의 현재 실내온도는 {{현재_옆반_실내온도}}℃이고, 설정온도는 {{현재_옆반_설정온도}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"옆반\": \"01_IB7\"}, \"modalities\": {\"실내온도\": \"roomtemp\", \"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_현재_옆반_df = data(t=\\'현재\\',s=\\'옆반\\',m=[\\'실내온도\\', \\'설정온도\\'])\", \"v_현재_옆반_실내온도 = v_현재_옆반_df[\\'실내온도\\'].values[0]\", \"v_현재_옆반_설정온도 = v_현재_옆반_df[\\'설정온도\\'].values[0]\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '이번주 우리반 평균 온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 이번주 우리반의 평균 실내온도를 알고싶어함. 이번주 우리반의 실내온도 데이터를 쿼리한 후 평균값을 계산하여 반환하면 됨.\", \"Expectations\": [\"이번주 우리반의 평균 실내온도는 {{이번주_우리반_실내온도_평균}}℃ 입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"이번주\": \"[DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\'), DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\' + INTERVAL \\'1 week\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_이번주_우리반_실내온도_df = data(t=\\'이번주\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_이번주_우리반_실내온도_평균 = v_이번주_우리반_실내온도_df[\\'실내온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '지난달 설정온도 평균을 알려줘.', 'Response': '{\"Thinking\": \"사용자는 지난달 우리반의 설정온도 평균을 알고싶어함. 해당 기간의 설정온도 데이터를 쿼리한 후 평균값을 반환하면 됨.\", \"Expectations\": [\"지난달 설정온도 평균은 {{지난달_우리반_설정온도_평균}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"지난달\": \"[DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\') - INTERVAL \\'1 month\\', DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_지난달_우리반_설정온도_df = data(t=\\'지난달\\',s=\\'우리반\\',m=\\'설정온도\\')\", \"v_지난달_우리반_설정온도_평균 = v_지난달_우리반_설정온도_df[\\'설정온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '1층 평균 실내온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 현재 1층의 평균 실내온도를 알고싶어함. 현재 1층의 전체 공간의 실내온도 데이터를 쿼리한 후 평균값을 반환하면 됨.\", \"Expectations\": [\"1층의 평균 실내온도는 {{현재_1층_실내온도_평균}}℃ 입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"1층\": \"Unknown\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '우리반 이번달 제일 추웠던 날은 언제냐?', 'Response': '{\"Thinking\": \"사용자는 이번달 우리반의 실내온도값이 최소인 날짜을 알고싶어함. 이번달 우리반의 실내온도 데이터를 쿼리한 후 최소 실내온도를 가진 날과 그 온도를 반환하면 됨.\", \"Expectations\": [\"이번달 우리반은 {{이번달_우리반_실내온도_최소_날짜}}에 실내온도 {{이번달_우리반_실내온도_최소_온도}}℃로 가장 추웠습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"이번달\": \"[DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\'), DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\' + INTERVAL \\'1 month\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_이번달_우리반_실내온도_df = data(t=\\'이번달\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_이번달_우리반_실내온도_최소_온도 = v_이번달_우리반_실내온도_df[\\'실내온도\\'].min()\", \"v_이번달_우리반_실내온도_최소_df = v_이번달_우리반_실내온도_df[v_이번달_우리반_실내온도_df[\\'실내온도\\'] == v_이번달_우리반_실내온도_최소_온도]\", \"v_이번달_우리반_실내온도_최소_날짜 = get_time(v_이번달_우리반_실내온도_최소_df, fmt=\\'date\\')\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '작년 옆반 가장 더웠던 달은?', 'Response': '{\"Thinking\": \"사용자는 작년 옆반 실내온도가 최고였던 달과 그때의 실내온도를 알고싶어함. 해당 기간의 실내온도 데이터를 쿼리한 후 가장 높은 실내온도를 기록한 달과 그때의 실내온도를 반환하면 됨.\", \"Expectations\": [\"작년 옆반은 {{작년_옆반_실내온도_최고_달}}에 {{작년_옆반_실내온도_최고_온도}}℃로 가장 더웠습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"작년\": \"[DATE_TRUNC(\\'year\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'1 year\\'), DATE_TRUNC(\\'year\\', DATE \\'CURRENT_DATE\\'))\"}, \"spatials\": {\"옆반\": \"01_IB7\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_작년_옆반_실내온도_df = data(t=\\'작년\\',s=\\'옆반\\',m=\\'실내온도\\')\", \"v_작년_옆반_실내온도_최고_온도 = v_작년_옆반_실내온도_df[\\'실내온도\\'].max()\", \"v_작년_옆반_실내온도_최고_df = v_작년_옆반_실내온도_df[v_작년_옆반_실내온도_df[\\'실내온도\\'] == v_작년_옆반_실내온도_최고_온도]\", \"v_작년_옆반_실내온도_최고_달 = get_time(v_작년_옆반_실내온도_최고_df, fmt=\\'month\\')\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '오늘 오전 11시에 옆반의 실내온도는 어땠어?', 'Response': '{\"Thinking\": \"사용자는 오늘 오전 11시에 옆반의 실내온도를 알고싶어함. 오전 11시의 옆반 실내온도 데이터를 쿼리한 후 반환하면 됨.\", \"Expectations\": [\"오늘 오전 11시 옆반의 실내온도는 {{오전11시_옆반_실내온도}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"오늘오전11시\": \"DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\') + INTERVAL \\'11 hours\\'\"}, \"spatials\": {\"옆반\": \"01_IB7\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_오전11시_옆반_실내온도_df = data(t=\\'오늘오전11시\\',s=\\'옆반\\',m=\\'실내온도\\')\", \"v_오전11시_옆반_실내온도 = v_오전11시_옆반_실내온도_df[\\'실내온도\\'].values[0]\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '오늘 오후 4시부터 6시까지 실내온도 평균 알려줘', 'Response': '{\"Thinking\": \"사용자는 오늘 오후 4시부터 6시까지의 시간 동안 우리반의 실내온도 평균을 알고싶어함. 해당 시간 범위의 실내온도 데이터를 쿼리한 후 평균값을 계산하면 됨.\", \"Expectations\": [\"오늘 오후 4시부터 6시까지 우리반의 평균 실내온도는 {{오후4시부터6시_우리반_실내온도_평균}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"오후4시부터6시\": \"[DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\') + INTERVAL \\'16 hours\\', DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\') + INTERVAL \\'18 hours\\')\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_오후4시부터6시_우리반_실내온도_df = data(t=\\'오후4시부터6시\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_오후4시부터6시_우리반_실내온도_평균 = v_오후4시부터6시_우리반_실내온도_df[\\'실내온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?', 'Response': '{\"Thinking\": \"사용자는 지난주 우리반 설정온도와 실내온도 차이가 가장 컸던 날짜를 알고 싶어함. 지난주 우리반의 설정온도와 실내온도 데이터를 쿼리한 후 차이를 계산하면 됨.\", \"Expectations\": [\"지난주 {{지난주_우리반_온도차이_최고_날짜}}에 설정온도({{지난주_우리반_온도차이_최고_설정온도}}℃)와 실내온도({{지난주_우리반_온도차이_최고_실내온도}}℃) 차이가 {{지난주_우리반_온도차이_최고}}℃로 가장 컸습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"지난주\": \"[DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\') - INTERVAL \\'1 week\\', DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"설정온도\": \"settemp\", \"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_지난주_우리반_df = data(t=\\'지난주\\',s=\\'우리반\\',m=[\\'설정온도\\',\\'실내온도\\'])\", \"v_지난주_우리반_df[\\'온도차이\\'] = v_지난주_우리반_df[\\'설정온도\\'] - v_지난주_우리반_df[\\'실내온도\\']\", \"v_지난주_우리반_온도차이_최고 = v_지난주_우리반_df[\\'온도차이\\'].max()\", \"v_지난주_우리반_온도차이_최고_df = v_지난주_우리반_df[v_지난주_우리반_df[\\'온도차이\\'] == v_지난주_우리반_온도차이_최고]\", \"v_지난주_우리반_온도차이_최고_날짜 = get_time(v_지난주_우리반_온도차이_최고_df, fmt=\\'date\\')\", \"v_지난주_우리반_온도차이_최고_설정온도 = v_지난주_우리반_온도차이_최고_df[\\'설정온도\\'].values[0]\", \"v_지난주_우리반_온도차이_최고_실내온도 = v_지난주_우리반_온도차이_최고_df[\\'실내온도\\'].values[0]\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '우리반과 옆반중 더 추운곳은 어디야?', 'Response': '{\"Thinking\": \"사용자는 현재 우리반과 옆반 중 실내온도가 더 낮은 곳을 알고싶어함. 현재 우리반과 옆반의 실내온도를 쿼리한 후 더 낮은 실내온도를 가진 방과 그 온도를 반환하면 됨.\", \"Expectations\": [\"{{현재_실내온도_낮은_공간}}({{현재_실내온도_낮은_공간_실내온도}}℃)이 {{현재_실내온도_높은_공간}}({{현재_실내온도_높은_공간_실내온도}}℃)보다 {{현재_실내온도_차이}}℃ 더 낮아요.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"옆반\": \"01_IB7\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_현재_우리반_실내온도_df = data(t=\\'현재\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_현재_옆반_실내온도_df = data(t=\\'현재\\',s=\\'옆반\\',m=\\'실내온도\\')\", \"v_현재_실내온도 = {\\'우리반\\': v_현재_우리반_실내온도_df[\\'실내온도\\'].values[0], \\'옆반\\': v_현재_옆반_실내온도_df[\\'실내온도\\'].values[0]}\", \"v_현재_실내온도_낮은_공간, v_현재_실내온도_높은_공간 = (\\'우리반\\', \\'옆반\\') if v_현재_실내온도[\\'우리반\\'] < v_현재_실내온도[\\'옆반\\'] else (\\'옆반\\', \\'우리반\\')\", \"v_현재_실내온도_차이 = abs(v_현재_실내온도[v_현재_실내온도_낮은_공간] - v_현재_실내온도[v_현재_실내온도_높은_공간])\", \"v_현재_실내온도_낮은_공간_실내온도 = v_현재_실내온도[v_현재_실내온도_낮은_공간]\", \"v_현재_실내온도_높은_공간_실내온도 = v_현재_실내온도[v_현재_실내온도_높은_공간]\"]}]}'}]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '이번주 우리반과 앞반의 평균 온도 알려줘', 'Response': '{\"Thinking\": \"사용자는 이번주 우리반과 앞반의 실내온도의 평균을 알고싶어함. 이번주 우리반과 앞반의 실내온도 데이터를 쿼리한 후 평균값을 계산하면 됨.\", \"Expectations\": [\"이번주 우리반의 평균 실내온도({{이번주_우리반_실내온도_평균}}℃)와 앞반의 평균 실내온도({{이번주_앞반_실내온도_평균}}℃)의 차이는 {{이번주_실내온도_평균_차이}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"이번주\": \"[DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\'), DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\' + INTERVAL \\'1 week\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"앞반\": \"02_I81\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_이번주_우리반_df = data(t=\\'이번주\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_이번주_앞반_df = data(t=\\'이번주\\',s=\\'앞반\\',m=\\'실내온도\\')\", \"v_이번주_우리반_실내온도_평균 = v_이번주_우리반_df[\\'실내온도\\'].mean()\", \"v_이번주_앞반_실내온도_평균 = v_이번주_앞반_df[\\'실내온도\\'].mean()\", \"v_이번주_실내온도_평균_차이 = abs(v_이번주_우리반_실내온도_평균 - v_이번주_앞반_실내온도_평균)\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '현재 설정온도랑 실내온도 차이 알려줘.', 'Response': '{\"Thinking\": \"사용자는 현재 우리반의 설정온도와 실내온도의 차이를 알고싶어함. 현재 우리반의 설정온도와 실내온도 데이터를 쿼리한 후 차이를 계산하면 됨.\", \"Expectations\": [\"현재 설정온도({{현재_우리반_설정온도}}℃)와 실내온도({{현재_우리반_실내온도}}℃)의 차이는 {{현재_우리반_온도_차이}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"설정온도\": \"settemp\", \"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_현재_우리반_df = data(t=\\'현재\\',s=\\'우리반\\',m=[\\'설정온도\\',\\'실내온도\\'])\", \"v_현재_우리반_설정온도 = v_현재_우리반_df[\\'설정온도\\'].values[0]\", \"v_현재_우리반_실내온도 = v_현재_우리반_df[\\'실내온도\\'].values[0]\", \"v_현재_우리반_온도_차이 = abs(v_현재_우리반_설정온도 - v_현재_우리반_실내온도)\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '지난달에 설정온도와 실내온도 차이가 가장 많이 났던 날은?', 'Response': '{\"Thinking\": \"사용자는 지난달 우리반 설정온도와 실내온도 차이가 가장 컸던 날짜를 알고 싶어함. 지난달 우리반의 설정온도와 실내온도 데이터를 쿼리한 후 차이를 계산하면 됨.\", \"Expectations\": [\"지난달 {{지난달_우리반_온도차이_최고_날짜}}에 설정온도({{지난달_우리반_온도차이_최고_설정온도}}℃)와 실내온도({{지난달_우리반_온도차이_최고_실내온도}}℃) 차이가 {{지난달_우리반_온도차이_최고}}℃로 가장 컸습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"지난달\": \"[DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\') - INTERVAL \\'1 month\\', DATE_TRUNC(\\'month\\', DATE \\'CURRENT_DATE\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"설정온도\": \"settemp\", \"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_지난달_우리반_df = data(t=\\'지난달\\',s=\\'우리반\\',m=[\\'설정온도\\',\\'실내온도\\'])\", \"v_지난달_우리반_df[\\'온도차이\\'] = v_지난달_우리반_df[\\'설정온도\\'] - v_지난달_우리반_df[\\'실내온도\\']\", \"v_지난달_우리반_온도차이_최고 = v_지난달_우리반_df[\\'온도차이\\'].max()\", \"v_지난달_우리반_온도차이_최고_df = v_지난달_우리반_df[v_지난달_우리반_df[\\'온도차이\\'] == v_지난달_우리반_온도차이_최고]\", \"v_지난달_우리반_온도차이_최고_날짜 = get_time(v_지난달_우리반_온도차이_최고_df, fmt=\\'date\\')\", \"v_지난달_우리반_온도차이_최고_설정온도 = v_지난달_우리반_온도차이_최고_df[\\'설정온도\\'].values[0]\", \"v_지난달_우리반_온도차이_최고_실내온도 = v_지난달_우리반_온도차이_최고_df[\\'실내온도\\'].values[0]\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '이번주 우리반과 옆반의 평균 실내온도 차이 알려줘', 'Response': '{\"Thinking\": \"사용자는 이번주 우리반과 옆반의 평균 실내온도 차이를 알고싶어함. 이번주 우리반과 옆반의 실내온도 데이터를 쿼리한 후 평균값을 계산하면 됨.\", \"Expectations\": [\"이번주 우리반 평균 실내온도({{이번주_우리반_실내온도_평균}}℃)와 옆반 평균 실내온도({{이번주_옆반_실내온도_평균}}℃) 차이는 {{이번주_실내온도_평균_차이}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"이번주\": \"[DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\'), DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\' + INTERVAL \\'1 week\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"옆반\": \"01_IB7\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_이번주_우리반_df = data(t=\\'이번주\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_이번주_옆반_df = data(t=\\'이번주\\',s=\\'옆반\\',m=\\'실내온도\\')\", \"v_이번주_우리반_실내온도_평균 = v_이번주_우리반_df[\\'실내온도\\'].mean()\", \"v_이번주_옆반_실내온도_평균 = v_이번주_옆반_df[\\'실내온도\\'].mean()\", \"v_이번주_실내온도_평균_차이 = abs(v_이번주_우리반_실내온도_평균 - v_이번주_옆반_실내온도_평균)\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '2주전 가장 더웠던 날 알려줘', 'Response': '{\"Thinking\": \"사용자는 2주전 우리반의 실내온도가 가장 높았던 날짜를 알고 싶어함. 2주전 우리반의 실내온도 데이터를 쿼리한 후 최고값을 기록한 날짜와 값을 반환하면 됨.\", \"Expectations\": [\"2주전 {{2주전_우리반_실내온도_최고_날짜}}에 실내온도({{2주전_우리반_실내온도_최고_실내온도}}℃)가 가장 높았습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"2주전\": \"[DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'2 week\\'), DATE_TRUNC(\\'week\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'1 week\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_2주전_우리반_df = data(t=\\'2주전\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_2주전_우리반_실내온도_최고 = v_2주전_우리반_df[\\'실내온도\\'].max()\", \"v_2주전_우리반_실내온도_최고_df = v_2주전_우리반_df[v_2주전_우리반_df[\\'실내온도\\'] == v_2주전_우리반_실내온도_최고]\", \"v_2주전_우리반_실내온도_최고_날짜 = get_time(v_2주전_우리반_실내온도_최고_df, fmt=\\'date\\')\", \"v_2주전_우리반_실내온도_최고_실내온도 = v_2주전_우리반_실내온도_최고_df[\\'실내온도\\'].values[0]\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '화성의 설정온도 확인해줘', 'Response': '{\"Thinking\": \"사용자는 현재 화성의 설정온도를 알고 싶어함. 현재 화성의 설정온도 데이터를 쿼리한 후 값을 반환하면 됨.\", \"Expectations\": [\"현재 화성의 설정온도는 {{현재_화성_설정온도}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"화성\": \"Unknown\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '옆반 습도 알려줘', 'Response': '{\"Thinking\": \"사용자는 옆반의 현재 습도를 알고 싶어함. 옆반의 현재 습도 데이터를 쿼리한 후 값을 반환하면 됨.\", \"Expectations\": [\"옆반의 현재 습도는 {{옆반_습도}}%입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"옆반\": \"01_IB7\"}, \"modalities\": {\"습도\": \"Unknown\"}}}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '지난 3일 동안 우리반 실내 온도 평균 값 알려줘.', 'Response': '{\"Thinking\": \"사용자는 지난 3일 동안 우리반의 실내온도 평균값을 알고 싶어함. 지난 3일 동안 우리반의 실내온도 데이터를 쿼리한 후 평균값을 계산하면 됨.\", \"Expectations\": [\"지난 3일간 우리반 실내온도 평균값은 {{지난3일_우리반_실내온도_평균}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"지난3일\": \"[DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\' - INTERVAL \\'3 day\\'), DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\'))\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_지난3일_우리반_실내온도_df = data(t=\\'지난3일\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_지난3일_우리반_실내온도_평균 = v_지난3일_우리반_실내온도_df[\\'실내온도\\'].mean()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '오늘 오후 5시에 옆반의 설정온도는 어땠어?', 'Response': '{\"Thinking\": \"사용자는 오늘 오후 5시에 옆반의 설정온도를 알고 싶어함. 오늘 오후 5시에 옆반의 설정온도 데이터를 쿼리한 후 값을 반환하면 됨.\", \"Expectations\": [\"오늘 오후 5시 옆반의 설정온도는 {{오늘오후5시_옆반_설정온도}}℃였습니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"오늘오후5시\": \"DATE_TRUNC(\\'day\\', DATE \\'CURRENT_DATE\\') + INTERVAL \\'17 hours\\'\"}, \"spatials\": {\"옆반\": \"01_IB7\"}, \"modalities\": {\"설정온도\": \"settemp\"}}}, {\"type\": \"o\", \"script\": [\"v_오늘오후5시_옆반_df = data(t=\\'오늘오후5시\\',s=\\'옆반\\',m=\\'설정온도\\')\", \"v_오늘오후5시_옆반_설정온도 = v_오늘오후5시_옆반_df[\\'설정온도\\'].values[0]\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '올해 여름 우리반 실내온도 최대값과 최소값 알려줘', 'Response': '{\"Thinking\": \"사용자는 올해 6월 ~ 8월 동안 우리반의 실내온도 최대값과 최소값을 알고 싶어함. 올해 6월 ~ 8월 동안 우리반의 실내온도 데이터를 쿼리한 후 최대값과 최소값을 계산하면 됨.\", \"Expectations\": [\"올해 여름(6월 ~ 8월) 우리반의 실내온도 최대값과 최소값은 각각 {{올해여름_우리반_실내온도_최대}}℃와 {{올해여름_우리반_실내온도_최소}}℃입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"올해여름\": \"[DATE \\'CURRENT_YEAR-06-01\\', DATE \\'CURRENT_YEAR-09-01\\')\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_올해여름_우리반_df = data(t=\\'올해여름\\',s=\\'우리반\\',m=\\'실내온도\\')\", \"v_올해여름_우리반_실내온도_최대 = v_올해여름_우리반_df[\\'실내온도\\'].max()\", \"v_올해여름_우리반_실내온도_최소 = v_올해여름_우리반_df[\\'실내온도\\'].min()\"]}]}'}, {'Metadata': {'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-30 12:00:00'}, 'Input': '우리반과 앞반 중 가장 더운 방은?', 'Response': '{\"Thinking\": \"사용자는 현재 우리반과 앞반 중 실내온도가 최고인 방을 알고싶어함.현재 두 방의 실내온도를 쿼리한 후 실내온도가 최고인 방과 그 온도를 반환하면 됨.\", \"Expectations\": [\"현재 두 방 중 {{현재_실내온도_최고_방}}이(가) {{현재_실내온도_최고}}℃로 가장 더운 방이에요.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\", \"앞반\": \"02_I81\"}, \"modalities\": {\"실내온도\": \"roomtemp\"}}}, {\"type\": \"o\", \"script\": [\"v_현재_방별_실내온도_df = data(t=\\'현재\\',s=[\\'우리반\\',\\'앞반\\'],m=\\'실내온도\\')\", \"v_현재_실내온도_최고 = v_현재_방별_실내온도_df[\\'실내온도\\'].max()\", \"v_현재_실내온도_최고_df = v_현재_방별_실내온도_df[v_현재_방별_실내온도_df[\\'실내온도\\'] == v_현재_실내온도_최고]\", \"v_현재_실내온도_최고_방 = get_spatials(v_현재_실내온도_최고_df)\"]}]}'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745363bebcdd474b823ae94c10c0ad8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefd0ef416cc4f48abda437bb0d089ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1090\n",
      "{'Metadata': {'current_datetime': '2022-09-30 12:00:00', 'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}}, 'Input': '지금 에너지 사용량 알려줘', 'Response': '{\"Thinking\": \"사용자는 현재 우리반의 에너지 사용량을 알고싶어함. 현재 우리반의 에너지 사용량을 쿼리한 후 반환하면 됨.\", \"Expectations\": [\"현재 우리반의 에너지 사용량은 {{현재_우리반_에너지_사용량}}kWh입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"에너지 사용량\": \"Unknown\"}}}]}', 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n너는 유저의 HVAC 관련 질문에 답변하는 Agent의 계획을 설계하는 정확하고 훌룡한 인공지능이다. \\n사용자의 질문(Input)을 받아 Agent의 Instructions를 출력해 주어야 한다.\\nThinking에서는 Metadata를 바탕으로 Input을 low-level term으로 수정하고, 유저의 의도를 추측하며, 답변 계획을 세워야함.\\nExpectation에서는 유저가 기대할만한 답변을 추측해야함.\\n\\n너는 type=q의 instruction으로 쿼리를, type=o로 python 기반 data operation을 계획할 수 있다.\\ntype=q에서는 필요한 모든 data들을 temporal, spatial, modality(columns) 필터를 거쳐 조회해야한다.\\n시간과 관련된 필터링은 type=o가 아닌 type=q에서 수행한다.\\n\\nexpectation의 명시된 모든 variable이 type=o에서 계산되야 한다.\\n\\njson형식으로 출력하며, eval() 함수를 사용할 수 있도록 괄호들과 따옴표들의 순서와 닫힘을 매우 신경써서 출력해야한다.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMetadata:{\\'current_datetime\\': \\'2022-09-30 12:00:00\\', \\'idu_mapping\\': {\\'01_IB5\\': [\\'우리반\\'], \\'01_IB7\\': [\\'옆반\\'], \\'02_I81\\': [\\'앞반\\']}, \\'modality_mapping\\': {\\'roomtemp\\': [\\'실내온도\\'], \\'settemp\\': [\\'설정온도\\']}};Input:지금 에너지 사용량 알려줘;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{\"Thinking\": \"사용자는 현재 우리반의 에너지 사용량을 알고싶어함. 현재 우리반의 에너지 사용량을 쿼리한 후 반환하면 됨.\", \"Expectations\": [\"현재 우리반의 에너지 사용량은 {{현재_우리반_에너지_사용량}}kWh입니다.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"temporal\": {\"현재\": \"LAST_RECORD\"}, \"spatials\": {\"우리반\": \"01_IB5\"}, \"modalities\": {\"에너지 사용량\": \"Unknown\"}}}]}<|eot_id|>'}\n",
      "27 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def read_dataset(dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if \"v6\" in dataset_name:\n",
    "            if train_type in [\"woall\"]:\n",
    "                del d[\"Response\"][\"생각\"]\n",
    "        elif \"v7\" in dataset_name:\n",
    "            if train_type in [\"woCoTExp\"]:\n",
    "                del d[\"Response\"][\"Thinking\"]\n",
    "                # expectations = d[\"Response\"][\"Expectations\"]\n",
    "                del d[\"Response\"][\"Expectations\"]\n",
    "                # d[\"Response\"][\"Expectations\"] = expectations\n",
    "            elif train_type in [\"woCoT\"]:\n",
    "                del d[\"Response\"][\"Thinking\"]\n",
    "            elif train_type in [\"woQM\"]:\n",
    "                # del d[\"Response\"]???\n",
    "                instructions = d[\"Response\"][\"Instructions\"]\n",
    "                instruction_qs = [i for i in instructions if i[\"type\"] == \"q\"]\n",
    "                final_instructions = []\n",
    "                for raw_instruction in instruction_qs:\n",
    "                    args = raw_instruction[\"args\"]\n",
    "                    if \"table_name\" in args:\n",
    "                        del args[\"table_name\"]\n",
    "                    \n",
    "                    if \"idu_name\" not in args[\"columns\"]:\n",
    "                        args[\"columns\"].append(\"idu_name\")\n",
    "                    \n",
    "                    instruction = InstructionQ(\n",
    "                        args=args,\n",
    "                        result_name=raw_instruction[\"result_name\"]\n",
    "                    )\n",
    "                    query_string = DBManager.get_query_strings(metadata, instruction.args, exp_tag=train_type)\n",
    "                    final_instructions.append({\n",
    "                        \"type\": \"q\",\n",
    "                        \"query\": query_string,\n",
    "                        \"result_name\": raw_instruction[\"result_name\"]\n",
    "                    })\n",
    "\n",
    "                    \n",
    "                \n",
    "                instructions_os = [i for i in instructions if i[\"type\"] == \"o\"]\n",
    "                final_instructions.extend(instructions_os)\n",
    "                print(\"INPUT:\", d[\"Input\"])\n",
    "                print(final_instructions)\n",
    "                d[\"Response\"][\"Instructions\"] = final_instructions\n",
    "                # raise NotImplementedError\n",
    "            elif train_type in [\"woOp\"]:\n",
    "                instructions = d[\"Response\"][\"Instructions\"]\n",
    "                d[\"Response\"][\"Instructions\"] = [i for i in instructions if i[\"type\"] == \"q\"]\n",
    "\n",
    "        \n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "\n",
    "        skip = False\n",
    "        for skip_tag in skip_tags:\n",
    "            if skip_tag in tags:\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "dataset_trs = []\n",
    "dataset_tss = []\n",
    "for scenario_dir in scenario_dirs:\n",
    "    dataset_trs.extend(read_dataset(scenario_dir, \"onlyq_tr.json\"))\n",
    "    dataset_tss.extend(read_dataset(scenario_dir, \"onlyq_ts.json\"))\n",
    "    # print(\"Warning!!!: Test set is not mutually exclusive with training set.\")\n",
    "\n",
    "dataset_sampling_rate = 1\n",
    "dataset_trs = np.random.choice(dataset_trs, size=int(len(dataset_trs) * dataset_sampling_rate), replace=False)\n",
    "dataset_trs = dataset_trs.tolist()\n",
    "dataset_tr = Dataset.from_list(dataset_trs) # 서로 다른 메타데이터 합치면서\n",
    "dataset_ts = Dataset.from_list(dataset_tss) # Mutually exclusive한 애들은 None 됨\n",
    "\n",
    "max_seq_length = 0\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for metadata, input, response in zip(examples['Metadata'], examples['Input'], examples['Response']):\n",
    "        # global max_seq_length\n",
    "        response.replace(\"    \", \"\")\n",
    "\n",
    "        # print(metadata['current_datetime'])\n",
    "        # print(metadata['idu_mapping'])\n",
    "\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        if \"llama\" in model_id.lower():\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            user_input = {\n",
    "                \"content\": f\"Metadata:{metadata};Input:{input};\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "        elif \"gemma\" in model_id.lower():\n",
    "            user_input = {\n",
    "                \"content\": f\"{common_prompt};{metadata};{input}\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([user_input, answer])\n",
    "        else:\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            user_input = {\n",
    "                \"content\": f\"Metadata:{metadata};Input:{input};\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        # print(\"Answer length: \", len(response))\n",
    "        # convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        # if len(response) + 50 > max_seq_length:\n",
    "        #     max_seq_length = len(response) + len(metadata) + len(input) + 50\n",
    "            # print(response)\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    \n",
    "    # remove \\n\\nCutting Knowledge Date: BLAH BLAH \\nToday Date: BLAH BLAH\\n\\n using regex\n",
    "    # texts = [re.sub(r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', '', text) for text in texts]\n",
    "\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset_tr = dataset_tr.map(formatting_prompts_func, batched=True)\n",
    "dataset_ts = dataset_ts.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "max_seq_length = 10000\n",
    "max_seq_length = max([len(tokenizer.encode(dataset_tr[i]['text'])) for i in range(len(dataset_tr))]) + 100\n",
    "# max_seq_length += len(common_prompt)\n",
    "print(max_seq_length)\n",
    "print(dataset_tr[0])\n",
    "print(len(dataset_tr), len(dataset_ts))\n",
    "# print(f\"seq length: {len(tokenizer.encode(dataset_tr[0]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v7_r200_a400_ours_tr27_0613\n"
     ]
    }
   ],
   "source": [
    "lora_r = 200\n",
    "lora_alpha = lora_r * 2\n",
    "lora_repr = f\"v7_r{lora_r}_a{lora_alpha}_{train_type}_tr{len(dataset_tr)}_0613\"\n",
    "print(lora_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    pretrained_model,\n",
    "    r=lora_r,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    # \"embed_tokens\",\n",
    "                    # \"lm_head\"\n",
    "                    ],\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None,   # No LoftQ, for standard fine-tuning\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "# del pretrained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(len(dataset_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(f\"{model_dir}/chkpts/{lora_repr}\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Accumulation Steps: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5d3919da8e427ca4a8e3c55247185f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cc78ff355740aeb4dfc15911f53550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "per_device_train_batch_size, epochs = 60, 60 # 8\n",
    "gradient_accumulation_steps = int(np.ceil(len(dataset_tr) / per_device_train_batch_size))\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "# clear all checkpoints\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # num_train_epochs = 1,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,  # Controls the batch size per device\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # Accumulates gradients to simulate a larger batch\n",
    "    max_steps=gradient_accumulation_steps * epochs,\n",
    "    # 리소스 제약때문에 batch size를 타협해야하는 경우가 발생 -> micro batch size를 줄이고,\n",
    " \t# accumulated step을 늘려, 적절한 size로 gradient를 구해 weight update\n",
    "    # https://www.youtube.com/watch?v=ptlmj9Y9iwE\n",
    "    warmup_steps = gradient_accumulation_steps,\n",
    "    learning_rate = 1e-4,             # Sets the learning rate for optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_torch\", # adamw_torch, adafactor, prodigy\n",
    "    weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "    lr_scheduler_type = \"cosine\",  # Sets the learning rate scheduler\n",
    "    seed = 3407,                        \n",
    "    output_dir = f\"{model_dir}/chkpts/{lora_repr}\",  # Output directory for checkpoints and predictions     \n",
    "    report_to = \"none\",              # Enables Weights & Biases (W&B) logging\n",
    "    logging_steps = gradient_accumulation_steps,                # Sets frequency of logging to W&B\n",
    "    logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "    eval_strategy=\"steps\",  # enable evaluation during training\n",
    "    eval_steps=gradient_accumulation_steps,\n",
    "    eval_accumulation_steps=1, # 낮을수록 eval시 사용하는 메모리 줄어듦\n",
    "    save_steps=gradient_accumulation_steps,\n",
    "    save_strategy = \"steps\",               \n",
    "    # load_best_model_at_end = True,    # Loads the best model at the end\n",
    "    save_only_model = False,           # Saves entire model, not only weights\n",
    "    # resume_from_checkpoint = f\"{model_dir}/chkpts/{lora_repr}\",  # Resumes training from a checkpoint\n",
    ")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     pred_ids = eval_pred.predictions\n",
    "#     print(pred_ids, type(pred_ids), pred_ids.shape)\n",
    "#     predictions = tokenizer.batch_decode(\n",
    "#         pred_ids, \n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "#     print(predictions)\n",
    "#     return {\"accuracy\": 0}\n",
    "\n",
    "# class CustomCallback(TrainerCallback):\n",
    "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "#         print(f\"Step {state.global_step}: {logs}\")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_tr,\n",
    "    eval_dataset = dataset_ts,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,        # Can make training 5x faster for short sequences.\n",
    "    args = args,\n",
    "    # compute_metrics = compute_metrics,\n",
    "    # callbacks = [CustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 27 | Num Epochs = 60 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 60 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (60 x 1 x 1) = 60\n",
      " \"-____-\"     Trainable parameters = 524,288,000/8,554,549,248 (6.13% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/60 04:12 < 05:10, 0.10 it/s, Epoch 27/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.286900</td>\n",
       "      <td>2.193689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.286900</td>\n",
       "      <td>1.381161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>0.790342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.357131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.230546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.233500</td>\n",
       "      <td>0.154765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.120741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.106401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.097515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.092635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.095857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.101172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.106438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.115625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.121086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.127652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.133869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.143648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.146368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.151379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.157372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.161456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.164084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.166027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.168748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.170644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.173156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43munsloth_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/unsloth/trainer.py:45\u001b[39m, in \u001b[36munsloth_train\u001b[39m\u001b[34m(trainer, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munsloth_train\u001b[39m(trainer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:381\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:3102\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3099\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3102\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3103\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:3199\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3197\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3198\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3199\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3202\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:3911\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3908\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3910\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3911\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3913\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3914\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:4015\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   4013\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   4014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4015\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   4017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4020\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:323\u001b[39m, in \u001b[36mPeftModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, path_initial_model_for_weight_conversion, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m         peft_config.save_pretrained(path_initial_model_for_weight_conversion)\n\u001b[32m    320\u001b[39m         output_state_dict = save_mutated_as_lora(\n\u001b[32m    321\u001b[39m             peft_config, path_initial_model_for_weight_conversion, output_state_dict, kwargs\n\u001b[32m    322\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFETENSORS_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_main_process:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path_initial_model_for_weight_conversion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base model\n",
    "\n",
    "# save_dir = f\"{model_dir}/gguf\"\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# command = (\n",
    "#     f\"python ../../llama.cpp/convert_hf_to_gguf.py \"\n",
    "#     # f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "#     f\"--outfile {save_dir}/base.gguf \"              # Output file for the GGUF model\n",
    "#     f\"--outtype f16 \"                      # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "#     f\"--verbose \"                            # Optional: increase logging output\n",
    "#     f\"{model_dir}\"                      # Positional argument: path to the LoRA adapter files\n",
    "# )\n",
    "\n",
    "# print(command)\n",
    "\n",
    "# os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56\n",
      "/model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56\n"
     ]
    }
   ],
   "source": [
    "step = 56\n",
    "\n",
    "# lora_repr = \"v7_r8_a16_ours_70B\"\n",
    "checkpoint_dir = f\"{model_dir}/chkpts/{lora_repr}/checkpoint-{step}\"\n",
    "output_path = f\"{model_dir}/gguf/{lora_repr}-checkpoint-{step}.gguf\"\n",
    "lora_output_dir = f\"{model_dir}/lora_output/\"\n",
    "\n",
    "if not os.path.exists(f\"{model_dir}/gguf\"):\n",
    "    os.makedirs(f\"{model_dir}/gguf\")\n",
    "print(checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(lora_output_dir):\n",
    "    os.makedirs(lora_output_dir)\n",
    "print(checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56/*.json (invalid repository id)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:125\u001b[39m, in \u001b[36mHfFileSystem._repo_and_revision_exist\u001b[39m\u001b[34m(self, repo_type, repo_id, revision)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_HUB_ETAG_TIMEOUT\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '/model'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m#     checkpoint_dir,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m#     local_files_only=True\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     peft_model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# FastLanguageModel.for_inference(model)\u001b[39;00m\n\u001b[32m     29\u001b[39m     tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/unsloth/models/loader.py:199\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m     both_exist = exist_adapter_config \u001b[38;5;129;01mand\u001b[39;00m exist_config\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# Because HfFileSystem assumes linux paths, we need to set the path with forward slashes, even on Windows.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     files = \u001b[43mHfFileSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/*.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     files = (os.path.split(x)[-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files)\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(x == \u001b[33m\"\u001b[39m\u001b[33madapter_config.json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x == \u001b[33m\"\u001b[39m\u001b[33mconfig.json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files) >= \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:520\u001b[39m, in \u001b[36mHfFileSystem.glob\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Set expand_info=False by default to get a x10 speed boost\u001b[39;00m\n\u001b[32m    519\u001b[39m kwargs = {\u001b[33m\"\u001b[39m\u001b[33mexpand_info\u001b[39m\u001b[33m\"\u001b[39m: kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdetail\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m), **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrevision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.unresolve()\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().glob(path, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:216\u001b[39m, in \u001b[36mHfFileSystem.resolve_path\u001b[39m\u001b[34m(self, path, revision)\u001b[39m\n\u001b[32m    214\u001b[39m     repo_and_revision_exist, _ = \u001b[38;5;28mself\u001b[39m._repo_and_revision_exist(repo_type, repo_id, revision)\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[43m_raise_file_not_found\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    218\u001b[39m     _raise_file_not_found(path, err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:1136\u001b[39m, in \u001b[36m_raise_file_not_found\u001b[39m\u001b[34m(path, err)\u001b[39m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, HFValidationError):\n\u001b[32m   1135\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (invalid repository id)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: /model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56/*.json (invalid repository id)"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     torch_dtype=torch_dtype,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     # attn_implementation=attn_implementation,\n",
    "    #     local_files_only=True,\n",
    "    #     device_map=\"cuda\"\n",
    "    # )\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     local_files_only=True\n",
    "    # )\n",
    "    \n",
    "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        dtype = torch_dtype,\n",
    "        attn_implementation=attn_implementation,\n",
    "        load_in_4bit = False,\n",
    "        load_in_8bit=False,\n",
    "        cache_dir=f\"{model_dir}/cache\",\n",
    "        local_files_only=True,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    # FastLanguageModel.for_inference(model)\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # merge lora model and base pretrained model\n",
    "    # model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# peft_model.save_pretrained_gguf(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, quantization_method = \"q8_0\")\n",
    "peft_model.save_pretrained_merged(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, save_method=\"merged_16bit\")\n",
    "# tokenizer.save_pretrained(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# command = (\n",
    "#     f\"python ../../llama.cpp/convert_lora_to_gguf.py \"\n",
    "#     f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "#     f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "#     f\"--outtype f16 \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "#     f\"--verbose \"                            # Optional: increase logging output\n",
    "#     f\"{checkpoint_dir}\"                      # Positional argument: path to the LoRA adapter files\n",
    "# )\n",
    "\n",
    "command = (\n",
    "    f\"python ../../llama.cpp/convert_hf_to_gguf.py \"\n",
    "    # f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "    f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "    f\"--outtype f16 \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "    f\"--verbose \"                            # Optional: increase logging output\n",
    "    f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\"                      # Positional argument: path to the LoRA adapter files\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = (\n",
    "#     f\"../../llama.cpp/build/bin/llama-quantize \"\n",
    "#     f\"{output_path} \"\n",
    "#     f\"{output_path.replace('.gguf', '-Q4_K_M.gguf')} \"                      # Positional argument: path to the LoRA adapter files\n",
    "#     f\"Q4_K_M \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"Running command:\", command)\n",
    "# os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy(\n",
    "    f\"{output_path}\",\n",
    "    f\"../../src/i2i.gguf\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -cvf - ../../src/i2i.gguf | pigz -p 128 > src/i2i.tar.gz\n",
    "\n",
    "command = (\n",
    "    f\"tar -cvf - ../../src/i2i.gguf | pigz -p 128 > ../../src/i2i.tar.gz\"\n",
    ")\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
