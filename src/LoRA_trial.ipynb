{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n",
      "1\n",
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "from unsloth import FastLanguageModel, unsloth_train\n",
    "\n",
    "from transformers import TrainerCallback, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from src.db.manager import DBManager\n",
    "from src.input_to_instructions.types import InstructionQ\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    transformers.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# ì‹œë“œê°’ ì„¤ì •\n",
    "seed = 3407\n",
    "set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 0     # Unsloth auto supports RoPE Scaling internally!\n",
    "# dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "device = f\"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'sh2orc/Llama-3.1-Korean-8B-Instruct'\n",
    "# model_id = 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = 'Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B'\n",
    "# model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "# model_id = 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct'\n",
    "\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download(repo_id=model_id, local_dir=\"70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizer initialization\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     cache_dir=f\"{model_dir}/cache\",\n",
    "#     # attn_implementation=attn_implementation,\n",
    "#     local_files_only=True,\n",
    "#     device_map=\"cuda\"\n",
    "# )\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\n",
    "# #     model_id,\n",
    "# #     cache_dir=f\"{model_dir}/cache\",\n",
    "# #     local_files_only=True\n",
    "# # )\n",
    "# # if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "# pretrained_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer initialization\n",
    "\n",
    "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = torch_dtype,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     # load_in_8bit=True,\n",
    "    #     # llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    # ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # trust_remote_code=True\n",
    "    local_files_only=True\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "\n",
    "# if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "#     pretrained_model.save_pretrained(model_dir)\n",
    "#     tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_type: woQM+Script\n"
     ]
    }
   ],
   "source": [
    "train_type = [\n",
    "    \"BASE\", # 0\n",
    "\n",
    "    \"WoThinking\", # 1\n",
    "    \"WoMetadata\", # 2\n",
    "    \"WoMetadata+Thinking\", # 3\n",
    "\n",
    "    \"woQM\", # 4\n",
    "    \"woQM+Script\", # 5\n",
    "    \"woScript\", # 6\n",
    "\n",
    "    \"woExp\", # 7\n",
    "    \"ours\" # 8\n",
    "][5]\n",
    "print(f\"train_type: {train_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DATASET_DIR: ../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall\n",
      "[PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/refine.ipynb'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/summary.ipynb'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/prompt.txt'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario2')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# BASE_DATASET_DIR = Path(\"../dataset/v5-250228-multimetadata\")\n",
    "# dataset_name = \"v6-250306-optimizetoken\"\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "BASE_DATASET_DIR = Path(f\"../finetuning/dataset/{dataset_name}\")\n",
    "print(f\"BASE_DATASET_DIR: {BASE_DATASET_DIR}\")\n",
    "print(list(BASE_DATASET_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \n",
      "ì‚¬ìš©ìì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„ Agentì˜ Instructionsë¥¼ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\n",
      "Thinkingì—ì„œëŠ” HVAC ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìœ ì €ì˜ ì˜ë„ë¥¼ ì¶”ì¸¡í•´ ë¹ ì§„ contextë¥¼ ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì˜ ëª¨í˜¸í•¨ì„ ì—†ì•¤ ì™„ë²½í•œ í˜•íƒœì˜ ì§ˆë¬¸ì„ ì¶œë ¥í•˜ê³ , ì´ì— ëŒ€í•œ ë‹µë³€ ê³„íšì„ ì„¸ì›Œì•¼í•¨.\n",
      "Expectationì—ì„œëŠ” ìœ ì €ê°€ ê¸°ëŒ€í• ë§Œí•œ ë‹µë³€ì„ ì¶”ì¸¡í•´ì•¼í•¨.\n",
      "Mappingì—ì„œëŠ” ì§ˆë¬¸ì— ì‚¬ìš©ëœ high-level taxonomyë¥¼ metadataë¥¼ ë°”íƒ•ìœ¼ë¡œ low-level taxonomyë¡œì˜ mappingì„ ê³„ì‚°í•˜ì—¬ì•¼ í•¨.\n",
      "ì´ë•Œ metadataì— ì—†ëŠ” ì •ë³´ë¥¼ ê¸°ìˆ í•˜ëŠ” ë“± ê±°ì§“ëœ ì¶œë ¥ì„ í•˜ë©´ ì•ˆë˜ê³ , ëª¨ë¥´ëŠ” ì •ë³´ê°€ ìˆìœ¼ë©´ Unknownì´ë¼ ë‹µë³€í•´ì•¼í•¨.\n",
      "\n",
      "jsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«í˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sub(name, common_prompt):\n",
    "    # Remove the section between <|name|> ... <|name|> including the tags themselves\n",
    "    # Use re.DOTALL to match newlines with '.'\n",
    "    pattern = rf\"\\n?<\\|{name}\\|>[\\s\\S]*?<\\|{name}\\|>\"\n",
    "    common_prompt = re.sub(pattern, \"\", common_prompt, flags=re.DOTALL)\n",
    "    return common_prompt\n",
    "\n",
    "common_prompt = open(BASE_DATASET_DIR / f\"prompt.txt\", \"r\").read()\n",
    "\n",
    "sub_targets = []\n",
    "if train_type == \"ours\":\n",
    "    sub_targets = []\n",
    "elif train_type == \"BASE\":\n",
    "    sub_targets = [\"Thinking\", \"Expectation\", \"Mapping\", \"Script\", \"Examples\"]\n",
    "elif train_type in [\"WoThinking\"]:\n",
    "    sub_targets = [\"Thinking\"]\n",
    "elif train_type in [\"woMetadata\"]:\n",
    "    sub_targets = [\"Metadata\"]\n",
    "elif train_type in [\"WoMetadata+Thinking\"]:\n",
    "    sub_targets = [\"Metadata\", \"Thinking\"]\n",
    "elif train_type in [\"woExp\"]:\n",
    "    sub_targets = [\"Expectation\"]\n",
    "\n",
    "if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "    sub_targets = [\"QM\", \"Mapping\"]\n",
    "if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "    sub_targets = [\"Script\"]\n",
    "\n",
    "\n",
    "for sub_target in sub_targets:\n",
    "    common_prompt = sub(sub_target, common_prompt)\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "print(common_prompt)\n",
    "\n",
    "# print(common_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario2')]\n"
     ]
    }
   ],
   "source": [
    "scenario_dirs = [d for d in BASE_DATASET_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]\n",
    "print(scenario_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b422ff13c644df28377b8cdc3a04fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a2e6c89bd545bba61a7ae58ba8c559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "{'Metadata': {'current_datetime': '2022-09-30 12:00:00', 'idu_mapping': {'01_IB5': ['ìš°ë¦¬ë°˜', '4ì¸µ'], '01_IB7': ['ì˜†ë°˜', '4ì¸µ'], '02_I81': ['ì•ë°˜', '4ì¸µ']}, 'modality_mapping': {'roomtemp': ['ì‹¤ë‚´ì˜¨ë„'], 'settemp': ['ì„¤ì •ì˜¨ë„']}}, 'Input': 'ì§€ê¸ˆ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ì•Œë ¤ì¤˜', 'Response': '{\"Thinking\": \"ì‚¬ìš©ìëŠ” í˜„ì¬ ìš°ë¦¬ë°˜ì˜ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ì„ ì•Œê³ ì‹¶ì–´í•¨. í˜„ì¬ ìš°ë¦¬ë°˜ì˜ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ì„ ì¿¼ë¦¬í•œ í›„ ë°˜í™˜í•˜ë©´ ë¨.\", \"Expectations\": [\"í˜„ì¬ ìš°ë¦¬ë°˜ì˜ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ì€ {{í˜„ì¬_ìš°ë¦¬ë°˜_ì—ë„ˆì§€_ì‚¬ìš©ëŸ‰}}kWhì…ë‹ˆë‹¤.\"], \"Mapping\": {\"temporal\": {\"í˜„ì¬\": \"LAST_RECORD\"}, \"spatials\": {\"ìš°ë¦¬ë°˜\": \"01_IB5\"}, \"modalities\": {\"ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰\": \"Unknown\"}}}', 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\në„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \\nì‚¬ìš©ìì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„ Agentì˜ Instructionsë¥¼ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\\nThinkingì—ì„œëŠ” HVAC ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìœ ì €ì˜ ì˜ë„ë¥¼ ì¶”ì¸¡í•´ ë¹ ì§„ contextë¥¼ ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì˜ ëª¨í˜¸í•¨ì„ ì—†ì•¤ ì™„ë²½í•œ í˜•íƒœì˜ ì§ˆë¬¸ì„ ì¶œë ¥í•˜ê³ , ì´ì— ëŒ€í•œ ë‹µë³€ ê³„íšì„ ì„¸ì›Œì•¼í•¨.\\nExpectationì—ì„œëŠ” ìœ ì €ê°€ ê¸°ëŒ€í• ë§Œí•œ ë‹µë³€ì„ ì¶”ì¸¡í•´ì•¼í•¨.\\nMappingì—ì„œëŠ” ì§ˆë¬¸ì— ì‚¬ìš©ëœ high-level taxonomyë¥¼ metadataë¥¼ ë°”íƒ•ìœ¼ë¡œ low-level taxonomyë¡œì˜ mappingì„ ê³„ì‚°í•˜ì—¬ì•¼ í•¨.\\nì´ë•Œ metadataì— ì—†ëŠ” ì •ë³´ë¥¼ ê¸°ìˆ í•˜ëŠ” ë“± ê±°ì§“ëœ ì¶œë ¥ì„ í•˜ë©´ ì•ˆë˜ê³ , ëª¨ë¥´ëŠ” ì •ë³´ê°€ ìˆìœ¼ë©´ Unknownì´ë¼ ë‹µë³€í•´ì•¼í•¨.\\n\\njsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«í˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMetadata:{\\'current_datetime\\': \\'2022-09-30 12:00:00\\', \\'idu_mapping\\': {\\'01_IB5\\': [\\'ìš°ë¦¬ë°˜\\', \\'4ì¸µ\\'], \\'01_IB7\\': [\\'ì˜†ë°˜\\', \\'4ì¸µ\\'], \\'02_I81\\': [\\'ì•ë°˜\\', \\'4ì¸µ\\']}, \\'modality_mapping\\': {\\'roomtemp\\': [\\'ì‹¤ë‚´ì˜¨ë„\\'], \\'settemp\\': [\\'ì„¤ì •ì˜¨ë„\\']}};Input:ì§€ê¸ˆ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ì•Œë ¤ì¤˜;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{\"Thinking\": \"ì‚¬ìš©ìëŠ” í˜„ì¬ ìš°ë¦¬ë°˜ì˜ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ì„ ì•Œê³ ì‹¶ì–´í•¨. í˜„ì¬ ìš°ë¦¬ë°˜ì˜ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ì„ ì¿¼ë¦¬í•œ í›„ ë°˜í™˜í•˜ë©´ ë¨.\", \"Expectations\": [\"í˜„ì¬ ìš°ë¦¬ë°˜ì˜ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ì€ {{í˜„ì¬_ìš°ë¦¬ë°˜_ì—ë„ˆì§€_ì‚¬ìš©ëŸ‰}}kWhì…ë‹ˆë‹¤.\"], \"Mapping\": {\"temporal\": {\"í˜„ì¬\": \"LAST_RECORD\"}, \"spatials\": {\"ìš°ë¦¬ë°˜\": \"01_IB5\"}, \"modalities\": {\"ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰\": \"Unknown\"}}}<|eot_id|>', 'input': [{'content': 'ë„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \\nì‚¬ìš©ìì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„ Agentì˜ Instructionsë¥¼ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\\nThinkingì—ì„œëŠ” HVAC ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìœ ì €ì˜ ì˜ë„ë¥¼ ì¶”ì¸¡í•´ ë¹ ì§„ contextë¥¼ ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì˜ ëª¨í˜¸í•¨ì„ ì—†ì•¤ ì™„ë²½í•œ í˜•íƒœì˜ ì§ˆë¬¸ì„ ì¶œë ¥í•˜ê³ , ì´ì— ëŒ€í•œ ë‹µë³€ ê³„íšì„ ì„¸ì›Œì•¼í•¨.\\nExpectationì—ì„œëŠ” ìœ ì €ê°€ ê¸°ëŒ€í• ë§Œí•œ ë‹µë³€ì„ ì¶”ì¸¡í•´ì•¼í•¨.\\nMappingì—ì„œëŠ” ì§ˆë¬¸ì— ì‚¬ìš©ëœ high-level taxonomyë¥¼ metadataë¥¼ ë°”íƒ•ìœ¼ë¡œ low-level taxonomyë¡œì˜ mappingì„ ê³„ì‚°í•˜ì—¬ì•¼ í•¨.\\nì´ë•Œ metadataì— ì—†ëŠ” ì •ë³´ë¥¼ ê¸°ìˆ í•˜ëŠ” ë“± ê±°ì§“ëœ ì¶œë ¥ì„ í•˜ë©´ ì•ˆë˜ê³ , ëª¨ë¥´ëŠ” ì •ë³´ê°€ ìˆìœ¼ë©´ Unknownì´ë¼ ë‹µë³€í•´ì•¼í•¨.\\n\\njsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«í˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.', 'role': 'system'}, {'content': \"Metadata:{'current_datetime': '2022-09-30 12:00:00', 'idu_mapping': {'01_IB5': ['ìš°ë¦¬ë°˜', '4ì¸µ'], '01_IB7': ['ì˜†ë°˜', '4ì¸µ'], '02_I81': ['ì•ë°˜', '4ì¸µ']}, 'modality_mapping': {'roomtemp': ['ì‹¤ë‚´ì˜¨ë„'], 'settemp': ['ì„¤ì •ì˜¨ë„']}};Input:ì§€ê¸ˆ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ì•Œë ¤ì¤˜;\", 'role': 'user'}]}\n",
      "27 12\n"
     ]
    }
   ],
   "source": [
    "from src.dateutils import normalize_sql_dates\n",
    "\n",
    "def read_dataset(dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if train_type in [\"WoThinking\", \"WoMetadata+Thinking\"]:\n",
    "            del d[\"Response\"][\"Thinking\"]\n",
    "        elif train_type in [\"woExp\"]:\n",
    "            del d[\"Response\"][\"Expectations\"]\n",
    "        \n",
    "        if \"Script\" in d[\"Response\"]:\n",
    "            if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "                new_scripts = []\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" in script:\n",
    "                        new_scripts.append(script)\n",
    "                d[\"Response\"][\"Script\"] = new_scripts\n",
    "\n",
    "            if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "                mapping = d[\"Response\"][\"Mapping\"]\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" not in script:\n",
    "                        continue\n",
    "\n",
    "                    t_match = re.search(r\"t=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    s_match = re.search(r\"s=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    m_match = re.search(r\"m=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    t = eval(t_match.group(1)) if t_match else None\n",
    "                    s = eval(s_match.group(1)) if s_match else None\n",
    "                    m = eval(m_match.group(1)) if m_match else None\n",
    "                    \n",
    "                    if isinstance(t, str):\n",
    "                        t = [t]\n",
    "                    if isinstance(s, str):\n",
    "                        s = [s]\n",
    "                    if isinstance(m, str):\n",
    "                        m = [m]\n",
    "\n",
    "                    t_raw = [mapping['temporal'][t_highlevel] for t_highlevel in t]\n",
    "                    s_raw = [mapping['spatials'][s_highlevel] for s_highlevel in s]\n",
    "                    m_raw = [mapping['modalities'][m_highlevel] for m_highlevel in m]\n",
    "                    s_raw = [item for sublist in s_raw for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "                    sql = DBManager.get_query_strings_v2(\n",
    "                        metadata, m_raw, t_raw, s_raw\n",
    "                    )\n",
    "                    sql = normalize_sql_dates(sql)\n",
    "                    # replace data(...) with sql using regex\n",
    "                    d[\"Response\"][\"Script\"][i] = re.sub(r\"data\\(([^)]+)\\)\", lambda x: f\"\\\"{sql}\\\"\", script)\n",
    "                del d[\"Response\"][\"Mapping\"]\n",
    "            #     # raise NotImplementedError\n",
    "            # elif train_type in [\"woOp\"]:\n",
    "            #     instructions = d[\"Response\"][\"Instructions\"]\n",
    "            #     d[\"Response\"][\"Instructions\"] = [i for i in instructions if i[\"type\"] == \"q\"]\n",
    "\n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "\n",
    "        skip = False\n",
    "        for skip_tag in skip_tags:\n",
    "            if skip_tag in tags:\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "dataset_trs = []\n",
    "dataset_tss = []\n",
    "for scenario_dir in scenario_dirs:\n",
    "    dataset_trs.extend(read_dataset(scenario_dir, \"onlyq_tr.json\"))\n",
    "    dataset_tss.extend(read_dataset(scenario_dir, \"onlyq_ts.json\"))\n",
    "    # print(\"Warning!!!: Test set is not mutually exclusive with training set.\")\n",
    "\n",
    "dataset_sampling_rate = 1\n",
    "dataset_trs = np.random.choice(dataset_trs, size=int(len(dataset_trs) * dataset_sampling_rate), replace=False)\n",
    "dataset_trs = dataset_trs.tolist()\n",
    "dataset_tr = Dataset.from_list(dataset_trs) # ì„œë¡œ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„° í•©ì¹˜ë©´ì„œ\n",
    "dataset_ts = Dataset.from_list(dataset_tss) # Mutually exclusiveí•œ ì• ë“¤ì€ None ë¨\n",
    "\n",
    "max_seq_length = 0\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    inputs = []\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for metadata, input, response in zip(examples['Metadata'], examples['Input'], examples['Response']):\n",
    "        # global max_seq_length\n",
    "        response.replace(\"    \", \"\")\n",
    "\n",
    "        # print(metadata['current_datetime'])\n",
    "        # print(metadata['idu_mapping'])\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        if \"llama\" in model_id.lower():\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            content = \"\"\n",
    "            if train_type not in [\"WoMetadata\", \"WoMetadata+Thinking\"]:\n",
    "                content += f\"Metadata:{metadata};\"\n",
    "            content += f\"Input:{input};\"\n",
    "            user_input = {\n",
    "                \"content\": content,\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "            inputs.append([prompt, user_input])\n",
    "        elif \"gemma\" in model_id.lower():\n",
    "            user_input = {\n",
    "                \"content\": f\"{common_prompt};{metadata};{input}\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([user_input, answer])\n",
    "        else:\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            user_input = {\n",
    "                \"content\": f\"Metadata:{metadata};Input:{input};\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        # print(\"Answer length: \", len(response))\n",
    "        # convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        # if len(response) + 50 > max_seq_length:\n",
    "        #     max_seq_length = len(response) + len(metadata) + len(input) + 50\n",
    "            # print(response)\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    \n",
    "    # remove \\n\\nCutting Knowledge Date: BLAH BLAH \\nToday Date: BLAH BLAH\\n\\n using regex\n",
    "    # texts = [re.sub(r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', '', text) for text in texts]\n",
    "    return {\"text\": texts, \"input\": inputs}\n",
    "\n",
    "dataset_tr = dataset_tr.map(formatting_prompts_func, batched=True)\n",
    "dataset_ts = dataset_ts.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "max_seq_length = 10000\n",
    "max_seq_length = max([len(tokenizer.encode(dataset_tr[i]['text'])) for i in range(len(dataset_tr))]) + 100\n",
    "# max_seq_length += len(common_prompt)\n",
    "print(max_seq_length)\n",
    "print(dataset_tr[0])\n",
    "print(len(dataset_tr), len(dataset_ts))\n",
    "# print(f\"seq length: {len(tokenizer.encode(dataset_tr[0]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Metadata', 'Input', 'Response', 'text', 'input'],\n",
      "    num_rows: 12\n",
      "})\n",
      "\"v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-29') AND timestamp < DATE_TRUNC('week', DATE '2022-09-29' + INTERVAL '1 week') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\", \"v_ì´ë²ˆì£¼_ì•ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-29') AND timestamp < DATE_TRUNC('week', DATE '2022-09-29' + INTERVAL '1 week') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_ìš°ë¦¬ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE_TRUNC('month', DATE '2022-09-29') - INTERVAL '1 month' AND timestamp < DATE_TRUNC('month', DATE '2022-09-29') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-29') AND timestamp < DATE_TRUNC('week', DATE '2022-09-29' + INTERVAL '1 week') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\", \"v_ì´ë²ˆì£¼_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-29') AND timestamp < DATE_TRUNC('week', DATE '2022-09-29' + INTERVAL '1 week') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_2ì£¼ì „_ìš°ë¦¬ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-29' - INTERVAL '2 week') AND timestamp < DATE_TRUNC('week', DATE '2022-09-29' - INTERVAL '1 week') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì§€ë‚œ3ì¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-29' - INTERVAL '3 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-29') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì˜¤ëŠ˜ì˜¤í›„5ì‹œ_ì˜†ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp = DATE_TRUNC('day', DATE '2022-09-29') + INTERVAL '17 hours' AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì˜¬í•´ì—¬ë¦„_ìš°ë¦¬ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE '2022-06-01' AND timestamp < DATE '2022-09-01' AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_ë°©ë³„_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81', '01_IB7')) AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_4ì¸µ_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81', '01_IB5', '01_IB7')) AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\", \"v_í˜„ì¬_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_4ì›”_ì•ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE '2022-04-01' AND timestamp < DATE '2022-05-01' AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('month', DATE '2022-09-30') AND timestamp < DATE_TRUNC('month', DATE '2022-09-30' + INTERVAL '1 month') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_ì˜†ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-30') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30' + INTERVAL '1 week') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì§€ë‚œë‹¬ì˜¤ëŠ˜ì˜¤í›„2ì‹œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND timestamp = DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 month') + INTERVAL '14 hours' AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì˜¬í•´ì—¬ë¦„_ì•ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('02_I81')) AND timestamp >= DATE '2022-06-01' AND timestamp < DATE '2022-09-01' AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì˜¤í›„4ì‹œë¶€í„°6ì‹œ_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '16 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '18 hours' AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_ì‘ë…„ê²¨ìš¸_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE '2022-12-01' - INTERVAL '1 year' AND timestamp < DATE '2022-03-01' AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_4ì¸µ_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5', '01_IB7', '02_I81')) AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_ì–´ì œ_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\", \"v_ì–´ì œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-30') - INTERVAL '1 week' AND timestamp < DATE_TRUNC('week', DATE '2022-09-30') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì˜¤ì „11ì‹œ_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND timestamp = DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '11 hours' AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_í˜„ì¬_ë°©ë³„_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5', '01_IB7', '02_I81')) AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' AND timestamp = (SELECT MAX(timestamp) FROM \\\"data_t\\\") ORDER BY timestamp\\\"\"\n",
      "\"v_ì˜¤ëŠ˜_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30' + INTERVAL '1 day') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\", \"v_ì˜¤ëŠ˜_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30' + INTERVAL '1 day') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_8ì¼ì „_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '8 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '7 day') AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('month', DATE '2022-09-30') AND timestamp < DATE_TRUNC('month', DATE '2022-09-30' + INTERVAL '1 month') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì‘ë…„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND timestamp >= DATE_TRUNC('year', DATE '2022-09-30' - INTERVAL '1 year') AND timestamp < DATE_TRUNC('year', DATE '2022-09-30') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_2ì£¼ì „_ìš°ë¦¬ë°˜ê³¼ì˜†ë°˜_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5', '01_IB7')) AND timestamp >= DATE_TRUNC('week', DATE '2022-09-30' - INTERVAL '2 week') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30' - INTERVAL '1 week') AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"settemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('month', DATE '2022-09-30') - INTERVAL '1 month' AND timestamp < DATE_TRUNC('month', DATE '2022-09-30') AND \\\"settemp\\\" IS NOT NULL AND \\\"settemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_ì˜¬í•´ë´„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')) AND timestamp >= DATE '2022-03-01' AND timestamp < DATE '2022-06-01' AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n",
      "\"v_10ë…„ì „ì˜¤ëŠ˜_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_df = \\\"SELECT (SELECT name FROM idu_t WHERE id = data_t.idu_id) AS idu_name, \\\"roomtemp\\\", \\\"timestamp\\\" FROM \\\"data_t\\\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')) AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '10 year') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '10 year' + INTERVAL '1 day') AND \\\"roomtemp\\\" IS NOT NULL AND \\\"roomtemp\\\" IS DISTINCT FROM 'NaN' ORDER BY timestamp\\\"\"\n"
     ]
    }
   ],
   "source": [
    "# print(dataset_ts, )\n",
    "\n",
    "# for t in dataset_ts['text'] + dataset_tr['text']:\n",
    "#     # print(t)\n",
    "\n",
    "    \n",
    "#     # from t regex \"Script\": [(text)]\n",
    "\n",
    "#     match = re.search(r'\"Script\": \\[(.*?)\\]', t)\n",
    "#     if match:\n",
    "#         script = match.group(1)\n",
    "#         print(script)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v7_r211_a422_woQM+Script_tr27_0623\n"
     ]
    }
   ],
   "source": [
    "lora_r = 211\n",
    "lora_alpha = lora_r * 2\n",
    "lora_repr = f\"v7_r{lora_r}_a{lora_alpha}_{train_type}_tr{len(dataset_tr)}_0623\"\n",
    "print(lora_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.2 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    pretrained_model,\n",
    "    r=lora_r,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    # \"embed_tokens\",\n",
    "                    # \"lm_head\"\n",
    "                    ],\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None,   # No LoftQ, for standard fine-tuning\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "# del pretrained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(len(dataset_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(f\"{model_dir}/chkpts/{lora_repr}\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(text: str):\n",
    "    \"\"\"Extract content from model output.\"\"\"\n",
    "    if \"start_header_id\" in text:\n",
    "        pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "    elif \"start_of_turn\" in text:\n",
    "        pattern = r\"<start_of_turn>model\\n(.*?)<eos>\"\n",
    "    elif \"im_start\" in text:\n",
    "        # <|im_start|>assistant{\"Thinking\": \"ì‚¬ìš©ìëŠ” ì˜¤ëŠ˜ 4ì¸µì— ìˆëŠ” ëª¨ë“  ë°©ì˜ ì„¤ì •ì˜¨ë„ì˜ í‰ê· ê°’ì„ ì•Œê³  ì‹¶ì–´í•©ë‹ˆë‹¤. 4ì¸µì— í•´ë‹¹í•˜ëŠ” iduë“¤(01_IB7, 02_I84, 02_I85)ì˜ ì˜¤ëŠ˜ ì„¤ì •ì˜¨ë„ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•œ í›„ í‰ê· ê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.\", \"Expectations\": [\"ì˜¤ëŠ˜ 4ì¸µì˜ í‰ê·  ì„¤ì •ì˜¨ë„ëŠ” {{settemp_avg}}â„ƒ ì…ë‹ˆë‹¤.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"table_name\": \"data_t\", \"columns\": [\"settemp\"], \"temporal\": \"[DATE_TRUNC('day', DATE 'CURRENT_DATE'), DATE_TRUNC('day', DATE 'CURRENT_DATE' + INTERVAL '1 day'))\", \"spatials\": [\"01_IB7\", \"02_I84\", \"02_I85\"]}, \"result_name\": \"qr\"}, {\"type\": \"o\", \"script\": \"settemp_avg = qr['settemp'].mean();\", \"returns\": [\"settemp_avg\"]}]}<|im_end|>\n",
    "        pattern = r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\"\n",
    "    elif \"|endofturn|\" in text:\n",
    "        pattern = r\"\\[\\|assistant\\|\\](.*?)\\[\\|endofturn\\|\\]\"\n",
    "    \n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    result = match.group(1).strip() if match else None\n",
    "    result = result.replace(\"<|finetune_right_pad_id|>\", \"\")\n",
    "    # result = result.replace(\"<|start_header_id|>\", \"\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Accumulation Steps: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca6fb09933f46b08243b017363af49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "per_device_train_batch_size, epochs = 60, 60 # 8\n",
    "gradient_accumulation_steps = int(np.ceil(len(dataset_tr) / per_device_train_batch_size))\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "# clear all checkpoints\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # num_train_epochs = 1,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,  # Controls the batch size per device\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # Accumulates gradients to simulate a larger batch\n",
    "    max_steps=gradient_accumulation_steps * epochs,\n",
    "    # ë¦¬ì†ŒìŠ¤ ì œì•½ë•Œë¬¸ì— batch sizeë¥¼ íƒ€í˜‘í•´ì•¼í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒ -> micro batch sizeë¥¼ ì¤„ì´ê³ ,\n",
    " \t# accumulated stepì„ ëŠ˜ë ¤, ì ì ˆí•œ sizeë¡œ gradientë¥¼ êµ¬í•´ weight update\n",
    "    # https://www.youtube.com/watch?v=ptlmj9Y9iwE\n",
    "    warmup_steps = gradient_accumulation_steps,\n",
    "    learning_rate = 1e-4,             # Sets the learning rate for optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_torch\", # adamw_torch, adafactor, prodigy\n",
    "    weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "    lr_scheduler_type = \"cosine\",  # Sets the learning rate scheduler\n",
    "    seed = 3407,                        \n",
    "    output_dir = f\"{model_dir}/chkpts/{lora_repr}\",  # Output directory for checkpoints and predictions     \n",
    "    report_to = \"none\",              # Enables Weights & Biases (W&B) logging\n",
    "    logging_steps = gradient_accumulation_steps,                # Sets frequency of logging to W&B\n",
    "    logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "    # eval_strategy=\"steps\",  # enable evaluation during training\n",
    "    # eval_steps=gradient_accumulation_steps,\n",
    "    # eval_accumulation_steps=1, # ë‚®ì„ìˆ˜ë¡ evalì‹œ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ì¤„ì–´ë“¦\n",
    "    # save_steps=gradient_accumulation_steps,\n",
    "    # save_strategy = \"steps\",               \n",
    "    # load_best_model_at_end = True,    # Loads the best model at the end\n",
    "    # save_only_model = False,           # Saves entire model, not only weights\n",
    "    # resume_from_checkpoint = f\"{model_dir}/chkpts/{lora_repr}\",  # Resumes training from a checkpoint\n",
    ")\n",
    "\n",
    "# metadata_str = {\"idu_mapping\": {\"01_IB5\": [\"ìš°ë¦¬ë°˜\", \"4ì¸µ\"], \"01_IB7\": [\"ì˜†ë°˜\", \"4ì¸µ\"], \"02_I81\": [\"ì•ë°˜\", \"4ì¸µ\"]}, \"modality_mapping\": {\"roomtemp\": [\"ì‹¤ë‚´ì˜¨ë„\"], \"settemp\": [\"ì„¤ì •ì˜¨ë„\"]}, \"current_datetime\": \"2022-09-30 12:00:00\"}\n",
    "metadata_str = {\n",
    "    \"idu_mapping\": {\"01_IB5\": [\"ì˜†ë°˜\", \"4ì¸µ\"], \"01_IB7\": [\"ì•ë°˜\", \"4ì¸µ\"], \"02_I81\": [\"ìš°ë¦¬ë°˜\", \"4ì¸µ\"]},\n",
    "    \"modality_mapping\": {\"roomtemp\": [\"ì‹¤ë‚´ì˜¨ë„\"], \"settemp\": [\"ì„¤ì •ì˜¨ë„\"]},\n",
    "    \"current_datetime\": \"2022-09-29 21:30:00\"\n",
    "}\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, model, eval_inputs, inputs, save_dir=\"./eval_generations\", max_new_tokens=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        \n",
    "        convos = []\n",
    "        for chat in eval_inputs:\n",
    "            convos.append(tokenizer.apply_chat_template(\n",
    "                chat,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device))\n",
    "        \n",
    "        padded_inputs = []\n",
    "        attention_masks = []\n",
    "        \n",
    "        max_length = max(cn.size(1) for cn in convos)\n",
    "        for cn in convos:\n",
    "            pad_length = max_length - cn.size(1)\n",
    "            \n",
    "            if pad_length > 0:\n",
    "                # íŒ¨ë”© ì¶”ê°€\n",
    "                padded = torch.cat([\n",
    "                    torch.full((1, pad_length), tokenizer.pad_token_id, device=model.device),\n",
    "                    cn,\n",
    "                ], dim=1)\n",
    "                \n",
    "                # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (ì›ë³¸ ì‹œí€€ìŠ¤ëŠ” 1, íŒ¨ë”©ì€ 0)\n",
    "                mask = torch.cat([\n",
    "                    torch.zeros(1, pad_length, device=model.device),\n",
    "                    torch.ones(1, cn.size(1), device=model.device),\n",
    "                ], dim=1)\n",
    "            else:\n",
    "                padded = cn\n",
    "                mask = torch.ones(1, cn.size(1), device=model.device)\n",
    "            \n",
    "            padded_inputs.append(padded)\n",
    "            attention_masks.append(mask)\n",
    "        \n",
    "        # ë°°ì¹˜ í…ì„œ ìƒì„±\n",
    "        self.batch_tensor = torch.cat(padded_inputs, dim=0)\n",
    "        self.attention_mask = torch.cat(attention_masks, dim=0)\n",
    "        self.inputs = inputs\n",
    "        self.save_dir = save_dir\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        step = state.global_step\n",
    "        if step == 0:\n",
    "            return  # skip at step 0\n",
    "        \n",
    "        loss = logs.get(\"loss\", None)\n",
    "        if loss > 0.009 and step < 50:\n",
    "            return\n",
    "        # inputs = self.tokenizer(\n",
    "        #     self.eval_inputs, \n",
    "        #     return_tensors=\"pt\",\n",
    "        #     padding=True,\n",
    "        #     # truncation=True,\n",
    "        # ).to(self.model.device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            FastLanguageModel.for_inference(self.model)\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=self.batch_tensor,\n",
    "                attention_mask=self.attention_mask,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        decoded = self.tokenizer.batch_decode(\n",
    "            outputs, \n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "\n",
    "        # Save to file\n",
    "        output_path = os.path.join(self.save_dir, f\"r-{lora_repr}-step-{step}.json\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            # f.write(\"[\\n\")\n",
    "            j = []\n",
    "            for i, out in enumerate(decoded):\n",
    "                try:\n",
    "                    out = extract_content(out)\n",
    "                    out = eval(out)\n",
    "                except:\n",
    "                    pass\n",
    "                result = {\n",
    "                    \"Input\": self.inputs[i],\n",
    "                    \"Scenario\": \"scenario2\",\n",
    "                    \"Metadata\": metadata_str,\n",
    "                    \"Candidate\": out\n",
    "                }\n",
    "                j.append(result)\n",
    "            f.write(json.dumps(j, ensure_ascii=False))\n",
    "        FastLanguageModel.for_training(self.model)\n",
    "\n",
    "custom_callback = CustomCallback(\n",
    "    tokenizer=tokenizer,\n",
    "    model=peft_model,\n",
    "    eval_inputs=dataset_ts['input'],  # \"input\"ì— prompt ë˜ëŠ” ì§ˆë¬¸ì´ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤\n",
    "    inputs=dataset_ts['Input'],\n",
    "    save_dir=f\"../experiments\",\n",
    "    max_new_tokens=1280\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_tr,\n",
    "    # eval_dataset = dataset_ts,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,        # Can make training 5x faster for short sequences.\n",
    "    args = args,\n",
    "    # compute_metrics = compute_metrics,\n",
    "    callbacks = [custom_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 27 | Num Epochs = 60 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 60 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (60 x 1 x 1) = 60\n",
      " \"-____-\"     Trainable parameters = 553,123,840/8,583,385,088 (6.44% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/60 05:05 < 00:28, 0.17 it/s, Epoch 54/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.148300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.038300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43munsloth_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/unsloth/trainer.py:45\u001b[39m, in \u001b[36munsloth_train\u001b[39m\u001b[34m(trainer, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munsloth_train\u001b[39m(trainer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:314\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/LGHVAC_2ndyear/src/unsloth_compiled_cache/UnslothSFTTrainer.py:846\u001b[39m, in \u001b[36m_UnslothSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m--> \u001b[39m\u001b[32m846\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:77\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:2454\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base model\n",
    "\n",
    "# save_dir = f\"{model_dir}/gguf\"\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# command = (\n",
    "#     f\"python ../../llama.cpp/convert_hf_to_gguf.py \"\n",
    "#     # f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "#     f\"--outfile {save_dir}/base.gguf \"              # Output file for the GGUF model\n",
    "#     f\"--outtype f16 \"                      # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "#     f\"--verbose \"                            # Optional: increase logging output\n",
    "#     f\"{model_dir}\"                      # Positional argument: path to the LoRA adapter files\n",
    "# )\n",
    "\n",
    "# print(command)\n",
    "\n",
    "# os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56\n",
      "/model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56\n"
     ]
    }
   ],
   "source": [
    "step = 56\n",
    "\n",
    "# lora_repr = \"v7_r8_a16_ours_70B\"\n",
    "checkpoint_dir = f\"{model_dir}/chkpts/{lora_repr}/checkpoint-{step}\"\n",
    "output_path = f\"{model_dir}/gguf/{lora_repr}-checkpoint-{step}.gguf\"\n",
    "lora_output_dir = f\"{model_dir}/lora_output/\"\n",
    "\n",
    "if not os.path.exists(f\"{model_dir}/gguf\"):\n",
    "    os.makedirs(f\"{model_dir}/gguf\")\n",
    "print(checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(lora_output_dir):\n",
    "    os.makedirs(lora_output_dir)\n",
    "print(checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56/*.json (invalid repository id)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:125\u001b[39m, in \u001b[36mHfFileSystem._repo_and_revision_exist\u001b[39m\u001b[34m(self, repo_type, repo_id, revision)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHF_HUB_ETAG_TIMEOUT\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (RepositoryNotFoundError, HFValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '/model'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m#     checkpoint_dir,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m#     local_files_only=True\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     peft_model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/cache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# FastLanguageModel.for_inference(model)\u001b[39;00m\n\u001b[32m     29\u001b[39m     tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/unsloth/models/loader.py:199\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m     both_exist = exist_adapter_config \u001b[38;5;129;01mand\u001b[39;00m exist_config\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# Because HfFileSystem assumes linux paths, we need to set the path with forward slashes, even on Windows.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     files = \u001b[43mHfFileSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/*.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     files = (os.path.split(x)[-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files)\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(x == \u001b[33m\"\u001b[39m\u001b[33madapter_config.json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x == \u001b[33m\"\u001b[39m\u001b[33mconfig.json\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files) >= \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:520\u001b[39m, in \u001b[36mHfFileSystem.glob\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Set expand_info=False by default to get a x10 speed boost\u001b[39;00m\n\u001b[32m    519\u001b[39m kwargs = {\u001b[33m\"\u001b[39m\u001b[33mexpand_info\u001b[39m\u001b[33m\"\u001b[39m: kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdetail\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m), **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrevision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.unresolve()\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().glob(path, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:216\u001b[39m, in \u001b[36mHfFileSystem.resolve_path\u001b[39m\u001b[34m(self, path, revision)\u001b[39m\n\u001b[32m    214\u001b[39m     repo_and_revision_exist, _ = \u001b[38;5;28mself\u001b[39m._repo_and_revision_exist(repo_type, repo_id, revision)\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[43m_raise_file_not_found\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    218\u001b[39m     _raise_file_not_found(path, err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/hf_file_system.py:1136\u001b[39m, in \u001b[36m_raise_file_not_found\u001b[39m\u001b[34m(path, err)\u001b[39m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, HFValidationError):\n\u001b[32m   1135\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (invalid repository id)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: /model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r256_a512_ours_tr17_0613/checkpoint-56/*.json (invalid repository id)"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     torch_dtype=torch_dtype,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     # attn_implementation=attn_implementation,\n",
    "    #     local_files_only=True,\n",
    "    #     device_map=\"cuda\"\n",
    "    # )\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     local_files_only=True\n",
    "    # )\n",
    "    \n",
    "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        dtype = torch_dtype,\n",
    "        attn_implementation=attn_implementation,\n",
    "        load_in_4bit = False,\n",
    "        load_in_8bit=False,\n",
    "        cache_dir=f\"{model_dir}/cache\",\n",
    "        local_files_only=True,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    # FastLanguageModel.for_inference(model)\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # merge lora model and base pretrained model\n",
    "    # model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# peft_model.save_pretrained_gguf(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, quantization_method = \"q8_0\")\n",
    "peft_model.save_pretrained_merged(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, save_method=\"merged_16bit\")\n",
    "# tokenizer.save_pretrained(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# command = (\n",
    "#     f\"python ../../llama.cpp/convert_lora_to_gguf.py \"\n",
    "#     f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "#     f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "#     f\"--outtype f16 \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "#     f\"--verbose \"                            # Optional: increase logging output\n",
    "#     f\"{checkpoint_dir}\"                      # Positional argument: path to the LoRA adapter files\n",
    "# )\n",
    "\n",
    "command = (\n",
    "    f\"python ../../llama.cpp/convert_hf_to_gguf.py \"\n",
    "    # f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "    f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "    f\"--outtype f16 \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "    f\"--verbose \"                            # Optional: increase logging output\n",
    "    f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\"                      # Positional argument: path to the LoRA adapter files\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = (\n",
    "#     f\"../../llama.cpp/build/bin/llama-quantize \"\n",
    "#     f\"{output_path} \"\n",
    "#     f\"{output_path.replace('.gguf', '-Q4_K_M.gguf')} \"                      # Positional argument: path to the LoRA adapter files\n",
    "#     f\"Q4_K_M \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"Running command:\", command)\n",
    "# os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy(\n",
    "    f\"{output_path}\",\n",
    "    f\"../../src/i2i.gguf\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -cvf - ../../src/i2i.gguf | pigz -p 128 > src/i2i.tar.gz\n",
    "\n",
    "command = (\n",
    "    f\"tar -cvf - ../../src/i2i.gguf | pigz -p 128 > ../../src/i2i.tar.gz\"\n",
    ")\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
