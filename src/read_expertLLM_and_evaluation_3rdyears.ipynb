{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.1+cu128 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.db.manager import DBManager\n",
    "from src.input_to_instructions.load_and_execute import *\n",
    "from src.input_to_instructions.types import *\n",
    "from src.operation.execute import *\n",
    "from src.response_generation.load_and_execute import *\n",
    "from src.dateutils import normalize_sql_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "# from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\"\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_implementation: flash_attention_2, torch_dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "print(f\"attn_implementation: {attn_implementation}, torch_dtype: {torch_dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DATASET_DIR: ../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall\n",
      "[PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/prompt.txt'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/refine.ipynb'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario2'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/summary.ipynb')]\n",
      "ë„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \n",
      "ì‚¬ìš©ìì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„ Agentì˜ Instructionsë¥¼ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\n",
      "Thinkingì—ì„œëŠ” HVAC ìƒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ìœ ì €ì˜ ì˜ë„ë¥¼ ì¶”ì¸¡í•´ ë¹ ì§„ contextë¥¼ ì¶”ì¸¡í•˜ì—¬ ì§ˆë¬¸ì˜ ëª¨í˜¸í•¨ì„ ì—†ì•¤ ì™„ë²½í•œ í˜•íƒœì˜ ì§ˆë¬¸ì„ ì¶œë ¥í•˜ê³ , ì´ì— ëŒ€í•œ ë‹µë³€ ê³„íšì„ ì„¸ì›Œì•¼í•¨.\n",
      "Expectationì—ì„œëŠ” ìœ ì €ê°€ ê¸°ëŒ€í• ë§Œí•œ ë‹µë³€ì„ ì¶”ì¸¡í•´ì•¼í•¨.\n",
      "Mappingì—ì„œëŠ” ì§ˆë¬¸ì— ì‚¬ìš©ëœ high-level taxonomyë¥¼ metadataë¥¼ ë°”íƒ•ìœ¼ë¡œ low-level taxonomyë¡œì˜ mappingì„ ê³„ì‚°í•˜ì—¬ì•¼ í•¨.\n",
      "ì´ë•Œ metadataì— ì—†ëŠ” ì •ë³´ë¥¼ ê¸°ìˆ í•˜ëŠ” ë“± ê±°ì§“ëœ ì¶œë ¥ì„ í•˜ë©´ ì•ˆë˜ê³ , ëª¨ë¥´ëŠ” ì •ë³´ê°€ ìˆìœ¼ë©´ Unknownì´ë¼ ë‹µë³€í•´ì•¼í•¨.\n",
      "Scriptì—ì„œëŠ” data í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ pandas dataframe í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•˜ì—¬ ë‹µë³€ì— í•„ìš”í•œ ì—°ì‚°ì„ python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§œ ìˆ˜í–‰í•œë‹¤. ì´ë•Œ ì‹¤í–‰ ì—ëŸ¬ì— ì¡°ì‹¬í•œë‹¤. python\n",
      "Expectationì˜ ëª…ì‹œëœ ëª¨ë“  variableì´ scriptì—ì„œ ê³„ì‚°ë˜ì•¼ í•œë‹¤.\n",
      "jsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«í˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.\n",
      "[PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1'), PosixPath('../finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario2')]\n"
     ]
    }
   ],
   "source": [
    "# BASE_DATASET_DIR = Path(\"../dataset/v5-250228-multimetadata\")\n",
    "# dataset_name = \"v6-250306-optimizetoken\"\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "BASE_DATASET_DIR = Path(f\"../finetuning/dataset/{dataset_name}\")\n",
    "print(f\"BASE_DATASET_DIR: {BASE_DATASET_DIR}\")\n",
    "print(list(BASE_DATASET_DIR.iterdir()))\n",
    "\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "train_type = \"ours\"\n",
    "def sub(name, common_prompt):\n",
    "    # Remove the section between <|name|> ... <|name|> including the tags themselves\n",
    "    # Use re.DOTALL to match newlines with '.'\n",
    "    pattern = rf\"\\n?<\\|{name}\\|>[\\s\\S]*?<\\|{name}\\|>\"\n",
    "    common_prompt = re.sub(pattern, \"\", common_prompt, flags=re.DOTALL)\n",
    "    return common_prompt\n",
    "\n",
    "common_prompt = open(BASE_DATASET_DIR / f\"prompt.txt\", \"r\").read()\n",
    "\n",
    "sub_targets = []\n",
    "if train_type == \"ours\":\n",
    "    sub_targets = []\n",
    "elif train_type == \"BASE\":\n",
    "    sub_targets = [\"Thinking\", \"Expectation\", \"Mapping\", \"Script\", \"Examples\"]\n",
    "elif train_type in [\"WoThinking\"]:\n",
    "    sub_targets = [\"Thinking\"]\n",
    "elif train_type in [\"woMetadata\"]:\n",
    "    sub_targets = [\"Metadata\"]\n",
    "elif train_type in [\"WoMetadata+Thinking\"]:\n",
    "    sub_targets = [\"Metadata\", \"Thinking\"]\n",
    "elif train_type in [\"woExp\"]:\n",
    "    sub_targets = [\"Expectation\"]\n",
    "\n",
    "if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "    sub_targets = [\"QM\", \"Mapping\"]\n",
    "if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "    sub_targets = [\"Script\"]\n",
    "\n",
    "\n",
    "for sub_target in sub_targets:\n",
    "    common_prompt = sub(sub_target, common_prompt)\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "print(common_prompt)\n",
    "\n",
    "scenario_dirs = [d for d in BASE_DATASET_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]\n",
    "print(scenario_dirs)\n",
    "\n",
    "from src.dateutils import normalize_sql_dates\n",
    "\n",
    "def read_dataset(dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if train_type in [\"WoThinking\", \"WoMetadata+Thinking\"]:\n",
    "            del d[\"Response\"][\"Thinking\"]\n",
    "        elif train_type in [\"woExp\"]:\n",
    "            del d[\"Response\"][\"Expectations\"]\n",
    "        \n",
    "        if \"Script\" in d[\"Response\"]:\n",
    "            if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "                new_scripts = []\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" in script:\n",
    "                        new_scripts.append(script)\n",
    "                d[\"Response\"][\"Script\"] = new_scripts\n",
    "\n",
    "            if train_type in [\"woQM\"]:\n",
    "                new_scripts = []\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" in script:\n",
    "                        new_scripts.append(script)\n",
    "                    else:\n",
    "                        for m, n in [(\"ì‹¤ë‚´ì˜¨ë„\", \"roomtemp\"), (\"ì„¤ì •ì˜¨ë„\", \"settemp\")]:\n",
    "                            script = script.replace(f\"'{m}'\", f\"'{n}'\")\n",
    "                        \n",
    "                        new_scripts.append(script)\n",
    "                d[\"Response\"][\"Script\"] = new_scripts\n",
    "\n",
    "            if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "                mapping = d[\"Response\"][\"Mapping\"]\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" not in script:\n",
    "                        continue\n",
    "\n",
    "                    t_match = re.search(r\"t=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    s_match = re.search(r\"s=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    m_match = re.search(r\"m=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    t = eval(t_match.group(1)) if t_match else None\n",
    "                    s = eval(s_match.group(1)) if s_match else None\n",
    "                    m = eval(m_match.group(1)) if m_match else None\n",
    "                    \n",
    "                    if isinstance(t, str):\n",
    "                        t = [t]\n",
    "                    if isinstance(s, str):\n",
    "                        s = [s]\n",
    "                    if isinstance(m, str):\n",
    "                        m = [m]\n",
    "\n",
    "                    t_raw = [mapping['temporal'][t_highlevel] for t_highlevel in t]\n",
    "                    s_raw = [mapping['spatials'][s_highlevel] for s_highlevel in s]\n",
    "                    m_raw = [mapping['modalities'][m_highlevel] for m_highlevel in m]\n",
    "                    s_raw = [item for sublist in s_raw for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "                    sql = DBManager.get_query_strings_v2(\n",
    "                        metadata, m_raw, t_raw, s_raw\n",
    "                    )\n",
    "                    sql = normalize_sql_dates(sql)\n",
    "                    # replace data(...) with sql using regex\n",
    "                    d[\"Response\"][\"Script\"][i] = re.sub(r\"data\\(([^)]+)\\)\", lambda x: f\"\\\"{sql}\\\"\", script)\n",
    "                del d[\"Response\"][\"Mapping\"]\n",
    "            #     # raise NotImplementedError\n",
    "            # elif train_type in [\"woOp\"]:\n",
    "            #     instructions = d[\"Response\"][\"Instructions\"]\n",
    "            #     d[\"Response\"][\"Instructions\"] = [i for i in instructions if i[\"type\"] == \"q\"]\n",
    "\n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "\n",
    "        skip = False\n",
    "        for skip_tag in skip_tags:\n",
    "            if skip_tag in tags:\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Scenarios\": dir.name, \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "dataset_tss = []\n",
    "for scenario_dir in scenario_dirs:\n",
    "    dataset_tss.extend(read_dataset(scenario_dir, \"onlyq_ts.json\"))\n",
    "    # print(\"Warning!!!: Test set is not mutually exclusive with training set.\")\n",
    "\n",
    "dataset_ts_ = Dataset.from_list(dataset_tss) # Mutually exclusiveí•œ ì• ë“¤ì€ None ë¨\n",
    "\n",
    "# print(common_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load export LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ids = [\n",
    "#     '3rdyear_r211_a422_sh2orc-Llama-3.1-Korean-8B-Instruct_tr27',\n",
    "#     # 'Bllossom/llama-3.2-Korean-Bllossom-AICA-5B',\n",
    "#     # 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# ]\n",
    "\n",
    "bit = [4, 8, 16][2]\n",
    "\n",
    "model_id = \"sh2orc-Llama-3.1-Korean-8B-Instruct\"\n",
    "model_repr = '3rdyear_r211_a422_sh2orc-Llama-3.1-Korean-8B-Instruct_tr27'\n",
    "checkpoint_num = 41\n",
    "\n",
    "# model_id = \"Bllossom-llama-3.2-Korean-Bllossom-3B\"\n",
    "# model_repr = '3rdyear_r450_a900_Bllossom-llama-3.2-Korean-Bllossom-3B_tr27'\n",
    "# checkpoint_num = 70\n",
    "\n",
    "\n",
    "expertLLM_dir = Path(\"/model\") / model_id\n",
    "checkpoint_dir = expertLLM_dir / f\"chkpts/{model_repr}/checkpoint-{checkpoint_num}\"\n",
    "cache_dir = expertLLM_dir / \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class GPUMonitor:\n",
    "    def __init__(self):\n",
    "        self.max_memory = 0\n",
    "        self.start_memory = 0\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        pynvml.nvmlInit()\n",
    "        self.handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "    \n",
    "    def get_memory(self):\n",
    "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)\n",
    "        used_gb = mem_info.used / 1024**3\n",
    "        return used_gb\n",
    "\n",
    "    def _monitor(self):\n",
    "        while self.running:\n",
    "            used_gb = self.get_memory()\n",
    "            self.max_memory = max(self.max_memory, used_gb)\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    def start(self, start_memory=None):\n",
    "        # ì‹œì‘ ì‹œì  ë©”ëª¨ë¦¬ ì¸¡ì •\n",
    "        if start_memory is not None:\n",
    "            self.start_memory = start_memory\n",
    "        else:\n",
    "            self.start_memory = self.get_memory()\n",
    "        self.max_memory = self.start_memory\n",
    "        \n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "        # ìµœëŒ€ê°’ì—ì„œ ì‹œì‘ê°’ì„ ëº€ ì¦ê°€ëŸ‰ ë°˜í™˜\n",
    "        return round(self.max_memory - self.start_memory, 2)\n",
    "    \n",
    "    def __del__(self):\n",
    "        pynvml.nvmlShutdown()\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "monitor = GPUMonitor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc44465b6ed946ec98672b6d77331997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.1 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006a1cf0281a465da59efdc317e4b153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting individual inference for 12 samples...\n",
      "------------------------------------------------------------\n",
      "Repeat 1/3\n",
      "Sample 1/12\n",
      "Sample 2/12\n",
      "Sample 3/12\n",
      "Sample 4/12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     89\u001b[39m start_time = time.time()\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     92\u001b[39m     \u001b[38;5;66;03m# ê°œë³„ ìƒ˜í”Œ ì¶”ë¡ \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# ì‹œê°„ ì¸¡ì • ì¢…ë£Œ\u001b[39;00m\n\u001b[32m    101\u001b[39m inference_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/peft/peft_model.py:1973\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1972\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1973\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1975\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/unsloth/models/llama.py:1740\u001b[39m, in \u001b[36munsloth_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[32m   1739\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(), torch.autocast(device_type = DEVICE_TYPE, dtype = dtype):\n\u001b[32m-> \u001b[39m\u001b[32m1740\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/generation/utils.py:2634\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2626\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2627\u001b[39m         input_ids=input_ids,\n\u001b[32m   2628\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2629\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2630\u001b[39m         **model_kwargs,\n\u001b[32m   2631\u001b[39m     )\n\u001b[32m   2633\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2634\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2645\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2646\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2647\u001b[39m         input_ids=input_ids,\n\u001b[32m   2648\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2649\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2650\u001b[39m         **model_kwargs,\n\u001b[32m   2651\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/transformers/generation/utils.py:3618\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3616\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3617\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3618\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3620\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3621\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3622\u001b[39m     outputs,\n\u001b[32m   3623\u001b[39m     model_kwargs,\n\u001b[32m   3624\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3625\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/unsloth/models/llama.py:1125\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_CausalLM_fast_forward\u001b[39m(\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1109\u001b[39m     input_ids: torch.LongTensor = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1122\u001b[39m     *args, **kwargs,\n\u001b[32m   1123\u001b[39m ) -> Union[Tuple, CausalLMOutputWithPast]:\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m         outputs = \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1133\u001b[39m         causal_mask = xformers.attn_bias.LowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/unsloth/models/llama.py:1050\u001b[39m, in \u001b[36m_LlamaModel_fast_forward_inference.<locals>.LlamaModel_fast_forward_inference_custom\u001b[39m\u001b[34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[39m\n\u001b[32m   1046\u001b[39m device_index = \u001b[38;5;28mgetattr\u001b[39m(decoder_layer, \u001b[33m\"\u001b[39m\u001b[33m_per_layer_device_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m   1047\u001b[39m X, residual, position_ids = move_to_device(\n\u001b[32m   1048\u001b[39m     device_index, X, residual, position_ids\n\u001b[32m   1049\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m \u001b[43mresidual\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[32m   1051\u001b[39m X = fast_rms_layernorm_inference(\n\u001b[32m   1052\u001b[39m     decoder_layer.input_layernorm,\n\u001b[32m   1053\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1056\u001b[39m     variance = variance,\n\u001b[32m   1057\u001b[39m )\n\u001b[32m   1058\u001b[39m X, present_key_value = attention_fast_forward_inference(\n\u001b[32m   1059\u001b[39m     decoder_layer.self_attn,\n\u001b[32m   1060\u001b[39m     hidden_states = X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1064\u001b[39m     do_prefill = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(decoder_layer.self_attn, \u001b[33m\"\u001b[39m\u001b[33mpaged_attention\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1065\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start_memory = monitor.get_memory()\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    checkpoint_dir.as_posix(),\n",
    "    attn_implementation=attn_implementation,\n",
    "    dtype=torch_dtype,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True if bit == 4 else False,\n",
    "    load_in_8bit=True if bit == 8 else False,\n",
    "    cache_dir=cache_dir.as_posix(),\n",
    "    local_files_only=True,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    inputs = []\n",
    "    for metadata, input, response in zip(examples['Metadata'], examples['Input'], examples['Response']):\n",
    "        response.replace(\"    \", \"\")\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        prompt = {\n",
    "            \"content\": common_prompt,\n",
    "            \"role\": \"system\"\n",
    "        }\n",
    "        content = \"\"\n",
    "        if train_type not in [\"WoMetadata\", \"WoMetadata+Thinking\"]:\n",
    "            content += f\"Metadata:{metadata};\"\n",
    "        content += f\"Input:{input};\"\n",
    "        user_input = {\n",
    "            \"content\": content,\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "        convos.append([prompt, user_input, answer])\n",
    "        inputs.append([prompt, user_input])\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    inputs = [\n",
    "        tokenizer.apply_chat_template(input, tokenize=False, add_generation_prompt=True)\n",
    "        for input in inputs\n",
    "    ]\n",
    "    return {\"text\": texts, \"input\": inputs}\n",
    "\n",
    "dataset_ts = dataset_ts_.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "eval_inputs = dataset_ts['input']\n",
    "inputs = dataset_ts['Input']\n",
    "scenarios = dataset_ts['Scenarios']\n",
    "metadatas = dataset_ts['Metadata']\n",
    "\n",
    "# í† í°í™” (íŒ¨ë”© ì—†ì´)\n",
    "tokenized_inputs = []\n",
    "for text_input in eval_inputs:\n",
    "    tokens = tokenizer(\n",
    "        text_input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )['input_ids'].to(model.device)\n",
    "    tokenized_inputs.append(tokens)\n",
    "\n",
    "mdscs = [(md, sc) for md, sc in zip(metadatas, scenarios)]\n",
    "\n",
    "# ì¶”ë¡  ì¤€ë¹„\n",
    "monitor.start(start_memory=start_memory)\n",
    "FastLanguageModel.for_inference(model)\n",
    "# ê°œë³„ ì¶”ë¡  ë° ì¸¡ì •\n",
    "results = []\n",
    "time_records = []\n",
    "memory_records = []\n",
    "\n",
    "print(f\"Starting individual inference for {len(tokenized_inputs)} samples...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for repeat_idx in range(2):\n",
    "    print(f\"Repeat {repeat_idx + 1}/3\")\n",
    "    for idx, input_tensor in enumerate(tokenized_inputs):\n",
    "        print(f\"Sample {idx+1}/{len(tokenized_inputs)}\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘\n",
    "        \n",
    "        # ì‹œê°„ ì¸¡ì • ì‹œì‘\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # ê°œë³„ ìƒ˜í”Œ ì¶”ë¡ \n",
    "            output = model.generate(\n",
    "                input_ids=input_tensor,\n",
    "                max_new_tokens=2000,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        # ì‹œê°„ ì¸¡ì • ì¢…ë£Œ\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        try:\n",
    "            out = extract_content(decoded)\n",
    "            out = eval(out)\n",
    "        except:\n",
    "            out = decoded\n",
    "        \n",
    "        result = {\n",
    "            \"Input\": inputs[idx],\n",
    "            \"Scenario\": mdscs[idx][1],\n",
    "            \"Metadata\": mdscs[idx][0],\n",
    "            \"Candidate\": out\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # ì¸¡ì •ê°’ ì €ì¥\n",
    "        time_records.append(inference_time)\n",
    "        \n",
    "        # ì§„í–‰ìƒí™© ì¶œë ¥\n",
    "        # print(f\"  - Time: {inference_time:.2f}s\")\n",
    "        # print(f\"  - Input length: {input_tensor.size(1)} tokens\")\n",
    "        # print()\n",
    "\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì¸¡ì • ì¢…ë£Œ\n",
    "peak_memory = monitor.stop()\n",
    "\n",
    "# í†µê³„ ì¶œë ¥\n",
    "\n",
    "# print(f\"Total time: {sum(time_records):.2f}s\")\n",
    "\n",
    "# # ê²°ê³¼ ì €ì¥\n",
    "# save_dir = \"../experiments/result_3rdyear\"\n",
    "# output_path = os.path.join(save_dir, f\"r-{model_repr}_{bit}bit-step-{checkpoint_num}.json\")\n",
    "\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json_str = json.dumps(results, ensure_ascii=False, indent=2)\n",
    "#     f.write(json_str)\n",
    "\n",
    "# print(f\"\\nResults saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-3rdyear_r450_a900_Bllossom-llama-3.2-Korean-Bllossom-3B_tr27_16bit-step-70\n",
      "============================================================\n",
      "peak memory: 12.08 GB\n",
      "Inference time avg, min, max, std: 26.19s, 24.81s, 26.46s, 0.26s\n",
      "Average per-sample std: 0.139s\n"
     ]
    }
   ],
   "source": [
    "print(f\"r-{model_repr}_{bit}bit-step-{checkpoint_num}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"peak memory: {peak_memory} GB\")\n",
    "\n",
    "print(f\"Inference time avg, min, max, std: {np.mean(time_records):.2f}s, {np.min(time_records):.2f}s, {np.max(time_records):.2f}s, {np.std(time_records):.2f}s\")\n",
    "\n",
    "# ê° ìƒ˜í”Œë³„ í‘œì¤€í¸ì°¨ ê³„ì‚° (ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•œ ê²½ìš°)\n",
    "num_unique_samples = len(tokenized_inputs)\n",
    "if len(time_records) > num_unique_samples:  # ë°˜ë³µ ì‹¤í–‰í•œ ê²½ìš°\n",
    "    # ê° ìƒ˜í”Œë³„ë¡œ ì‹œê°„ ê¸°ë¡ ê·¸ë£¹í™”\n",
    "    sample_times = {i: [] for i in range(num_unique_samples)}\n",
    "    for idx, time_val in enumerate(time_records):\n",
    "        sample_idx = idx % num_unique_samples\n",
    "        sample_times[sample_idx].append(time_val)\n",
    "    \n",
    "    # ê° ìƒ˜í”Œë³„ í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "    sample_stds = []\n",
    "    for sample_idx in range(num_unique_samples):\n",
    "        if len(sample_times[sample_idx]) > 1:\n",
    "            sample_std = np.std(sample_times[sample_idx])\n",
    "            sample_stds.append(sample_std)\n",
    "    if sample_stds:\n",
    "        print(f\"Average per-sample std: {np.mean(sample_stds):.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r-3rdyear_r211_a422_sh2orc-Llama-3.1-Korean-8B-Instruct_tr27_4bit-step-41\n",
    "============================================================\n",
    "peak memory: 10.22 GB\n",
    "Inference time avg, min, max, std: 5.72s, 1.69s, 15.12s, 3.38s\n",
    "Average per-sample std: 0.137s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r-3rdyear_r211_a422_sh2orc-Llama-3.1-Korean-8B-Instruct_tr27_8bit-step-41\n",
    "============================================================\n",
    "peak memory: 17.9 GB\n",
    "Inference time avg, min, max, std: 9.57s, 2.93s, 17.45s, 4.11s\n",
    "Average per-sample std: 0.580s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r-3rdyear_r211_a422_sh2orc-Llama-3.1-Korean-8B-Instruct_tr27_16bit-step-41\n",
    "============================================================\n",
    "peak memory: 19.86 GB\n",
    "Inference time avg, min, max, std: 5.25s, 1.66s, 11.44s, 2.40s\n",
    "Average per-sample std: 0.273s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r-3rdyear_r450_a900_Bllossom-llama-3.2-Korean-Bllossom-3B_tr27_16bit-step-70\n",
    "============================================================\n",
    "peak memory: 12.08 GB\n",
    "Inference time avg, min, max, std: 26.19s, 24.81s, 26.46s, 0.26s\n",
    "Average per-sample std: 0.139s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
