{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from db.manager import DBManager\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"TruePositive\"\n",
    "    false_positive = \"FalsePositive\"\n",
    "    false_negative = \"FalseNegative\"\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result\n",
    "\n",
    "def run_query_and_get_report(input, metadata, scenario, instruction_set):\n",
    "    input_report = {}\n",
    "    input_report[\"Input\"] = input\n",
    "    input_report[\"Scenario\"] = scenario\n",
    "    input_report[\"Result\"] = []\n",
    "    for instruction in instruction_set:\n",
    "        i_type = instruction[\"type\"]\n",
    "        if i_type == \"q\":\n",
    "            # query\n",
    "            args = instruction[\"args\"]\n",
    "            result_var_name = instruction[\"result_name\"]\n",
    "            # print(f\"Query: {args}, {result_var_name}\")\n",
    "            if \"temporal\" in args:\n",
    "                del args[\"table_name\"]\n",
    "                args[\"metadata\"] = metadata\n",
    "                result_df = DBManager.structured_query_data_t(args, get_rowids=True)\n",
    "            else:\n",
    "                result_df = DBManager.structured_query(args, get_rowids=True)\n",
    "            # print(f\"Result:\\n{result_df}\")\n",
    "            try:\n",
    "                if \"timestamp\" in result_df.columns:\n",
    "                    try:\n",
    "                        result_df[\"timestamp\"] = result_df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    except Exception as e:\n",
    "                        print(args)\n",
    "                        print(result_df[\"timestamp\"])\n",
    "                result = result_df.to_dict(orient=\"index\")\n",
    "                cols = list(result_df.columns)\n",
    "                result = [[row[col] for col in cols] for row in result.values()]\n",
    "                input_report[\"Metadata\"] = metadata\n",
    "                input_report[\"Result\"].append({\n",
    "                    \"type\": \"q\",\n",
    "                    # \"args\": args,\n",
    "                    # \"result_name\": result_var_name,\n",
    "                    \"result_shape\": result_df.shape,\n",
    "                    \"result_columns\": cols,\n",
    "                    \"result_indices\": list(result_df[\"id\"]),\n",
    "                    # \"result\": result\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inside: {e}\")\n",
    "                logger.error(f\"Invoked with Query: {args}, {result_var_name}\")\n",
    "        \n",
    "    return input_report\n",
    "\n",
    "def build_query_groundtruth(dateset_name):\n",
    "    ds_ts = []\n",
    "    dt_tr = []\n",
    "    base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dateset_name}\")\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir() and \"scenario\" in directory.name:\n",
    "            ts = read_json(f\"{directory}/onlyq_ts.json\")\n",
    "            if \"v7\" in dateset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            for i, d in enumerate(ts):\n",
    "                ts[i][\"Scenario\"] = directory.name\n",
    "                if \"v7\" in dateset_name:\n",
    "                    ts[i][\"Metadata\"] = metadata\n",
    "            ds_ts.extend(ts)\n",
    "            \n",
    "\n",
    "            tr = read_json(f\"{directory}/onlyq_tr.json\")\n",
    "            for i, d in enumerate(tr):\n",
    "                tr[i][\"Scenario\"] = directory.name\n",
    "                if \"v7\" in dateset_name:\n",
    "                    tr[i][\"Metadata\"] = metadata\n",
    "            dt_tr.extend(tr)\n",
    "    \n",
    "    ds = ds_ts + dt_tr\n",
    "    print(len(ds))\n",
    "    \n",
    "    if \"v7\" in dateset_name:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "    else:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "        metadata = None\n",
    "    \n",
    "    with open(db_gt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[\")\n",
    "        with tqdm(total=len(ds)) as pbar:\n",
    "            for d in ds:\n",
    "                pbar.set_description(f\"Processing {d['Input']}\")\n",
    "                # print(\"--\")\n",
    "                \n",
    "                input = d[\"Input\"]\n",
    "                # print(f\"Input: {input}\")\n",
    "                scenario = d[\"Scenario\"]\n",
    "                \n",
    "                metadata = d[\"Metadata\"]\n",
    "                response = d[\"Response\"]\n",
    "                # instruction_set = response[\"Instruction Set\"]\n",
    "                instruction_set = response[\"Instructions\"]\n",
    "                # print(f\"Instruction Set: {type(instruction_set)}, {len(instruction_set)}\")\n",
    "                input_report = run_query_and_get_report(input, metadata, scenario, instruction_set)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # print(input_report)\n",
    "                    # del input_report[\"Metadata\"]\n",
    "                    f.write(json.dumps(input_report, ensure_ascii=False) + \",\\n\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error outside: {e}\")\n",
    "                    logger.error(f\"Invoked with Input: {input}\")\n",
    "                    logger.error(f\"Input Report: {input_report}\")\n",
    "                    # exit()\n",
    "                    raise e\n",
    "                \n",
    "                # print(\"\\n\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "        # make it json array format\n",
    "        # remove last comma\n",
    "        f.seek(f.tell() - 2, 0)\n",
    "        f.write(\"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_query_groundtruth(\"v5-250228-multimetadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_query(db_gt_filename, cand_response_filename):\n",
    "#     db_gts = read_json(db_gt_filename)\n",
    "#     cand_responses = read_json(cand_response_filename)\n",
    "\n",
    "#     evaluation_reports = []\n",
    "\n",
    "#     with tqdm(total=len(cand_responses)) as pbar:\n",
    "#         for cand_response in cand_responses:\n",
    "#             pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "#             input = cand_response[\"Input\"]\n",
    "#             scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "#             if \"Metadata\" in cand_response:\n",
    "#                 metadata = cand_response[\"Metadata\"]\n",
    "#             else:\n",
    "#                 metadata = None\n",
    "#             # 관계 없는 질문들은 건너뛰자\n",
    "#             gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "#             assert len(gt_report) <= 1\n",
    "#             if len(gt_report) == 0:\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "#             gt_report = gt_report[0]\n",
    "#             if gt_report[\"Result\"] == []:\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "            \n",
    "#             evaluation_report = defaultdict(lambda: None)\n",
    "#             evaluation_report[\"Input\"] = input\n",
    "#             evaluation_report[\"Scenario\"] = scenario\n",
    "            \n",
    "#             if isinstance(cand_response[\"Candidate\"], dict) and (\"Instruction Set\" in cand_response[\"Candidate\"] or \"지시\" in cand_response[\"Candidate\"]):\n",
    "#                 if \"Instruction Set\" in cand_response[\"Candidate\"]:\n",
    "#                     cand_instruction_set = cand_response[\"Candidate\"][\"Instruction Set\"]\n",
    "#                 elif \"지시\" in cand_response[\"Candidate\"]:\n",
    "#                     cand_instruction_set = cand_response[\"Candidate\"][\"지시\"]\n",
    "                    \n",
    "#                 evaluation_report[EM.json_structure] = True\n",
    "#             else:\n",
    "#                 evaluation_report[EM.json_structure] = False\n",
    "#                 try:\n",
    "#                     import re\n",
    "#                     # get data between \"Instruction Set\": [ and the last]\n",
    "#                     cand_instruction_set = re.search(r'(?<=\"Instruction Set\": \\[)(.*)(?=\\])', cand_response[\"Candidate\"], re.DOTALL).group(0)\n",
    "#                     # find all {\"type\": ~ }, {\"type\": ~ }, {\"type\": ~ }\n",
    "#                     cand_instruction_set = re.findall(r'({\"type\".*?})', cand_instruction_set)\n",
    "#                     # print(list(cand_instruction_set))\n",
    "#                     cand_instruction_set = [eval(d) for d in cand_instruction_set]\n",
    "#                 except Exception as e:\n",
    "#                     evaluation_report[EM.json_structure] = False\n",
    "#                     print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "#                     print(e)\n",
    "#                     evaluation_reports.append(evaluation_report)\n",
    "#                     pbar.update(1)\n",
    "#                     print(evaluation_report)\n",
    "#                     continue\n",
    "                    \n",
    "#             cand_report = run_query_and_get_report(input, metadata, scenario, cand_instruction_set) \n",
    "            \n",
    "#             # print(f\"Input: {input}\")\n",
    "            \n",
    "#             gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "#             gt_cols, gt_rows = set(gt_results[\"result_columns\"]), set(gt_results[\"result_indices\"])\n",
    "#             cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "#             # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "#             true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "#             # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "#             false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "#             # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "#             false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "\n",
    "#             # # print(len(gt_flatten), len(cand_flatten))\n",
    "            \n",
    "            \n",
    "#             # # check if all gt results are in cand results\n",
    "#             # true_positive, false_positive, false_negative = 0, 0, 0\n",
    "#             # for gt_data in gt_flatten:\n",
    "#             #     try:\n",
    "#             #         cand_flatten.remove(gt_data)\n",
    "#             #         true_positive += 1\n",
    "#             #     except ValueError as e:\n",
    "#             #         false_negative += 1\n",
    "            \n",
    "#             # false_positive = len(cand_flatten)\n",
    "            \n",
    "#             evaluation_report[EM.true_positive] = true_positive\n",
    "#             evaluation_report[EM.false_positive] = false_positive\n",
    "#             evaluation_report[EM.false_negative] = false_negative\n",
    "            \n",
    "#             evaluation_reports.append(evaluation_report)\n",
    "#             # print(evaluation_report)\n",
    "            \n",
    "#             pbar.update(1)\n",
    "\n",
    "#     eval_df = pd.DataFrame(evaluation_reports)\n",
    "#     # print(eval_df)\n",
    "\n",
    "#     eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "#     # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "#     # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "#     # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "#     final_result = {}\n",
    "\n",
    "#     for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "#         # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "#         final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "#     # normalize per query\n",
    "#     eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "#     eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "#     eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "#     eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "#     # # F1 score except nans.\n",
    "#     truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "#     precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "#     recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     # print(f\"F1: {f1}\")\n",
    "#     final_result[\"F1\"] = f1\n",
    "#     final_result[\"Recall\"] = recall\n",
    "#     for col in final_result:\n",
    "#         print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "#     return eval_df\n",
    "\n",
    "def eval_query(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            gt_report = gt_report[0]\n",
    "            if gt_report[\"Result\"] == []:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Scenario\"] = scenario\n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict) and (\"Instruction Set\" in cand_response[\"Candidate\"] or \"지시\" in cand_response[\"Candidate\"] or \"Instructions\" in cand_response[\"Candidate\"]):\n",
    "                if \"Instruction Set\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instruction Set\"]\n",
    "                elif \"지시\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"지시\"]\n",
    "                elif \"Instructions\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instructions\"]\n",
    "\n",
    "                evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                try:\n",
    "                    import re\n",
    "                    # get data between \"Instruction Set\": [ and the last]\n",
    "                    cand_instruction_set = re.search(r'(?<=\"Instruction Set\": \\[)(.*)(?=\\])', cand_response[\"Candidate\"], re.DOTALL).group(0)\n",
    "                    # find all {\"type\": ~ }, {\"type\": ~ }, {\"type\": ~ }\n",
    "                    cand_instruction_set = re.findall(r'({\"type\".*?})', cand_instruction_set)\n",
    "                    # print(list(cand_instruction_set))\n",
    "                    cand_instruction_set = [eval(d) for d in cand_instruction_set]\n",
    "                except Exception as e:\n",
    "                    evaluation_report[EM.json_structure] = False\n",
    "                    print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                    print(e)\n",
    "                    evaluation_reports.append(evaluation_report)\n",
    "                    pbar.update(1)\n",
    "                    print(evaluation_report)\n",
    "                    continue\n",
    "                    \n",
    "            cand_report = run_query_and_get_report(input, metadata, scenario, cand_instruction_set) \n",
    "            \n",
    "            # print(f\"Input: {input}\")\n",
    "            \n",
    "            gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "\n",
    "            if len(cand_results) == 0:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                # evaluation_report[EM.true_positive] = 0\n",
    "                # evaluation_report[EM.false_positive] = false_positive\n",
    "                # evaluation_report[EM.false_negative] = false_negative\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                # print(evaluation_report)\n",
    "                            \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            cand_results = cand_results[0]\n",
    "\n",
    "            gt_rows = []\n",
    "            for gt_result in gt_results:\n",
    "                gt_rows.extend(gt_result[\"result_indices\"])\n",
    "            \n",
    "            gt_rows = set(gt_rows)\n",
    "            gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "            cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "            gt_cols.remove(\"id\")\n",
    "            cand_cols.remove(\"id\")\n",
    "            gt_cols.remove(\"idu\")\n",
    "            cand_cols.remove(\"idu\")\n",
    "\n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "\n",
    "\n",
    "            # print(len(gt_flatten), len(cand_flatten))\n",
    "            \n",
    "            # gt_counter = Counter(gt_flatten)\n",
    "            # cand_counter = Counter(cand_flatten)\n",
    "\n",
    "            # true_positive = sum(min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_negative = sum(gt_counter[item] - min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_positive = sum(cand_counter[item] - min(cand_counter[item], gt_counter.get(item, 0)) for item in cand_counter)\n",
    "            \n",
    "            # # check if all gt results are in cand results\n",
    "            # true_positive, false_positive, false_negative = 0, 0, 0\n",
    "            # for gt_data in gt_flatten:\n",
    "            #     try:\n",
    "            #         cand_flatten.remove(gt_data)\n",
    "            #         true_positive += 1\n",
    "            #     except ValueError as e:\n",
    "            #         false_negative += 1\n",
    "            \n",
    "            # false_positive = len(cand_flatten)\n",
    "            \n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            \n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WoAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r128_a256_woall-checkpoint-60\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-v5_r64_a128_woall-checkpoint-72\"\n",
    "# cand_response_filename = \"r-v5_r32_a64_woall-checkpoint-70-batch\"\n",
    "# cand_response_filename = \"r-v6_r64_a128_woall_shorten-checkpoint-53\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "# print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"r-v5_r256_a512_FI-checkpoint-43-batch\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r256_a512_ISP-checkpoint-104\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-v5_r256_a512_ours-checkpoint-20\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours-checkpoint-52-batch\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours_noexample-checkpoint-50-batch\"\n",
    "# cand_response_filename = \"r-v6_r128_a256_ours-checkpoint-52\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours-checkpoint-40\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours_shorten-checkpoint-30\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Why is our classroom so cold:   0%|          | 0/61 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_query_and_get_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuild_query_groundtruth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv7-250309-reduceinputanddatefunctioncall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 64\u001b[0m, in \u001b[0;36mbuild_query_groundtruth\u001b[0;34m(dateset_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m instruction_set \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# print(f\"Instruction Set: {type(instruction_set)}, {len(instruction_set)}\")\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m input_report \u001b[38;5;241m=\u001b[39m \u001b[43mrun_query_and_get_report\u001b[49m(\u001b[38;5;28minput\u001b[39m, metadata, scenario, instruction_set)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# print(input_report)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# del input_report[\"Metadata\"]\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(input_report, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_query_and_get_report' is not defined"
     ]
    }
   ],
   "source": [
    "build_query_groundtruth(\"v7-250309-reduceinputanddatefunctioncall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m db_gt_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiments/db_gt.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m cand_response_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiments/db_gt_v7.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 101\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_query_gtgt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_gt_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand_response_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_df)\n",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m, in \u001b[0;36meval_query_gtgt\u001b[0;34m(db_gt_filename, cand_response_filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_query_gtgt\u001b[39m(db_gt_filename, cand_response_filename):\n\u001b[0;32m----> 2\u001b[0m     db_gts \u001b[38;5;241m=\u001b[39m \u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_gt_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     cand_responses \u001b[38;5;241m=\u001b[39m read_json(cand_response_filename)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_json\u001b[39m(path):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 9\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "def eval_query_gtgt(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_report in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_report['Input']}\")\n",
    "            input = cand_report[\"Input\"]\n",
    "            scenario = cand_report[\"Scenario\"]\n",
    "\n",
    "\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            \n",
    "            \n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            gt_report = gt_report[0]\n",
    "            if gt_report[\"Result\"] == []:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # print(f\"Input: {input}\")\n",
    "            \n",
    "            gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "            cand_results = cand_results[0]\n",
    "\n",
    "            gt_rows = []\n",
    "            for gt_result in gt_results:\n",
    "                gt_rows.extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_rows = set(gt_rows)\n",
    "            gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "            cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "            gt_cols.remove(\"id\")\n",
    "            cand_cols.remove(\"id\")\n",
    "            # gt_cols.remove(\"idu\")\n",
    "            cand_cols.remove(\"idu\")\n",
    "\n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "\n",
    "eval_df = eval_query_gtgt(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>TruePositive</th>\n",
       "      <th>FalsePositive</th>\n",
       "      <th>FalseNegative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is our classroom so cold</td>\n",
       "      <td>0.125088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오늘 아침과 저녁의 온도차이는 얼마나 돼?</td>\n",
       "      <td>0.504202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>지금 옆반 온도랑 우리반 온도 알려줘</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>현재 설정온도랑 실내온도 차이 알려줘.</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>앞반 전원 켜져있어?</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>최근에 설정온도가 가장 높았던 날 알려줘</td>\n",
       "      <td>0.875353</td>\n",
       "      <td>0.124647</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>지난 3일 동안 우리반 실내 온도 평균 값 알려줘.</td>\n",
       "      <td>0.749428</td>\n",
       "      <td>0.250572</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>올여름 제일 더웠던 날 알려줘</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>올해 봄 옆반 제일 추웠던 날 알려줘</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>우리반의 가장 최근 설정 온도 알려줘</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>옆반의 가장 최근 온도랑 설정온도 알려줘</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>지금 옆반 너무 추워</td>\n",
       "      <td>0.125088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>우리반 왜이리 덥노</td>\n",
       "      <td>0.125088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What is the current temperature in our class?</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Input  TruePositive  \\\n",
       "0                    Why is our classroom so cold      0.125088   \n",
       "1                         오늘 아침과 저녁의 온도차이는 얼마나 돼?      0.504202   \n",
       "2                            지금 옆반 온도랑 우리반 온도 알려줘      0.166667   \n",
       "4                           현재 설정온도랑 실내온도 차이 알려줘.      0.166667   \n",
       "6                                     앞반 전원 켜져있어?      0.166667   \n",
       "8                          최근에 설정온도가 가장 높았던 날 알려줘      0.875353   \n",
       "9                    지난 3일 동안 우리반 실내 온도 평균 값 알려줘.      0.749428   \n",
       "14                               올여름 제일 더웠던 날 알려줘      0.500000   \n",
       "15                           올해 봄 옆반 제일 추웠던 날 알려줘      0.500000   \n",
       "20                           우리반의 가장 최근 설정 온도 알려줘      0.000604   \n",
       "21                         옆반의 가장 최근 온도랑 설정온도 알려줘      0.000603   \n",
       "22                                    지금 옆반 너무 추워      0.125088   \n",
       "23                                     우리반 왜이리 덥노      0.125088   \n",
       "24  What is the current temperature in our class?      0.166667   \n",
       "\n",
       "    FalsePositive  FalseNegative  \n",
       "0        0.000000       0.874912  \n",
       "1        0.000000       0.495798  \n",
       "2        0.833333       0.000000  \n",
       "4        0.833333       0.000000  \n",
       "6        0.833333       0.000000  \n",
       "8        0.124647       0.000000  \n",
       "9        0.250572       0.000000  \n",
       "14       0.500000       0.000000  \n",
       "15       0.500000       0.000000  \n",
       "20       0.000000       0.999396  \n",
       "21       0.000000       0.999397  \n",
       "22       0.000000       0.874912  \n",
       "23       0.000000       0.874912  \n",
       "24       0.833333       0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Why is our classroom so cold:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '3 hours' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 오늘 아침과 저녁의 온도차이는 얼마나 돼?:   8%|▊         | 1/12 [00:00<00:00, 106.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= DATE_TRUNC('day', DATE '2022-09-30') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30' + INTERVAL '1 day')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지금 옆반 온도랑 우리반 온도 알려줘:  17%|█▋        | 2/12 [00:00<00:00, 53.45it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 우리반과 옆반 온도 변화 추이 비교해줘:  25%|██▌       | 3/12 [00:00<00:00, 68.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= DATE_TRUNC('week', DATE '2022-09-30') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30' + INTERVAL '1 week')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 현재 설정온도랑 실내온도 차이 알려줘.:  33%|███▎      | 4/12 [00:00<00:00, 20.84it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?:  42%|████▏     | 5/12 [00:00<00:00, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= DATE_TRUNC('week', DATE '2022-09-30' - INTERVAL '1 week') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 앞반 전원 켜져있어?:  50%|█████     | 6/12 [00:00<00:00, 20.84it/s]                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 어제 전원 껐어?:  58%|█████▊    | 7/12 [00:00<00:00, 20.84it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 최근에 설정온도가 가장 높았던 날 알려줘:  67%|██████▋   | 8/12 [00:00<00:00, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= DATE_TRUNC('month', DATE '2022-09-30' - INTERVAL '1 month') AND timestamp < DATE_TRUNC('month', DATE '2022-09-30')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난 3일 동안 우리반 실내 온도 평균 값 알려줘.:  92%|█████████▏| 11/12 [00:00<00:00, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '3 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난 3일 동안 우리반 실내 온도 평균 값 알려줘.: 100%|██████████| 12/12 [00:00<00:00, 19.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JsonStructureCorrectness: 1.00\n",
      "ExactMatch: 0.40\n",
      "F1: 0.80\n",
      "Recall: 0.94\n",
      "                              Input   Scenario  JsonStructureCorrectness  \\\n",
      "0      Why is our classroom so cold  scenario1                      True   \n",
      "1           오늘 아침과 저녁의 온도차이는 얼마나 돼?  scenario1                      True   \n",
      "2              지금 옆반 온도랑 우리반 온도 알려줘  scenario1                      True   \n",
      "3         이번주 우리반과 옆반 온도 변화 추이 비교해줘  scenario1                      True   \n",
      "4             현재 설정온도랑 실내온도 차이 알려줘.  scenario1                      True   \n",
      "5  지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?  scenario1                      True   \n",
      "6                       앞반 전원 켜져있어?  scenario1                      True   \n",
      "7                         어제 전원 껐어?  scenario1                      True   \n",
      "8            최근에 설정온도가 가장 높았던 날 알려줘  scenario1                      True   \n",
      "9      지난 3일 동안 우리반 실내 온도 평균 값 알려줘.  scenario1                      True   \n",
      "\n",
      "   TruePositive  FalsePositive  FalseNegative  ExactMatch   Total  \n",
      "0      1.000000       0.000000       0.000000           1     712  \n",
      "1      0.249473       0.750527       0.000000           0    1423  \n",
      "2      0.500000       0.500000       0.000000           0      24  \n",
      "3      1.000000       0.000000       0.000000           1   28402  \n",
      "4      0.666667       0.333333       0.000000           0      18  \n",
      "5      1.000000       0.000000       0.000000           1   29757  \n",
      "6      1.000000       0.000000       0.000000           1       6  \n",
      "7      0.500000       0.500000       0.000000           0    2880  \n",
      "8      0.000000       0.795323       0.204677           0  110848  \n",
      "9      0.749428       0.000000       0.250572           0    5679  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/r-v7_r8_a16_ours-checkpoint-40.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>TruePositive</th>\n",
       "      <th>FalsePositive</th>\n",
       "      <th>FalseNegative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오늘 아침과 저녁의 온도차이는 얼마나 돼?</td>\n",
       "      <td>0.249473</td>\n",
       "      <td>0.750527</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>지금 옆반 온도랑 우리반 온도 알려줘</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>현재 설정온도랑 실내온도 차이 알려줘.</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>어제 전원 껐어?</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>최근에 설정온도가 가장 높았던 날 알려줘</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795323</td>\n",
       "      <td>0.204677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>지난 3일 동안 우리반 실내 온도 평균 값 알려줘.</td>\n",
       "      <td>0.749428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Input  TruePositive  FalsePositive  FalseNegative\n",
       "1       오늘 아침과 저녁의 온도차이는 얼마나 돼?      0.249473       0.750527       0.000000\n",
       "2          지금 옆반 온도랑 우리반 온도 알려줘      0.500000       0.500000       0.000000\n",
       "4         현재 설정온도랑 실내온도 차이 알려줘.      0.666667       0.333333       0.000000\n",
       "7                     어제 전원 껐어?      0.500000       0.500000       0.000000\n",
       "8        최근에 설정온도가 가장 높았던 날 알려줘      0.000000       0.795323       0.204677\n",
       "9  지난 3일 동안 우리반 실내 온도 평균 값 알려줘.      0.749428       0.000000       0.250572"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
