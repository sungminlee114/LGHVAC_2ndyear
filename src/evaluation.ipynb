{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.6.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.db.manager import DBManager\n",
    "from src.input_to_instructions.load_and_execute import *\n",
    "from src.input_to_instructions.types import *\n",
    "from src.plot_graph.execute import *\n",
    "from src.operation.execute import *\n",
    "from src.response_generation.load_and_execute import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:db.instance:Connected to the database PerSite_DB\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\"\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 2. Max memory: 23.689 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6653e4d6d16148fdb4a0dc0f3142af3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ResponseGeneration.initialize(\n",
    "    log_output=False,\n",
    "    instance_type=\"unsloth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_variables_to_report(variables):\n",
    "#     for k, v in variables.items():\n",
    "#         if isinstance(v, pd.DataFrame):\n",
    "            \n",
    "#         elif isinstance(v, np.ndarray):\n",
    "#             variables[k] = v.tolist()\n",
    "#     return variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from src.input_to_instructions.types import InstructionQ_raw, InstructionQ_v2\n",
    "from src.plot_graph.execute import plot_graph_plotly\n",
    "matplotlib.rcParams['font.family'] = 'NanumGothicCoding'\n",
    "\n",
    "def get_time(df, fmt=\"datetime\"):\n",
    "    # from df get 'timestamp' column and return them in format\n",
    "    if fmt == \"date\":\n",
    "        fmt = '%Y-%m-%d'\n",
    "    elif fmt == \"month\":\n",
    "        fmt = '%Y-%m'\n",
    "    elif fmt == \"year\":\n",
    "        fmt = '%Y'\n",
    "    else:\n",
    "        fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    if isinstance(df['timestamp'], pd.Timestamp):\n",
    "        result = df['timestamp'].strftime(fmt)\n",
    "    else:\n",
    "        result = df['timestamp'].apply(lambda x: x.strftime(fmt))\n",
    "    return sorted(list(set(result)))\n",
    "\n",
    "def get_spatials(df):\n",
    "    return pd.unique(df['idu_name'])\n",
    "\n",
    "def get_tv(df, col:str|list[str], fmt=\"datetime\"):\n",
    "    if isinstance(col, str):\n",
    "        col = [col]\n",
    "    \n",
    "    timestamps = get_time(df, fmt)\n",
    "    return_tuple = tuple([timestamps] + [df[c] for c in col])\n",
    "    return return_tuple\n",
    "\n",
    "def data(metadata, mapping, query_results, t=str|list[str], s=str|list[str], m=str|list[str]):\n",
    "    if isinstance(t, str):\n",
    "        t = [t]\n",
    "    if isinstance(s, str):\n",
    "        s = [s]\n",
    "    if isinstance(m, str):\n",
    "        m = [m]\n",
    "\n",
    "    t_raw = [mapping.temporal[t_highlevel] for t_highlevel in t]\n",
    "    s_raw = [mapping.spatials[s_highlevel] for s_highlevel in s]\n",
    "    m_raw = [mapping.modalities[m_highlevel] for m_highlevel in m]\n",
    "\n",
    "    result_df = DBManager.structured_query_data_t_v2(metadata, m_raw, t_raw, s_raw, get_rowids=True)\n",
    "\n",
    "    cols = list(result_df.columns)\n",
    "    cols.remove(\"id\")\n",
    "    cols.remove(\"idu_name\")\n",
    "    cols.remove(\"timestamp\")\n",
    "    rows = list(result_df[\"id\"])\n",
    "    query_results.append({\n",
    "        \"result_columns\": cols,\n",
    "        \"result_indices\": rows,\n",
    "    })\n",
    "\n",
    "    # For demo, drop rows where any value is -1\n",
    "    result_df = result_df.loc[(result_df != -1).all(axis=1)]\n",
    "\n",
    "    # drop \"id\" from result_df\n",
    "    result_df = result_df.drop(columns=['id'])\n",
    "\n",
    "    # change column names to high level\n",
    "    inverse_mapping = {v: k for k, v in mapping.modalities.items()}\n",
    "    result_df.columns = [inverse_mapping[col] if col in inverse_mapping else col for col in result_df.columns]\n",
    "\n",
    "    # change idu_name raw values to high level\n",
    "    inverse_mapping = {v: k for k, v in mapping.spatials.items()}\n",
    "    result_df[\"idu_name\"] = result_df[\"idu_name\"].map(inverse_mapping)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def run_query_v2(user_input, metadata, instructions):\n",
    "    instruction_q:InstructionQ_v2 = [i for i in instructions if isinstance(i, InstructionQ_v2)][0]\n",
    "    mapping = instruction_q\n",
    "    \n",
    "    instruction_o:InstructionO = [i for i in instructions if isinstance(i, InstructionO)]\n",
    "    query_results = []\n",
    "    if len(instruction_o) != 0:\n",
    "        instruction_o = instruction_o[0]\n",
    "        scripts = instruction_o.scripts\n",
    "\n",
    "        # search data(t=~~, ...,)\n",
    "        globals()['metadata'] = metadata\n",
    "        globals()['mapping'] = mapping\n",
    "        globals()['query_results'] = query_results\n",
    "        for script in scripts:\n",
    "            \n",
    "            if \"data\" in script:\n",
    "                script = script.replace(\"data(\", \"data(metadata, mapping, query_results, \")\n",
    "            exec(script, globals())\n",
    "        \n",
    "        variables = {name:globals()[name] for name in globals() if name.startswith(\"v_\")}\n",
    "        instruction_r:InstructionR = [i for i in instructions if isinstance(i, InstructionR)][0]\n",
    "    else:\n",
    "        variables = {}\n",
    "        unknown_spatials = [k for k, v in mapping.spatials.items() if v == \"Unknown\"]\n",
    "        unknown_modalities = [k for k, v in mapping.modalities.items() if v == \"Unknown\"]\n",
    "        \n",
    "        response_unknown = f\"ì£„ì†¡í•©ë‹ˆë‹¤, {unknown_spatials + unknown_modalities}ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"\n",
    "        return response_unknown, variables, [], query_results\n",
    "\n",
    "\n",
    "    response, required_variables = ResponseGeneration.execute(instruction_r, variables, user_input, None, exp_tag=None)\n",
    "    return response, variables, required_variables, query_results\n",
    "\n",
    "\n",
    "def run_query(user_input, metadata, instructions, exp_tag=None):\n",
    "    variables = {\n",
    "        \"Metadata\": metadata,\n",
    "    }\n",
    "    query_results = []\n",
    "        \n",
    "    \n",
    "    for instruction in instructions:\n",
    "        # logger.debug(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        # print(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        \n",
    "        if type(instruction) == InstructionQ:\n",
    "            # Execute query\n",
    "            result_df = DBManager.structured_query_data_t(metadata, instruction.args, get_rowids=True)\n",
    "            # if result_df is None:\n",
    "                # print(\"ì£„ì†¡í•©ë‹ˆë‹¤, ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\", \"response\")\n",
    "                # return\n",
    "\n",
    "            cols = list(result_df.columns)\n",
    "            cols.remove(\"id\")\n",
    "            cols.remove(\"idu\")\n",
    "            rows = list(result_df[\"id\"])\n",
    "\n",
    "            query_results.append({\n",
    "                \"result_columns\": cols,\n",
    "                \"result_indices\": rows,\n",
    "            })\n",
    "\n",
    "            # For demo, drop rows where any value is -1\n",
    "            result_df = result_df.loc[(result_df != -1).all(axis=1)]\n",
    "\n",
    "            # drop \"id\" from result_df\n",
    "            result_df = result_df.drop(columns=['id'])\n",
    "           \n",
    "            #pd.set_option('display.max_rows', 10000)        \n",
    "            #pd.set_option('display.max_columns', 1000)\n",
    "            #pd.set_option('display.width', 1000)\n",
    "            #pd.set_option('display.max_colwidth', 1000)\n",
    "            #print(f\"QueryResult: {result_df}\")\n",
    "\n",
    "            variables[instruction.result_name] = result_df\n",
    "        elif type(instruction) == InstructionQ_raw:\n",
    "            instruction.query = instruction.query.replace(\" FROM \\\"data_t\\\"\", \", \\\"id\\\" FROM \\\"data_t\\\"\")\n",
    "            result_df = DBManager.execute_structured_query_string(\n",
    "                instruction.query\n",
    "            )\n",
    "            # rename idu_name to idu\n",
    "            result_df = result_df.rename(columns={'idu_name': 'idu'})\n",
    "            \n",
    "            cols = list(result_df.columns)\n",
    "            cols.remove(\"id\")\n",
    "            cols.remove(\"idu\")\n",
    "            rows = list(result_df[\"id\"])\n",
    "\n",
    "            query_results.append({\n",
    "                \"result_columns\": cols,\n",
    "                \"result_indices\": rows,\n",
    "            })\n",
    "\n",
    "            # drop \"id\" from result_df\n",
    "            result_df = result_df.drop(columns=['id'])\n",
    "            \n",
    "            variables[instruction.result_name] = result_df\n",
    "            # print(result_df, flush=True)\n",
    "\n",
    "        elif type(instruction) == InstructionO:\n",
    "            # Execute operation\n",
    "            # variables_to_report = {k: v for k, v in variables.items() if k not in [\"Metadata\"]}\n",
    "            # print(variables_to_report)\n",
    "            result_dict = OperationExecutor.execute(variables, instruction.scripts)\n",
    "            # print(instruction.scripts, instruction.returns, result_dict)\n",
    "            variables.update(result_dict)\n",
    "            pass\n",
    "            # print(fig, \"graph\")\n",
    "        elif type(instruction) == InstructionR:\n",
    "            # Execute response generation\n",
    "            variables_to_report = {k: v for k, v in variables.items() if k not in [\"Metadata\"]}\n",
    "            # print(variables_to_report)\n",
    "            # variables_to_report = ResponseGeneration.stringify_variables(variables_to_report)\n",
    "            # variables_to_report = summarize_variables_to_report(variables_to_report)\n",
    "\n",
    "            # print(f\"Variables: {variables_to_report}\")\n",
    "\n",
    "            keys_to_leave = [\"modality_mapping\", \"idu_mapping\"]\n",
    "            metadata_ = {}\n",
    "            for key in metadata.keys():\n",
    "                if key in keys_to_leave:\n",
    "                    metadata_[key] = metadata[key]\n",
    "\n",
    "            response, required_variables = ResponseGeneration.execute(instruction, variables, user_input, metadata_, exp_tag=exp_tag)\n",
    "            # print(f\"Required variables: {required_variables}\")\n",
    "            \n",
    "            # response = instruction.expectations[0] # \"{{var}}...\"\n",
    "            # for var_name, var_value in required_variables.items():\n",
    "            #     placeholder = f\"{{{{{var_name}}}}}\"\n",
    "            #     if placeholder in response:\n",
    "            #         response = response.replace(placeholder, str(var_value))\n",
    "\n",
    "            \n",
    "            return response, variables_to_report, required_variables, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def build_query_groundtruth():\n",
    "    dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "    def read(path):\n",
    "        data = read_json(path)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = directory.name\n",
    "            if \"v7\" in dataset_name:\n",
    "                data[i][\"Metadata\"] = metadata\n",
    "        return data\n",
    "\n",
    "    ds_ts = []\n",
    "    base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dataset_name}\")\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir():\n",
    "            if \"v7\" in dataset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            \n",
    "            # d = read(f\"{directory}/onlyq_ts.json\")\n",
    "            \n",
    "            ds_ts.extend(read(f\"{directory}/onlyq_ts.json\"))\n",
    "            ds_ts.extend(read(f\"{directory}/onlyq_tr.json\"))\n",
    "            # ds_tr.extend(read(f\"{directory}/graph.json\"))\n",
    "    \n",
    "    ds = ds_ts\n",
    "    print(len(ds))\n",
    "    # if \"v7\" in dataset_name:\n",
    "    #     db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "    # else:\n",
    "    #     db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "    #     metadata = None\n",
    "    \n",
    "    # with open(db_gt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        # f.write(\"[\")\n",
    "    # with tqdm(total=len(ds)) as pbar:\n",
    "    \n",
    "    gts = []\n",
    "\n",
    "    for d in ds:\n",
    "        cont = False\n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "        for st in skip_tags:\n",
    "            if st in tags:\n",
    "                cont = True\n",
    "                break\n",
    "        if cont:\n",
    "            continue\n",
    "\n",
    "        # pbar.set_description(f\"Processing {d['Input']}\")\n",
    "        # print(\"--\")\n",
    "        exp_tag = \"v2\"\n",
    "        # print(f\"Warning! exp_tag is v2\")\n",
    "        instructions = InputToInstruction.postprocess(deepcopy(d['Response']), exp_tag=exp_tag)\n",
    "        user_input, tags, metadata, scenario = d[\"Input\"], d[\"Tags\"], d[\"Metadata\"], d[\"Scenario\"]\n",
    "        # if user_input != \"ì§€ê¸ˆ ëª‡ì‹œì•¼?\":\n",
    "        #     continue\n",
    "\n",
    "        response, variables_to_report, required_variables, query_results = run_query_v2(user_input, metadata, instructions)\n",
    "        # print(f\"ì¶œë ¥: {response}\")\n",
    "        # print({k: (v, type(v)) for k, v in variables_to_report.items()})\n",
    "        gts.append({\n",
    "            \"Input\": user_input,\n",
    "            \"Metadata\": metadata,\n",
    "            \"Scenario\": scenario,\n",
    "            \"Tags\": tags,\n",
    "            \"GT\": d['Response'],\n",
    "            \"Response\": response,\n",
    "            # \"RequiredVariables\": required_variables,\n",
    "            \"QueryResults\": query_results,\n",
    "            # \"VariablesToReport\": variables_to_report,\n",
    "        })\n",
    "\n",
    "    # save to json\n",
    "    with open(f\"./gts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(gts, f, ensure_ascii=False, indent=4)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "ì§ˆë¬¸: ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ê³¼ ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„({{v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ)ì™€ ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„({{v_ì´ë²ˆì£¼_ì•žë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ)ì˜ ì°¨ì´ëŠ” {{v_ì´ë²ˆì£¼_ì‹¤ë‚´ì˜¨ë„_í‰ê· _ì°¨ì´}}â„ƒìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(25.66922751621088), 'v_ì´ë²ˆì£¼_ì•žë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(25.98149711551991), 'v_ì´ë²ˆì£¼_ì‹¤ë‚´ì˜¨ë„_í‰ê· _ì°¨ì´': np.float64(0.31226959930902964)};\n",
      "ì¶œë ¥: ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.67â„ƒ)ì™€ ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.98â„ƒ)ì˜ ì°¨ì´ëŠ” 0.31â„ƒìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: í˜„ìž¬ ì„¤ì •ì˜¨ë„ëž‘ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.; í¬ë§·: ['í˜„ìž¬ ì„¤ì •ì˜¨ë„({{v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„}}â„ƒ)ì™€ ì‹¤ë‚´ì˜¨ë„({{v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„}}â„ƒ)ì˜ ì°¨ì´ëŠ” {{v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì˜¨ë„_ì°¨ì´}}â„ƒìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„': np.float64(23.0), 'v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„': np.float64(28.5), 'v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì˜¨ë„_ì°¨ì´': np.float64(5.5)};\n",
      "ì¶œë ¥: í˜„ìž¬ ì„¤ì •ì˜¨ë„(23.00â„ƒ)ì™€ ì‹¤ë‚´ì˜¨ë„(28.50â„ƒ)ì˜ ì°¨ì´ëŠ” 5.50â„ƒìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì§€ë‚œë‹¬ì— ì„¤ì •ì˜¨ë„ì™€ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ê°€ ê°€ìž¥ ë§Žì´ ë‚¬ë˜ ë‚ ì€?; í¬ë§·: ['ì§€ë‚œë‹¬ {{v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ë‚ ì§œ}}ì— ì„¤ì •ì˜¨ë„({{v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì„¤ì •ì˜¨ë„}}â„ƒ)ì™€ ì‹¤ë‚´ì˜¨ë„({{v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì‹¤ë‚´ì˜¨ë„}}â„ƒ) ì°¨ì´ê°€ {{v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ }}â„ƒë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ ': np.float64(1.0), 'v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ë‚ ì§œ': ['2022-08-19'], 'v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì„¤ì •ì˜¨ë„': np.float64(23.0), 'v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì‹¤ë‚´ì˜¨ë„': np.float64(22.0)};\n",
      "ì¶œë ¥: ì§€ë‚œë‹¬ 2022-08-19ì— ì„¤ì •ì˜¨ë„(23.00â„ƒ)ì™€ ì‹¤ë‚´ì˜¨ë„(22.00â„ƒ) ì°¨ì´ê°€ 1.00â„ƒë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜; í¬ë§·: ['ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„({{v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ)ì™€ ì˜†ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„({{v_ì´ë²ˆì£¼_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ) ì°¨ì´ëŠ” {{v_ì´ë²ˆì£¼_ì‹¤ë‚´ì˜¨ë„_í‰ê· _ì°¨ì´}}â„ƒìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(25.66922751621088), 'v_ì´ë²ˆì£¼_ì‹¤ë‚´ì˜¨ë„_í‰ê· _ì°¨ì´': np.float64(0.5593358600971889), 'v_ì´ë²ˆì£¼_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(25.109891656113692)};\n",
      "ì¶œë ¥: ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.67â„ƒ)ì™€ ì˜†ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.11â„ƒ) ì°¨ì´ëŠ” 0.56â„ƒìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: 2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚  ì•Œë ¤ì¤˜; í¬ë§·: ['2ì£¼ì „ {{v_2ì£¼ì „_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë‚ ì§œ}}ì— ì‹¤ë‚´ì˜¨ë„({{v_2ì£¼ì „_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ì‹¤ë‚´ì˜¨ë„}}â„ƒ)ê°€ ê°€ìž¥ ë†’ì•˜ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_2ì£¼ì „_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë‚ ì§œ': ['2022-09-12', '2022-09-13', '2022-09-14', '2022-09-15', '2022-09-16', '2022-09-17', '2022-09-18'], 'v_2ì£¼ì „_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ì‹¤ë‚´ì˜¨ë„': np.float64(26.0)};\n",
      "ì¶œë ¥: 2ì£¼ì „ 2022-09-12ì— ì‹¤ë‚´ì˜¨ë„(26.00â„ƒ)ê°€ ê°€ìž¥ ë†’ì•˜ìŠµë‹ˆë‹¤.\n",
      "ì¶œë ¥: ì£„ì†¡í•©ë‹ˆë‹¤, ['í™”ì„±']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\n",
      "ì¶œë ¥: ì£„ì†¡í•©ë‹ˆë‹¤, ['ìŠµë„']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì§€ë‚œ 3ì¼ ë™ì•ˆ ìš°ë¦¬ë°˜ ì‹¤ë‚´ ì˜¨ë„ í‰ê·  ê°’ ì•Œë ¤ì¤˜.; í¬ë§·: ['ì§€ë‚œ 3ì¼ê°„ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ê°’ì€ {{v_ì§€ë‚œ3ì¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì§€ë‚œ3ì¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(25.13190039934226)};\n",
      "ì¶œë ¥: ì§€ë‚œ 3ì¼ê°„ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ê°’ì€ 25.13â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œì— ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” ì–´ë• ì–´?; í¬ë§·: ['ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” {{v_ì˜¤ëŠ˜ì˜¤í›„5ì‹œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„}}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì˜¤ëŠ˜ì˜¤í›„5ì‹œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„': np.float64(23.0)};\n",
      "ì¶œë ¥: ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ ì•Œë ¤ì¤˜; í¬ë§·: ['ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì€ ê°ê° {{v_ì˜¬í•´ì—¬ë¦„_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœëŒ€}}â„ƒì™€ {{v_ì˜¬í•´ì—¬ë¦„_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ}}â„ƒìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì˜¬í•´ì—¬ë¦„_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœëŒ€': np.float64(27.5), 'v_ì˜¬í•´ì—¬ë¦„_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ': np.float64(22.0)};\n",
      "ì¶œë ¥: ['ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì€ ê°ê° 27.50â„ƒì™€ 22.00â„ƒìž…ë‹ˆë‹¤.']\n",
      "ì§ˆë¬¸: ìš°ë¦¬ë°˜ê³¼ ì•žë°˜ ì¤‘ ê°€ìž¥ ë”ìš´ ë°©ì€?; í¬ë§·: ['í˜„ìž¬ ë‘ ë°© ì¤‘ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë°©}}ì´(ê°€) {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœê³ }}â„ƒë¡œ ê°€ìž¥ ë”ìš´ ë°©ì´ì—ìš”.']; ë°ì´í„°: {'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœê³ ': np.float64(28.5), 'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë°©': array(['ìš°ë¦¬ë°˜'], dtype=object)};\n",
      "ì¶œë ¥: í˜„ìž¬ ë‘ ë°© ì¤‘ ìš°ë¦¬ë°˜ì´(ê°€) 28.50â„ƒë¡œ ê°€ìž¥ ë”ìš´ ë°©ì´ì—ìš”.\n",
      "ì§ˆë¬¸: ì–´ì œ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜; í¬ë§·: ['ì–´ì œ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„({{v_ì–´ì œ_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_í‰ê· }}â„ƒ)ëŠ” ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„({{v_ì–´ì œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„_í‰ê· }}â„ƒ)ë³´ë‹¤ {{v_ì–´ì œ_ì„¤ì •ì˜¨ë„_í‰ê· ì˜_ì°¨ì´}}â„ƒ ë†’ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì–´ì œ_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_í‰ê· ': np.float64(23.0), 'v_ì–´ì œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„_í‰ê· ': np.float64(23.0), 'v_ì–´ì œ_ì„¤ì •ì˜¨ë„_í‰ê· ì˜_ì°¨ì´': np.float64(0.0)};\n",
      "ì¶œë ¥: ì–´ì œ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00â„ƒ)ëŠ” ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00â„ƒ)ë³´ë‹¤ 0.00â„ƒ ë†’ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì˜¨ë„ì°¨ì´ ì•Œë ¤ì¤˜; í¬ë§·: ['ìš°ë¦¬ë°˜({{v_ì˜¤ëŠ˜_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ)ì´ ì˜†ë°˜({{v_ì˜¤ëŠ˜_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ)ë³´ë‹¤ {{v_ì˜¤ëŠ˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ì˜_ì°¨ì´}}â„ƒ ë†’ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì˜¤ëŠ˜_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(27.447997189037245), 'v_ì˜¤ëŠ˜_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(26.646521433591005), 'v_ì˜¤ëŠ˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ì˜_ì°¨ì´': np.float64(0.8014757554462406)};\n",
      "ì¶œë ¥: ['ìš°ë¦¬ë°˜(27.45â„ƒ)ì´ ì˜†ë°˜(26.65â„ƒ)ë³´ë‹¤ 0.80â„ƒ ë†’ìŠµë‹ˆë‹¤.'];\n",
      "ì§ˆë¬¸: ìž‘ë…„ ê²¨ìš¸ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” {{v_ìž‘ë…„ê²¨ìš¸_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ ìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ìž‘ë…„ê²¨ìš¸_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': nan};\n",
      "ì¶œë ¥: ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜¬í•´ ì—¬ë¦„ ì•žë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” {{v_ì˜¬í•´ì—¬ë¦„_ì•žë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ ìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì˜¬í•´ì—¬ë¦„_ì•žë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(26.108745462794918)};\n",
      "ì¶œë ¥: ['ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” 26.11â„ƒ ìž…ë‹ˆë‹¤.']\n",
      "ì§ˆë¬¸: ì˜¬í•´ ë´„ ì˜†ë°˜ ì œì¼ ì¶”ì› ë˜ ë‚  ì•Œë ¤ì¤˜; í¬ë§·: ['ì˜¬í•´ ë´„(3ì›” ~ 5ì›”) ì˜†ë°˜ì€ {{v_ì˜¬í•´ë´„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ_ë‚ ì§œ}}ì— {{v_ì˜¬í•´ë´„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ}}â„ƒë¡œ ì œì¼ ì¶”ì› ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì˜¬í•´ë´„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ': nan, 'v_ì˜¬í•´ë´„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ_ë‚ ì§œ': []};\n",
      "ì¶œë ¥: ì˜¬í•´ ë´„(3ì›” ~ 5ì›”) ì˜†ë°˜ì€ []ì— nanâ„ƒë¡œ ì œì¼ ì¶”ì› ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: 4ì›” ì•žë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['4ì›” ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” {{v_4ì›”_ì•žë°˜_í‰ê· _ì‹¤ë‚´ì˜¨ë„}}â„ƒ ìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_4ì›”_ì•žë°˜_í‰ê· _ì‹¤ë‚´ì˜¨ë„': nan};\n",
      "ì¶œë ¥: 4ì›” ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì´ë²ˆë‹¬ ì¤‘ ìš°ë¦¬ë°˜ ì˜¨ë„ê°€ ê°€ìž¥ ëœ ë”ìš´ë‚ ì´ ì–¸ì œì•¼?; í¬ë§·: ['ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ {{v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ìµœì†Œì‹¤ë‚´ì˜¨ë„_ë‚ ì§œ}}ì— {{v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ìµœì†Œì‹¤ë‚´ì˜¨ë„}}â„ƒë¡œ ê°€ìž¥ ëœ ë”ì› ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {};\n",
      "ì¶œë ¥: ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2024-08-01ì— 23.00â„ƒë¡œ ê°€ìž¥ ëœ ë”ì› ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: 2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ë‚ ì´ ì–¸ì œì•¼?; í¬ë§·: ['2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì€ {{v_2ì£¼ì „_ìš°ë¦¬ë°˜ê³¼ì˜†ë°˜_ì„¤ì •ì˜¨ë„_ìµœì†Œ_ë‚ ì§œ}}ë¡œ {{v_2ì£¼ì „_ìš°ë¦¬ë°˜ê³¼ì˜†ë°˜_ì„¤ì •ì˜¨ë„_ìµœì†Œ}}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_2ì£¼ì „_ìš°ë¦¬ë°˜ê³¼ì˜†ë°˜_ì„¤ì •ì˜¨ë„_ìµœì†Œ': np.float64(23.0), 'v_2ì£¼ì „_ìš°ë¦¬ë°˜ê³¼ì˜†ë°˜_ì„¤ì •ì˜¨ë„_ìµœì†Œ_ë‚ ì§œ': ['2022-09-12', '2022-09-13', '2022-09-14', '2022-09-15', '2022-09-16', '2022-09-17', '2022-09-18']};\n",
      "ì¶œë ¥: 2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì€ 2022-09-12ë¡œ 23.00â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì¶œë ¥: ì£„ì†¡í•©ë‹ˆë‹¤, ['ë’·ë°˜']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ìš°ë¦¬ë°˜ì˜ í˜„ìž¬ ì„¤ì • ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ìš°ë¦¬ë°˜ì˜ í˜„ìž¬ ì„¤ì •ì˜¨ë„ëŠ” {{v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„}}â„ƒìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„': np.float64(23.0)};\n",
      "ì¶œë ¥: ['ìš°ë¦¬ë°˜ì˜ í˜„ìž¬ ì„¤ì •ì˜¨ë„ëŠ” 23.00â„ƒìž…ë‹ˆë‹¤.']\n",
      "ì§ˆë¬¸: 8ì¼ì „ ì„¤ì •ì˜¨ë„ëŠ”?; í¬ë§·: ['8ì¼ì „ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì„¤ì •ì˜¨ë„ëŠ” {{v_8ì¼ì „_ìš°ë¦¬ë°˜_í‰ê· _ì„¤ì •ì˜¨ë„}}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_8ì¼ì „_ìš°ë¦¬ë°˜_í‰ê· _ì„¤ì •ì˜¨ë„': np.float64(23.0)};\n",
      "ì¶œë ¥: 8ì¼ì „ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì„¤ì •ì˜¨ë„ëŠ” 23.00â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: 10ë…„ ì „ ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ ì˜¨ë„ëŠ”?; í¬ë§·: ['10ë…„ ì „ ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” {{v_10ë…„ì „ì˜¤ëŠ˜_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_10ë…„ì „ì˜¤ëŠ˜_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': nan};\n",
      "ì¶œë ¥: 10ë…„ ì „ ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "ì¶œë ¥: ì£„ì†¡í•©ë‹ˆë‹¤, ['ë¡¯ë°ìºìŠ¬']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\n",
      "ì¶œë ¥: ì£„ì†¡í•©ë‹ˆë‹¤, ['1ì¸µ']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ìš°ë¦¬ë°˜, ì˜†ë°˜, ì•žë°˜ ì¤‘ ê°€ìž¥ ì¶”ìš´ ë°©ì€?; í¬ë§·: ['í˜„ìž¬ ì„¸ ë°© ì¤‘ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœì €_ë°©}}ì´(ê°€) {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœì €}}â„ƒë¡œ ê°€ìž¥ ì¶”ì›Œìš”.']; ë°ì´í„°: {'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœì €': np.float64(28.5), 'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœì €_ë°©': array(['ìš°ë¦¬ë°˜'], dtype=object)};\n",
      "ì¶œë ¥: í˜„ìž¬ ì„¸ ë°© ì¤‘ ìš°ë¦¬ë°˜ì´(ê°€) 28.50â„ƒë¡œ ê°€ìž¥ ì¶”ì›Œìš”.\n",
      "ì¶œë ¥: ì£„ì†¡í•©ë‹ˆë‹¤, ['ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì§€ë‚œë‹¬ ì˜¤ëŠ˜ ì˜¤í›„ 2ì‹œì— ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” ì–´ë• ì–´?; í¬ë§·: ['ì§€ë‚œë‹¬ ì˜¤ëŠ˜ ì˜¤í›„ 2ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” {{v_ì§€ë‚œë‹¬ì˜¤ëŠ˜ì˜¤í›„2ì‹œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„}}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì§€ë‚œë‹¬ì˜¤ëŠ˜ì˜¤í›„2ì‹œ_ì˜†ë°˜_ì„¤ì •ì˜¨ë„': 0    23.0\n",
      "Name: ì„¤ì •ì˜¨ë„, dtype: float64};\n",
      "ì¶œë ¥: ì§€ë‚œë‹¬ ì˜¤ëŠ˜ ì˜¤í›„ 2ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜†ë°˜ì˜ í˜„ìž¬ ì˜¨ë„ëž‘ ì„¤ì •ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ì˜†ë°˜ì˜ í˜„ìž¬ ì‹¤ë‚´ì˜¨ë„ëŠ” {{v_í˜„ìž¬_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„}}â„ƒì´ê³ , ì„¤ì •ì˜¨ë„ëŠ” {{v_í˜„ìž¬_ì˜†ë°˜_ì„¤ì •ì˜¨ë„}}â„ƒìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_í˜„ìž¬_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„': np.float64(27.0), 'v_í˜„ìž¬_ì˜†ë°˜_ì„¤ì •ì˜¨ë„': np.float64(23.0)};\n",
      "ì¶œë ¥: ['ì˜†ë°˜ì˜ í˜„ìž¬ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00â„ƒì´ê³ , ì„¤ì •ì˜¨ë„ëŠ” 23.00â„ƒìž…ë‹ˆë‹¤.']\n",
      "ì§ˆë¬¸: ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” {{v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ ìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(25.66922751621088)};\n",
      "ì¶œë ¥: ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.67â„ƒ ìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì„ ì•Œë ¤ì¤˜.; í¬ë§·: ['ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì€ {{v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_í‰ê· }}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì§€ë‚œë‹¬_ìš°ë¦¬ë°˜_ì„¤ì •ì˜¨ë„_í‰ê· ': np.float64(23.019010889292197)};\n",
      "ì¶œë ¥: ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì€ 23.02â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì¶œë ¥: ì£„ì†¡í•©ë‹ˆë‹¤, ['1ì¸µ']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ìš°ë¦¬ë°˜ ì´ë²ˆë‹¬ ì œì¼ ì¶”ì› ë˜ ë‚ ì€ ì–¸ì œëƒ?; í¬ë§·: ['ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ {{v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ_ë‚ ì§œ}}ì— ì‹¤ë‚´ì˜¨ë„ {{v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ_ì˜¨ë„}}â„ƒë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ_ë‚ ì§œ': ['2022-09-07', '2022-09-27'], 'v_ì´ë²ˆë‹¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ_ì˜¨ë„': np.float64(22.5)};\n",
      "ì¶œë ¥: ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-07ì— ì‹¤ë‚´ì˜¨ë„ 22.50â„ƒë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ìž‘ë…„ ì˜†ë°˜ ê°€ìž¥ ë”ì› ë˜ ë‹¬ì€?; í¬ë§·: ['ìž‘ë…„ ì˜†ë°˜ì€ {{v_ìž‘ë…„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë‹¬}}ì— {{v_ìž‘ë…„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ì˜¨ë„}}â„ƒë¡œ ê°€ìž¥ ë”ì› ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ìž‘ë…„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ì˜¨ë„': nan, 'v_ìž‘ë…„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë‹¬': []};\n",
      "ì¶œë ¥: ìž‘ë…„ ì˜†ë°˜ì€ []ì— []ë¡œ ê°€ìž¥ ë”ì› ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜¤ëŠ˜ ì˜¤ì „ 11ì‹œì— ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” ì–´ë• ì–´?; í¬ë§·: ['ì˜¤ëŠ˜ ì˜¤ì „ 11ì‹œ ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” {{v_ì˜¤ì „11ì‹œ_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„}}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì˜¤ì „11ì‹œ_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„': np.float64(27.0)};\n",
      "ì¶œë ¥: ì˜¤ëŠ˜ ì˜¤ì „ 11ì‹œ ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ì‹¤ë‚´ì˜¨ë„ í‰ê·  ì•Œë ¤ì¤˜; í¬ë§·: ['ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” {{v_ì˜¤í›„4ì‹œë¶€í„°6ì‹œ_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒì˜€ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì˜¤í›„4ì‹œë¶€í„°6ì‹œ_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(28.045833333333334)};\n",
      "ì¶œë ¥: ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 28.05â„ƒì˜€ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì§€ë‚œì£¼ì— ì„¤ì •ì˜¨ë„ì™€ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ê°€ ê°€ìž¥ ë§Žì´ ë‚¬ë˜ ë‚ ì€?; í¬ë§·: ['ì§€ë‚œì£¼ {{v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ë‚ ì§œ}}ì— ì„¤ì •ì˜¨ë„({{v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì„¤ì •ì˜¨ë„}}â„ƒ)ì™€ ì‹¤ë‚´ì˜¨ë„({{v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì‹¤ë‚´ì˜¨ë„}}â„ƒ) ì°¨ì´ê°€ {{v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ }}â„ƒë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ ': np.float64(-0.5), 'v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ë‚ ì§œ': ['2022-09-23'], 'v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì„¤ì •ì˜¨ë„': np.float64(23.0), 'v_ì§€ë‚œì£¼_ìš°ë¦¬ë°˜_ì˜¨ë„ì°¨ì´_ìµœê³ _ì‹¤ë‚´ì˜¨ë„': np.float64(23.5)};\n",
      "ì¶œë ¥: ['ì§€ë‚œì£¼ 2022-09-23ì— ì„¤ì •ì˜¨ë„(23.00â„ƒ)ì™€ ì‹¤ë‚´ì˜¨ë„(23.50â„ƒ) ì°¨ì´ê°€ -0.50â„ƒë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.']\n",
      "ì§ˆë¬¸: ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì¤‘ ë” ì¶”ìš´ê³³ì€ ì–´ë””ì•¼?; í¬ë§·: ['{{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë‚®ì€_ê³µê°„}}({{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë‚®ì€_ê³µê°„_ì‹¤ë‚´ì˜¨ë„}}â„ƒ)ì´ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë†’ì€_ê³µê°„}}({{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë†’ì€_ê³µê°„_ì‹¤ë‚´ì˜¨ë„}}â„ƒ)ë³´ë‹¤ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ì°¨ì´}}â„ƒ ë” ë‚®ì•„ìš”.']; ë°ì´í„°: {'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë‚®ì€_ê³µê°„': 'ì˜†ë°˜', 'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë†’ì€_ê³µê°„': 'ìš°ë¦¬ë°˜', 'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ì°¨ì´': np.float64(1.5), 'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë‚®ì€_ê³µê°„_ì‹¤ë‚´ì˜¨ë„': np.float64(27.0), 'v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ë†’ì€_ê³µê°„_ì‹¤ë‚´ì˜¨ë„': np.float64(28.5)};\n",
      "ì¶œë ¥: ['ì˜†ë°˜(27.00â„ƒ)ì´ ìš°ë¦¬ë°˜(28.50â„ƒ)ë³´ë‹¤ 1.50â„ƒ ë” ë‚®ì•„ìš”.']\n",
      "\n",
      "(ë°ì´í„°ì—ì„œ np.float64()ì´ ë¶™ì–´ìžˆëŠ” ê°’ì€ numpyì˜ float64 íƒ€ìž…ìœ¼ë¡œ, ì†Œìˆ˜ì ê¹Œì§€ ì •í™•í•˜ê²Œ í‘œí˜„ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì†Œìˆ˜ì  ë‘˜ì§¸ ìžë¦¬ê¹Œì§€ í‘œí˜„í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "build_query_groundtruth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"QueryTruePositive\"\n",
    "    false_positive = \"QueryFalsePositive\"\n",
    "    false_negative = \"QueryFalseNegative\"\n",
    "    \n",
    "def eval_query(cand_response_filename, db_gt_filename=\"./gts.json\"):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "    response_reports = []\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            # if \"ì˜¤ëŠ˜ ì•„ì¹¨ê³¼ ì €ë…\" not in input:\n",
    "            #     continue\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # ê´€ê³„ ì—†ëŠ” ì§ˆë¬¸ë“¤ì€ ê±´ë„ˆë›°ìž\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            gt_report = gt_report[0]\n",
    "            tags = gt_report[\"Tags\"]\n",
    "            # assert gt_report[\"QueryResults\"] != []\n",
    "            # if gt_report[\"Result\"] == []:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            \n",
    "            gt_results = [d for d in gt_report[\"QueryResults\"]]\n",
    "            gt_query_results = defaultdict(list)\n",
    "            for gt_result in gt_results:\n",
    "                for col in gt_result[\"result_columns\"]:\n",
    "                    gt_query_results[col].extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            gt_response = gt_report[\"Response\"]\n",
    "            # gt_required_variables = gt_report[\"RequiredVariables\"]\n",
    "            # gt_variables_to_report = gt_report[\"VariablesToReport\"]\n",
    "            user_input = gt_report[\"Input\"]\n",
    "\n",
    "            response_report = {\n",
    "                \"Input\": user_input,\n",
    "                \"Metadata\": metadata,\n",
    "                \"GT_Response\": gt_response,\n",
    "                # \"GT_RequiredVariables\": gt_required_variables,\n",
    "                # \"GT_VariablesToReport\": gt_variables_to_report,\n",
    "            }\n",
    "            # ---\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Metadata\"] = metadata\n",
    "            evaluation_report[\"Tags\"] = tags\n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict) and (\"Instruction Set\" in cand_response[\"Candidate\"] or \"ì§€ì‹œ\" in cand_response[\"Candidate\"] or \"Instructions\" in cand_response[\"Candidate\"]):\n",
    "                if \"Instruction Set\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instruction Set\"]\n",
    "                elif \"ì§€ì‹œ\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"ì§€ì‹œ\"]\n",
    "                elif \"Instructions\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instructions\"]\n",
    "\n",
    "                evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                try:\n",
    "                    import re\n",
    "                    # get data between \"Instruction Set\": [ and the last]\n",
    "                    cand_instruction_set = re.search(r'(?<=\"Instructions\": \\[)(.*)(?=\\])', cand_response[\"Candidate\"], re.DOTALL).group(0)\n",
    "                    # find all {\"type\": ~ }, {\"type\": ~ }, {\"type\": ~ }\n",
    "                    cand_instruction_set = re.findall(r'({\"type\".*?})', cand_instruction_set)\n",
    "                    # print(list(cand_instruction_set))\n",
    "                    cand_instruction_set = [eval(d) for d in cand_instruction_set]\n",
    "                except Exception as e:\n",
    "                    evaluation_report[EM.json_structure] = False\n",
    "                    evaluation_report[EM.true_positive] = 0\n",
    "                    evaluation_report[EM.false_positive] = 0\n",
    "                    evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "\n",
    "                    print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                    print(e)\n",
    "                    evaluation_reports.append(evaluation_report)\n",
    "                    pbar.update(1)\n",
    "                    response_reports.append(response_report)\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "            exp_tag = \\\n",
    "                \"woCoTExp\" if \"woCoTExp\" in str(cand_response_filename) else \\\n",
    "                \"woOp\" if \"woOp\" in str(cand_response_filename) else \\\n",
    "                \"woQM\" if \"woQM\" in str(cand_response_filename) else \\\n",
    "                None\n",
    "            exp_tag = \"v2\"\n",
    "            instructions = InputToInstruction.postprocess(\n",
    "                deepcopy(cand_response[\"Candidate\"]), \n",
    "                exp_tag=exp_tag\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, instructions, exp_tag=exp_tag)\n",
    "                response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, instructions)\n",
    "                print(response)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "\n",
    "                response_reports.append(response_report)\n",
    "                            \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # required_variables = summarize_variables_to_report(required_variables)\n",
    "            required_variables = ResponseGeneration.stringify_variables(required_variables)\n",
    "            response_report[\"PD_Response\"] = response\n",
    "            # response_report[\"PD_RequiredVariables\"] = required_variables\n",
    "            # response_report[\"PD_VariablesToReport\"] = variables_to_report\n",
    "            response_reports.append(response_report)\n",
    "\n",
    "            if len(_cand_query_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            cand_query_results = defaultdict(list)\n",
    "            for cand_query_result in _cand_query_results:\n",
    "                for col in cand_query_result[\"result_columns\"]:\n",
    "                    cand_query_results[col].extend(cand_query_result[\"result_indices\"])\n",
    "\n",
    "            cand_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "\n",
    "            if len(gt_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = cand_total_combinations\n",
    "                evaluation_report[EM.false_negative] = 0\n",
    "\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "\n",
    "                continue\n",
    "            \n",
    "            # print(gt_total_combinations, cand_total_combinations)\n",
    "            # True Positive: ê³µí†µëœ ì»¬ëŸ¼ê³¼ ë¡œìš°ì˜ ëª¨ë“  ì¡°í•©\n",
    "            true_positive = 0\n",
    "            false_negative = 0\n",
    "            false_positive = 0\n",
    "            for col in set(gt_query_results.keys())&set(cand_query_results.keys()):\n",
    "                s_gt_query_result = set(gt_query_results[col])\n",
    "                s_cand_query_result = set(cand_query_results[col])\n",
    "                true_positive += len(s_gt_query_result & s_cand_query_result)\n",
    "                false_negative += len(s_gt_query_result - s_cand_query_result)\n",
    "                false_positive += len(s_cand_query_result - s_gt_query_result)\n",
    "\n",
    "                # print(true_positive, false_negative, false_positive, len(s_gt_query_result), len(s_cand_query_result))\n",
    "            # assert true_positive + false_positive + false_negative == gt_total_combinations\n",
    "            \n",
    "\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    with open(f\"{cand_response_filename.replace('.json', '_response.json')}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(response_reports, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_print = eval_df.drop(columns=[\"Input\", \"Metadata\", \"Tags\"])\n",
    "    print(eval_print)\n",
    "    eval_df[EM.true_positive] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[EM.false_positive] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[EM.false_negative] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # replace nan with 0\n",
    "    # eval_df.fillna(0, inplace=True)\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    print(truepos_sum, falsepos_sum, falseneg_sum)\n",
    "    print(precision, recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ì§€ê¸ˆ ì˜†ë°˜ ì˜¨ë„ëž‘ ìš°ë¦¬ë°˜ ì˜¨ë„ ì•Œë ¤ì¤˜:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸: ì§€ê¸ˆ ì˜†ë°˜ ì˜¨ë„ëž‘ ìš°ë¦¬ë°˜ ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['í˜„ìž¬ ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„({{v_í˜„ìž¬_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„}}â„ƒ)ëŠ” ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„({{v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„}}â„ƒ)ë³´ë‹¤ {{v_í˜„ìž¬_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„ - í˜„ìž¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„}}â„ƒ ë†’ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_í˜„ìž¬_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„': 0    27.0\n",
      "Name: ì‹¤ë‚´ì˜¨ë„, dtype: float64, 'v_í˜„ìž¬_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„': 0    28.5\n",
      "Name: ì‹¤ë‚´ì˜¨ë„, dtype: float64};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1ì‹œê°„ ì „ ì„¤ì •ì˜¨ë„ëž‘ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.:  20%|â–ˆâ–ˆ        | 1/5 [00:01<00:04,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ìž¬ ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„(27.00â„ƒ)ëŠ” ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„(28.50â„ƒ)ë³´ë‹¤ -1.50â„ƒ ë‚®ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: 1ì‹œê°„ ì „ ì„¤ì •ì˜¨ë„ëž‘ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.; í¬ë§·: ['1ì‹œê°„ ì „ ì„¤ì •ì˜¨ë„({{v_1ì‹œê°„ì „_ì„¤ì •ì˜¨ë„_ì‹¤ë‚´ì˜¨ë„_ì°¨ì´}}â„ƒ)ëŠ” ì‹¤ë‚´ì˜¨ë„({{v_1ì‹œê°„ì „_ì‹¤ë‚´ì˜¨ë„}}â„ƒ)ë³´ë‹¤ {{v_1ì‹œê°„ì „_ì„¤ì •ì˜¨ë„_ì‹¤ë‚´ì˜¨ë„_ì°¨ì´}}â„ƒ ë†’ìŠµë‹ˆë‹¤.']; ë°ì´í„°: {'v_1ì‹œê°„ì „_ì„¤ì •ì˜¨ë„_ì‹¤ë‚´ì˜¨ë„_ì°¨ì´': 0     -1.0\n",
      "1     -2.5\n",
      "2     -2.5\n",
      "3     -2.5\n",
      "4     -2.5\n",
      "      ... \n",
      "175   -2.5\n",
      "176   -2.5\n",
      "177   -2.5\n",
      "178    0.0\n",
      "179   -2.5\n",
      "Length: 180, dtype: float64};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ì˜†ë°˜ì˜ ê°€ìž¥ ìµœê·¼ ì˜¨ë„ëž‘ ì„¤ì •ì˜¨ë„ ì•Œë ¤ì¤˜:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:02<00:03,  1.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ì‹œê°„ ì „ ì„¤ì •ì˜¨ë„(0.00â„ƒ)ëŠ” ì‹¤ë‚´ì˜¨ë„(28.00â„ƒ)ë³´ë‹¤ -2.50â„ƒ ë†’ìŠµë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì˜†ë°˜ì˜ ê°€ìž¥ ìµœê·¼ ì˜¨ë„ëž‘ ì„¤ì •ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ì˜†ë°˜ì˜ ê°€ìž¥ ìµœê·¼ ì‹¤ë‚´ì˜¨ë„ëŠ” {{v_ê°€ìž¥ìµœê·¼_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„}}â„ƒë¡œ, ì„¤ì •ì˜¨ë„ëŠ” {{v_ê°€ìž¥ìµœê·¼_ì˜†ë°˜_ì„¤ì •ì˜¨ë„}}â„ƒìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ê°€ìž¥ìµœê·¼_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„': 0    27.0\n",
      "Name: ì‹¤ë‚´ì˜¨ë„, dtype: float64, 'v_ê°€ìž¥ìµœê·¼_ì˜†ë°˜_ì„¤ì •ì˜¨ë„': 0    23.0\n",
      "Name: ì„¤ì •ì˜¨ë„, dtype: float64};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì˜¨ë„ ì•Œë ¤ì¤˜:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:02<00:01,  1.13it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.00â„ƒë¡œ, ì„¤ì •ì˜¨ë„ëŠ” 23.00â„ƒìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì˜¨ë„ ì•Œë ¤ì¤˜; í¬ë§·: ['ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” {{v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· }}â„ƒ ìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· ': np.float64(25.66922751621088)};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ì§€ë‚œ í•œë‹¬ê°„ ì„¤ì •ì˜¨ë„ í‰ê· ì„ ì•Œë ¤ì¤˜.:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:03<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” 25.67â„ƒ ìž…ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì§€ë‚œ í•œë‹¬ê°„ ì„¤ì •ì˜¨ë„ í‰ê· ì„ ì•Œë ¤ì¤˜.; í¬ë§·: ['ì§€ë‚œ í•œë‹¬ê°„ ì„¤ì •ì˜¨ë„ í‰ê· ì€ {{v_ì§€ë‚œí•œë‹¬_ì„¤ì •ì˜¨ë„_í‰ê· }}â„ƒ ìž…ë‹ˆë‹¤.']; ë°ì´í„°: {'v_ì§€ë‚œí•œë‹¬_ì„¤ì •ì˜¨ë„_í‰ê· ': np.float64(22.950033280890718)};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ì§€ë‚œ í•œë‹¬ê°„ ì„¤ì •ì˜¨ë„ í‰ê· ì„ ì•Œë ¤ì¤˜.: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§€ë‚œ í•œë‹¬ê°„ ì„¤ì •ì˜¨ë„ í‰ê· ì€ 22.95â„ƒ ìž…ë‹ˆë‹¤.\n",
      "   JsonStructureCorrectness  QueryTruePositive  QueryFalsePositive  \\\n",
      "0                      True                  2                   0   \n",
      "1                      True                  0                 360   \n",
      "2                      True                  2                   0   \n",
      "3                      True               7094                   0   \n",
      "4                      True               2865              129343   \n",
      "\n",
      "   QueryFalseNegative  ExactMatch   Total  \n",
      "0                   0           1       2  \n",
      "1                   2           0     362  \n",
      "2                   0           1       2  \n",
      "3                   0           1    7094  \n",
      "4               41136           0  173344  \n",
      "3.0165278290566735 1.740638835740155 0.24283333520317152\n",
      "0.6341017756176311 0.9254966470528234\n",
      "JsonStructureCorrectness: 1.00\n",
      "ExactMatch: 0.60\n",
      "F1: 0.75\n",
      "Recall: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# name = \"r-v7_r256_a512_ours_tr6_0503-checkpoint-63\"\n",
    "# name = \"r-v7_r256_a512_ours_tr18_0503-checkpoint-52\"\n",
    "# name = \"r-v7_r256_a512_ours_tr30_0503-checkpoint-54\"\n",
    "# name = \"r-v7_r256_a512_ours_tr45_0503-checkpoint-95\"\n",
    "# name = \"r-v7_r256_a512_ours_tr60_0503-checkpoint-108\"\n",
    "\n",
    "# name = \"r-v7_r256_a512_woall_tr6_0503-checkpoint-28\"\n",
    "# name = \"r-v7_r256_a512_woall_tr18_0503-checkpoint-70\"\n",
    "# name = \"r-v7_r256_a512_woall_tr30_0503-checkpoint-57\"\n",
    "# name = \"r-v7_r256_a512_woall_tr45_0503-checkpoint-95\"\n",
    "# name = \"r-v7_r256_a512_woall_tr60_0503-checkpoint-90\"\n",
    "\n",
    "names = [\n",
    "# \"r-v7_r256_a512_ours_tr6_0503-checkpoint-63\",\n",
    "# \"r-v7_r256_a512_ours_tr18_0503-checkpoint-52\",\n",
    "# \"r-v7_r256_a512_ours_tr30_0503-checkpoint-54\",\n",
    "# \"r-v7_r256_a512_ours_tr45_0503-checkpoint-95\",\n",
    "# \"r-v7_r256_a512_ours_tr60_0503-checkpoint-54\",\n",
    "# \"r-v7_r256_a512_woCoT_tr60_0503--checkpoint-84\",\n",
    "# \"r-v7_r256_a512_woCoTExp_tr60_0503--checkpoint-102\",\n",
    "# \"r-v7_r256_a512_woOp_tr60_0503--checkpoint-90\",\n",
    "# \"r-v7_r256_a512_woQM_tr60_0503--checkpoint-54\"\n",
    "# \"r-v7_r170_a340_ours_tr56_0613--checkpoint-60\",\n",
    "# \"r-v7_r256_a512_ours_tr56_0613--checkpoint-68\",\n",
    "\"r-v7_r256_a512_ours_tr17_0613--checkpoint-25\"\n",
    "# \"test\"\n",
    "# \"v8\"\n",
    "]\n",
    "\n",
    "for name in names:\n",
    "    eval_query(\n",
    "        f\"../experiments/{name}.json\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
