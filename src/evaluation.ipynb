{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.db.manager import DBManager\n",
    "from src.input_to_instructions.load_and_execute import *\n",
    "from src.input_to_instructions.types import *\n",
    "from src.operation.execute import *\n",
    "from src.response_generation.load_and_execute import *\n",
    "from src.dateutils import normalize_sql_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "# from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\"\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_implementation: flash_attention_2, torch_dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "print(f\"attn_implementation: {attn_implementation}, torch_dtype: {torch_dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct:\n",
      "- configuration_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.6.9: Fast Siglip patching. Transformers: 4.53.0.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Siglip does not support SDPA - switching to eager!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d13d4191744dfc98c2c5a430f485a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7b546b7eae4d76915ada7682c86f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_exaone.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct:\n",
      "- configuration_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db0e1ab6b424c3c8d6f6ce7aaf2baf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_exaone.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct:\n",
      "- modeling_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea13257e52f4ea0832f35e36af5ddb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945f5c0ad9aa4242b62c1c2ab94ebe48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00007.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6442b93c764b46999dfcaceaf471b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba004d9ae964549bbd17e392eafd94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4554b8b2f524c48a104a42ce3010c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7363f4a15e24b4ebc882c5749245559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dead73449ee48f1955f70409711c285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64e42f997b64366abf46063b15b17d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00007.safetensors:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5804c23665134d6191b91f48636adf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96de2cdcef914f6396f4eb9f6f613ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82413a481f7402d886f156bdae5145f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817c2f7e2fc1440cbb49625c11577566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69936b6bd833427e90ac59ff921aea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc1c751039e4584aaa0c001cbe374de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3d8078b51341df820f3a9591c85c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ResponseGeneration.update_prompt()\n",
    "\n",
    "ResponseGeneration.initialize(\n",
    "    log_output=False,\n",
    "    instance_type=\"unsloth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.input_to_instructions.types import InstructionQ_raw\n",
    "def get_time(df, fmt=\"datetime\"):\n",
    "    # from df get 'timestamp' column and return them in format\n",
    "    if fmt == \"date\":\n",
    "        fmt = '%Y-%m-%d'\n",
    "    elif fmt == \"month\":\n",
    "        fmt = '%Y-%m'\n",
    "    elif fmt == \"year\":\n",
    "        fmt = '%Y'\n",
    "    else:\n",
    "        fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    if isinstance(df['timestamp'], pd.Timestamp):\n",
    "        result = df['timestamp'].strftime(fmt)\n",
    "    else:\n",
    "        result = df['timestamp'].apply(lambda x: x.strftime(fmt))\n",
    "    return sorted(list(set(result)))\n",
    "\n",
    "def get_spatials(df):\n",
    "    return pd.unique(df['idu_name'])\n",
    "\n",
    "def get_tv(df, col:str|list[str], fmt=\"datetime\"):\n",
    "    if isinstance(col, str):\n",
    "        col = [col]\n",
    "    \n",
    "    timestamps = get_time(df, fmt)\n",
    "    return_tuple = tuple([timestamps] + [df[c] for c in col])\n",
    "    return return_tuple\n",
    "\n",
    "def data_(metadata, mapping, query_results, t=str|list[str], s=str|list[str], m=str|list[str]):\n",
    "    if isinstance(t, str):\n",
    "        t = [t]\n",
    "    if isinstance(s, str):\n",
    "        s = [s]\n",
    "    if isinstance(m, str):\n",
    "        m = [m]\n",
    "\n",
    "    t_raw = [mapping.temporal[t_highlevel] for t_highlevel in t]\n",
    "    s_raw = [mapping.spatials[s_highlevel] for s_highlevel in s]\n",
    "    m_raw = [mapping.modalities[m_highlevel] for m_highlevel in m]\n",
    "    \n",
    "    # flatten s_raw into a list of strings\n",
    "    # flattened = [item for sublist in data for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    s_raw = [item for sublist in s_raw for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    # print(s_raw)\n",
    "    result_df = DBManager.structured_query_data_t_v2(metadata, m_raw, t_raw, s_raw, get_rowids=True)\n",
    "    \n",
    "    cols = list(result_df.columns)\n",
    "    cols.remove(\"id\")\n",
    "    cols.remove(\"idu_name\")\n",
    "    cols.remove(\"timestamp\")\n",
    "    rows = list(result_df[\"id\"])\n",
    "    query_results.append({\n",
    "        \"result_columns\": cols,\n",
    "        \"result_indices\": rows,\n",
    "    })\n",
    "    # print(cols, rows)\n",
    "\n",
    "    # For demo, drop rows where any value is -1\n",
    "    result_df = result_df.loc[(result_df != -1).all(axis=1)]\n",
    "\n",
    "    # drop \"id\" from result_df\n",
    "    result_df = result_df.drop(columns=['id'])\n",
    "\n",
    "    # change column names to high level\n",
    "    inverse_mapping = {v: k for k, v in mapping.modalities.items()}\n",
    "    result_df.columns = [inverse_mapping[col] if col in inverse_mapping else col for col in result_df.columns]\n",
    "\n",
    "    # change idu_name raw values to high level\n",
    "    inverse_mapping = {}\n",
    "    for k, v in mapping.spatials.items():\n",
    "        if isinstance(v, list):\n",
    "            for v_ in v:\n",
    "                inverse_mapping[v_] = k\n",
    "        else:\n",
    "            inverse_mapping[v] = k\n",
    "\n",
    "    result_df[\"idu_name\"] = result_df[\"idu_name\"].map(inverse_mapping)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def run_query_v2(user_input, metadata, mapping, expectations, required_variables, scripts, exp_tag=None):\n",
    "    query_results = []\n",
    "    variables = {}\n",
    "    # print(f\"exp_tag: {exp_tag}\")\n",
    "    if scripts is not None:\n",
    "\n",
    "        # search data(t=~~, ...,)\n",
    "        globals()['metadata'] = metadata\n",
    "        globals()['mapping'] = mapping\n",
    "        globals()['query_results'] = query_results\n",
    "        for name in list(globals()):\n",
    "            if name.startswith(\"v_\"):\n",
    "                del globals()[name]\n",
    "        try:\n",
    "            for script in scripts:\n",
    "                try:\n",
    "                \n",
    "                    if \"data\" in script:\n",
    "                        script = script.replace(\"data(\", \"data_(metadata, mapping, query_results, \")\n",
    "                    \n",
    "                    if \"SELECT\" in script:\n",
    "                        # split only at the first '=' to avoid issues with '=' in SQL\n",
    "                        variable, sql = script.split(\"=\", 1)\n",
    "                        variable = variable.strip()\n",
    "                        sql = sql.strip()\n",
    "                        # get all between \\\" and \\\"\n",
    "                        sql = re.findall(r'\"(.*)\"', sql)\n",
    "                        sql = sql[0]\n",
    "                        # \"SELECT\"ÎùºÎäî Ï≤´ Î≤àÏß∏ Îì±Ïû•Îßå \"SELECT id \"Î°ú ÎåÄÏ≤¥Ìï©ÎãàÎã§.\n",
    "                        sql = sql.replace(\"SELECT\", \"SELECT id, \", 1)\n",
    "                        df = DBManager.execute_structured_query_string(sql)\n",
    "                        cols = list(df.columns)\n",
    "                        cols.remove(\"id\")\n",
    "                        cols.remove(\"idu_name\")\n",
    "                        cols.remove(\"timestamp\")\n",
    "                        rows = list(df[\"id\"])\n",
    "                        query_results.append({\n",
    "                            \"result_columns\": cols,\n",
    "                            \"result_indices\": rows,\n",
    "                        })\n",
    "                        df = df.drop(columns=['id'])\n",
    "                        globals()[variable] = df\n",
    "                    else:\n",
    "                        exec(script, globals())\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in executing script: {script}\")\n",
    "                    print(e)\n",
    "                    raise e\n",
    "        \n",
    "            variables = {name:globals()[name] for name in globals() if name.startswith(\"v_\")}\n",
    "            response, required_variables = ResponseGeneration.execute_v2(expectations, required_variables, variables, user_input, exp_tag=exp_tag)\n",
    "            return response, variables, required_variables, query_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error in running query_v2: {e}\")\n",
    "            return \"Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.\", variables, None, query_results\n",
    "    else:\n",
    "        if exp_tag in [\"woQM\", \"woQM+Script\"]:\n",
    "            response, required_variables = ResponseGeneration.execute_v2(expectations, required_variables, variables, user_input, exp_tag=exp_tag)\n",
    "            return response, variables, required_variables, query_results\n",
    "        else:\n",
    "            variables = {}\n",
    "            unknown_spatials = [k for k, v in mapping.spatials.items() if v == \"Unknown\"]\n",
    "            unknown_modalities = [k for k, v in mapping.modalities.items() if v == \"Unknown\"]\n",
    "            \n",
    "            response_unknown = f\"Ï£ÑÏÜ°Ìï©ÎãàÎã§, {unknown_spatials + unknown_modalities}Îäî Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≥µÍ∞ÑÏù¥ÎÇò Î™®Îã¨Î¶¨Ìã∞ ÏûÖÎãàÎã§.\"\n",
    "            return response_unknown, variables, [], query_results\n",
    "\n",
    "\n",
    "def run_query(user_input, metadata, instructions, exp_tag=None):\n",
    "    variables = {\n",
    "        \"Metadata\": metadata,\n",
    "    }\n",
    "    query_results = []\n",
    "        \n",
    "    \n",
    "    for instruction in instructions:\n",
    "        # logger.debug(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        # print(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        \n",
    "        if type(instruction) == InstructionQ:\n",
    "            # Execute query\n",
    "            result_df = DBManager.structured_query_data_t(metadata, instruction.args, get_rowids=True)\n",
    "            # if result_df is None:\n",
    "                # print(\"Ï£ÑÏÜ°Ìï©ÎãàÎã§, Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\", \"response\")\n",
    "                # return\n",
    "\n",
    "            cols = list(result_df.columns)\n",
    "            cols.remove(\"id\")\n",
    "            cols.remove(\"idu\")\n",
    "            rows = list(result_df[\"id\"])\n",
    "\n",
    "            query_results.append({\n",
    "                \"result_columns\": cols,\n",
    "                \"result_indices\": rows,\n",
    "            })\n",
    "\n",
    "            # For demo, drop rows where any value is -1\n",
    "            result_df = result_df.loc[(result_df != -1).all(axis=1)]\n",
    "\n",
    "            # drop \"id\" from result_df\n",
    "            result_df = result_df.drop(columns=['id'])\n",
    "           \n",
    "            #pd.set_option('display.max_rows', 10000)        \n",
    "            #pd.set_option('display.max_columns', 1000)\n",
    "            #pd.set_option('display.width', 1000)\n",
    "            #pd.set_option('display.max_colwidth', 1000)\n",
    "            #print(f\"QueryResult: {result_df}\")\n",
    "\n",
    "            variables[instruction.result_name] = result_df\n",
    "        elif type(instruction) == InstructionQ_raw:\n",
    "            instruction.query = instruction.query.replace(\" FROM \\\"data_t\\\"\", \", \\\"id\\\" FROM \\\"data_t\\\"\")\n",
    "            result_df = DBManager.execute_structured_query_string(\n",
    "                instruction.query\n",
    "            )\n",
    "            # rename idu_name to idu\n",
    "            result_df = result_df.rename(columns={'idu_name': 'idu'})\n",
    "            \n",
    "            cols = list(result_df.columns)\n",
    "            cols.remove(\"id\")\n",
    "            cols.remove(\"idu\")\n",
    "            rows = list(result_df[\"id\"])\n",
    "\n",
    "            query_results.append({\n",
    "                \"result_columns\": cols,\n",
    "                \"result_indices\": rows,\n",
    "            })\n",
    "\n",
    "            # drop \"id\" from result_df\n",
    "            result_df = result_df.drop(columns=['id'])\n",
    "            \n",
    "            variables[instruction.result_name] = result_df\n",
    "            # print(result_df, flush=True)\n",
    "\n",
    "        elif type(instruction) == InstructionO:\n",
    "            # Execute operation\n",
    "            # variables_to_report = {k: v for k, v in variables.items() if k not in [\"Metadata\"]}\n",
    "            # print(variables_to_report)\n",
    "            result_dict = OperationExecutor.execute(variables, instruction.scripts)\n",
    "            # print(instruction.scripts, instruction.returns, result_dict)\n",
    "            variables.update(result_dict)\n",
    "            pass\n",
    "            # print(fig, \"graph\")\n",
    "        elif type(instruction) == InstructionR:\n",
    "            # Execute response generation\n",
    "            variables_to_report = {k: v for k, v in variables.items() if k not in [\"Metadata\"]}\n",
    "            # print(variables_to_report)\n",
    "            # variables_to_report = ResponseGeneration.stringify_variables(variables_to_report)\n",
    "            # variables_to_report = summarize_variables_to_report(variables_to_report)\n",
    "\n",
    "            # print(f\"Variables: {variables_to_report}\")\n",
    "\n",
    "            keys_to_leave = [\"modality_mapping\", \"idu_mapping\"]\n",
    "            metadata_ = {}\n",
    "            for key in metadata.keys():\n",
    "                if key in keys_to_leave:\n",
    "                    metadata_[key] = metadata[key]\n",
    "\n",
    "            response, required_variables = ResponseGeneration.execute(instruction, variables, user_input, metadata_, exp_tag=exp_tag)\n",
    "            # print(f\"Required variables: {required_variables}\")\n",
    "            \n",
    "            # response = instruction.expectations[0] # \"{{var}}...\"\n",
    "            # for var_name, var_value in required_variables.items():\n",
    "            #     placeholder = f\"{{{{{var_name}}}}}\"\n",
    "            #     if placeholder in response:\n",
    "            #         response = response.replace(placeholder, str(var_value))\n",
    "\n",
    "            \n",
    "            return response, variables_to_report, required_variables, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dataset_name}\")\n",
    "\n",
    "def build_query_groundtruth():\n",
    "    \n",
    "    def read(path):\n",
    "        data = read_json(path)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = directory.name\n",
    "            if \"v7\" in dataset_name:\n",
    "                data[i][\"Metadata\"] = metadata\n",
    "        return data\n",
    "\n",
    "    ds_ts = []\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir():\n",
    "            if \"v7\" in dataset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            \n",
    "            # d = read(f\"{directory}/onlyq_ts.json\")\n",
    "            \n",
    "            ds_ts.extend(read(f\"{directory}/onlyq_ts.json\"))\n",
    "            # ds_ts.extend(read(f\"{directory}/onlyq_tr.json\"))\n",
    "            # ds_tr.extend(read(f\"{directory}/graph.json\"))\n",
    "    \n",
    "    ds = ds_ts\n",
    "    print(len(ds))\n",
    "    \n",
    "    gts = []\n",
    "\n",
    "    for d in ds:\n",
    "        cont = False\n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "        for st in skip_tags:\n",
    "            if st in tags:\n",
    "                cont = True\n",
    "                break\n",
    "        if cont:\n",
    "            continue\n",
    "\n",
    "        # pbar.set_description(f\"Processing {d['Input']}\")\n",
    "        # print(\"--\")\n",
    "        exp_tag = \"v2\"\n",
    "        # print(f\"Warning! exp_tag is v2\")\n",
    "        mapping, expectations, required_variables, scripts = InputToInstruction.postprocess_v2(deepcopy(d['Response']), exp_tag=exp_tag)\n",
    "        user_input, tags, metadata, scenario = d[\"Input\"], d[\"Tags\"], d[\"Metadata\"], d[\"Scenario\"]\n",
    "        # if user_input != \"ÏßÄÍ∏à Î™áÏãúÏïº?\":\n",
    "        #     continue\n",
    "\n",
    "        response, variables_to_report, required_variables, query_results = run_query_v2(\n",
    "            user_input, metadata, mapping, expectations, required_variables, scripts, exp_tag=exp_tag\n",
    "        )\n",
    "        print(f\"Ï∂úÎ†•: {response}\")\n",
    "        # print({k: (v, type(v)) for k, v in variables_to_report.items()})\n",
    "        gts.append({\n",
    "            \"Input\": user_input,\n",
    "            \"Metadata\": metadata,\n",
    "            \"Scenario\": scenario,\n",
    "            \"Tags\": tags,\n",
    "            \"GT\": d['Response'],\n",
    "            \"Response\": response,\n",
    "            # \"RequiredVariables\": required_variables,\n",
    "            \"QueryResults\": query_results,\n",
    "            # \"VariablesToReport\": variables_to_report,\n",
    "        })\n",
    "\n",
    "    # save to json\n",
    "    with open(f\"./gts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(gts, f, ensure_ascii=False, indent=4)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ResponseGeneration.update_prompt()\n",
    "# build_query_groundtruth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class UnslothInference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint_dir: str,\n",
    "        cache_dir: str,\n",
    "        max_seq_length: int = 3500,\n",
    "        attn_implementation: str = attn_implementation,\n",
    "        batch_size: int = 8  # Î∞∞Ïπò ÌÅ¨Í∏∞ Îß§Í∞úÎ≥ÄÏàò Ï∂îÍ∞Ä\n",
    "    ):\n",
    "        if 'checkpoint' in checkpoint_dir:\n",
    "            self.checkpoint_dir = Path(checkpoint_dir)\n",
    "            if not self.checkpoint_dir.exists():\n",
    "                raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "        else:\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.attn_implementation = attn_implementation\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Verify model files exist\n",
    "        # if not (self.checkpoint_dir / \"config.json\").exists():\n",
    "        #     raise ValueError(f\"config.json not found in {checkpoint_dir}\")\n",
    "        \n",
    "        # Set torch dtype based on GPU capability\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize model and tokenizer for the given rank.\"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            try:\n",
    "                if isinstance(self.checkpoint_dir, Path):\n",
    "                    checkpoint_dir = self.checkpoint_dir.as_posix()\n",
    "                else:\n",
    "                    checkpoint_dir = self.checkpoint_dir\n",
    "\n",
    "                self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "                    checkpoint_dir,\n",
    "                    dtype = self.torch_dtype,\n",
    "                    load_in_4bit = False,\n",
    "                    load_in_8bit = False,\n",
    "                    attn_implementation=self.attn_implementation,\n",
    "                    cache_dir=self.cache_dir.as_posix(),\n",
    "                    local_files_only=True,\n",
    "                    device_map=\"cuda\",\n",
    "                )\n",
    "                FastLanguageModel.for_inference(self.model)\n",
    "            \n",
    "                self.tokenizer.padding_side = \"left\"\n",
    "                print(f\"Model loaded from {self.checkpoint_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in setup_model {str(e)}\")\n",
    "                raise\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_content(text: str):\n",
    "        \"\"\"Extract content from model output.\"\"\"\n",
    "        if \"start_header_id\" in text:\n",
    "            pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "        elif \"start_of_turn\" in text:\n",
    "            pattern = r\"<start_of_turn>model\\n(.*?)<eos>\"\n",
    "        elif \"im_start\" in text:\n",
    "            # <|im_start|>assistant{\"Thinking\": \"ÏÇ¨Ïö©ÏûêÎäî Ïò§Îäò 4Ï∏µÏóê ÏûàÎäî Î™®Îì† Î∞©Ïùò ÏÑ§Ï†ïÏò®ÎèÑÏùò ÌèâÍ∑†Í∞íÏùÑ ÏïåÍ≥† Ïã∂Ïñ¥Ìï©ÎãàÎã§. 4Ï∏µÏóê Ìï¥ÎãπÌïòÎäî iduÎì§(01_IB7, 02_I84, 02_I85)Ïùò Ïò§Îäò ÏÑ§Ï†ïÏò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏøºÎ¶¨Ìïú ÌõÑ ÌèâÍ∑†Í∞íÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôòÌïòÎ©¥ Îê©ÎãàÎã§.\", \"Expectations\": [\"Ïò§Îäò 4Ï∏µÏùò ÌèâÍ∑† ÏÑ§Ï†ïÏò®ÎèÑÎäî {{settemp_avg}}‚ÑÉ ÏûÖÎãàÎã§.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"table_name\": \"data_t\", \"columns\": [\"settemp\"], \"temporal\": \"[DATE_TRUNC('day', DATE 'CURRENT_DATE'), DATE_TRUNC('day', DATE 'CURRENT_DATE' + INTERVAL '1 day'))\", \"spatials\": [\"01_IB7\", \"02_I84\", \"02_I85\"]}, \"result_name\": \"qr\"}, {\"type\": \"o\", \"script\": \"settemp_avg = qr['settemp'].mean();\", \"returns\": [\"settemp_avg\"]}]}<|im_end|>\n",
    "            pattern = r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\"\n",
    "        elif \"|endofturn|\" in text:\n",
    "            pattern = r\"\\[\\|assistant\\|\\](.*?)\\[\\|endofturn\\|\\]\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def process_batch(\n",
    "        self,\n",
    "        batch_data,\n",
    "        common_prompt,\n",
    "    ):\n",
    "        try:\n",
    "            batch_data = Dataset.from_list(batch_data)\n",
    "            model: AutoModelForCausalLM = self.model\n",
    "            tokenizer: AutoTokenizer = self.tokenizer\n",
    "\n",
    "            convos = []\n",
    "            for metadata, input in zip(batch_data[\"Metadata\"], batch_data[\"Input\"]):\n",
    "                if \"llama\" in model.config.architectures[0].lower():\n",
    "                    chat = [\n",
    "                        {\"role\": \"system\", \"content\": common_prompt},\n",
    "                        {\"role\": \"user\", \"content\": f\"Metadata:{metadata};Input:{input};\"},\n",
    "                    ]\n",
    "                elif \"gemma\" in model.config.architectures[0].lower():\n",
    "                    chat = [\n",
    "                        {\"role\": \"user\", \"content\": f\"{common_prompt};{json.dumps(metadata)};{input}\"},\n",
    "                    ]\n",
    "                else:\n",
    "                    chat = [\n",
    "                        {\"role\": \"system\", \"content\": common_prompt},\n",
    "                        {\"role\": \"user\", \"content\": f\"Metadata:{metadata};Input:{input};\"},\n",
    "                    ]\n",
    "                    # raise ValueError(f\"Unsupported model architecture: {model.config.architectures[0]}\")\n",
    "                \n",
    "                chat = tokenizer.apply_chat_template(\n",
    "                    chat,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(model.device)\n",
    "                convos.append(chat)\n",
    "            \n",
    "            max_length = max(inputs.size(1) for inputs in convos)\n",
    "        \n",
    "            # Ìå®Îî© Ï†ÅÏö©ÌïòÏó¨ ÏûÖÎ†• Ï§ÄÎπÑ\n",
    "            padded_inputs = []\n",
    "            attention_masks = []\n",
    "            \n",
    "            for inputs in convos:\n",
    "                pad_length = max_length - inputs.size(1)\n",
    "                \n",
    "                if pad_length > 0:\n",
    "                    # Ìå®Îî© Ï∂îÍ∞Ä\n",
    "                    padded = torch.cat([\n",
    "                        torch.full((1, pad_length), tokenizer.pad_token_id, device=model.device),\n",
    "                        inputs,\n",
    "                    ], dim=1)\n",
    "                    \n",
    "                    # Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ ÏÉùÏÑ± (ÏõêÎ≥∏ ÏãúÌÄÄÏä§Îäî 1, Ìå®Îî©ÏùÄ 0)\n",
    "                    mask = torch.cat([\n",
    "                        torch.zeros(1, pad_length, device=model.device),\n",
    "                        torch.ones(1, inputs.size(1), device=model.device),\n",
    "                    ], dim=1)\n",
    "                else:\n",
    "                    padded = inputs\n",
    "                    mask = torch.ones(1, inputs.size(1), device=model.device)\n",
    "                \n",
    "                padded_inputs.append(padded)\n",
    "                attention_masks.append(mask)\n",
    "            \n",
    "            # Î∞∞Ïπò ÌÖêÏÑú ÏÉùÏÑ±\n",
    "            batch_tensor = torch.cat(padded_inputs, dim=0)\n",
    "            attention_mask = torch.cat(attention_masks, dim=0)\n",
    "            # print(batch_tensor)\n",
    "            # Î∞∞Ïπò Ï∂îÎ°† Ïã§Ìñâ\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch_tensor,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=self.max_seq_length,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False  # Í≤∞Ï†ïÎ°†Ï†Å ÏÉùÏÑ±\n",
    "            )\n",
    "            \n",
    "            # Í≤∞Í≥º ÎîîÏΩîÎî© Î∞è ÌååÏã±\n",
    "            responses = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "            print(responses)\n",
    "            parsed_responses = []\n",
    "            for response in responses:\n",
    "                parsed = self.extract_content(response)\n",
    "                if parsed is None:\n",
    "                    print(f\"Error parsing response: {response[:100]}...\")\n",
    "                    parsed_responses.append(None)\n",
    "                else:\n",
    "                    parsed_responses.append(parsed)\n",
    "            # print(f\"Elapsed time: {end_time - start_time:.2f}s\")\n",
    "            return parsed_responses\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_batch: {str(e)}\")\n",
    "            return [None] * len(batch_data)\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        dataset,\n",
    "        common_prompt: str,\n",
    "        output_file: str\n",
    "    ):\n",
    "        \"\"\"Run inference in batches.\"\"\"\n",
    "            \n",
    "        # Setup model and tokenizer\n",
    "        self.setup_model()\n",
    "\n",
    "        self.model\n",
    "        # Î∞∞Ïπò Ï≤òÎ¶¨\n",
    "        # start_time = time.time()\n",
    "        with tqdm(total=len(dataset)) as pbar:\n",
    "            for batch_start in range(0, len(dataset), self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, len(dataset))\n",
    "                batch_data = dataset[batch_start:batch_end]\n",
    "                \n",
    "                # Î∞∞Ïπò Ï≤òÎ¶¨\n",
    "                responses = self.process_batch(\n",
    "                    batch_data, common_prompt\n",
    "                )\n",
    "                \n",
    "                # Í≤∞Í≥º Ï†ÄÏû•\n",
    "                for i, response in enumerate(responses):\n",
    "                    sample = batch_data[i]\n",
    "                    \n",
    "                    if response is not None:\n",
    "                        try:\n",
    "                            response = eval(response)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in eval: {str(e)}\")\n",
    "                        \n",
    "                        result = {\n",
    "                            \"Input\": sample[\"Input\"],\n",
    "                            \"Scenario\": sample[\"Scenario\"],\n",
    "                            \"Metadata\": sample[\"Metadata\"],\n",
    "                            \"Candidate\": response,\n",
    "                        }\n",
    "                        \n",
    "                        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                            f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "                    else:\n",
    "                        print(f\"Error in response for sample {batch_start + i}\")\n",
    "                \n",
    "                pbar.update(batch_end - batch_start)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_dataset(dir, path, train_type):\n",
    "\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    result = []\n",
    "    for d in data:\n",
    "        if train_type in [\"WoThinking\", \"WoMetadata+Thinking\"]:\n",
    "            del d[\"Response\"][\"Thinking\"]\n",
    "        elif train_type in [\"woExp\"]:\n",
    "            del d[\"Response\"][\"Expectations\"]\n",
    "        \n",
    "        if \"Script\" in d[\"Response\"]:\n",
    "            if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "                new_scripts = []\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" in script:\n",
    "                        new_scripts.append(script)\n",
    "                d[\"Response\"][\"Script\"] = new_scripts\n",
    "\n",
    "            if train_type in [\"woQM\"]:\n",
    "                new_scripts = []\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" in script:\n",
    "                        new_scripts.append(script)\n",
    "                    else:\n",
    "                        for m, n in [(\"Ïã§ÎÇ¥Ïò®ÎèÑ\", \"roomtemp\"), (\"ÏÑ§Ï†ïÏò®ÎèÑ\", \"settemp\")]:\n",
    "                            script = script.replace(f\"'{m}'\", f\"'{n}'\")\n",
    "                        \n",
    "                        new_scripts.append(script)\n",
    "                d[\"Response\"][\"Script\"] = new_scripts\n",
    "\n",
    "            if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "                mapping = d[\"Response\"][\"Mapping\"]\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" not in script:\n",
    "                        continue\n",
    "\n",
    "                    t_match = re.search(r\"t=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    s_match = re.search(r\"s=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    m_match = re.search(r\"m=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    t = eval(t_match.group(1)) if t_match else None\n",
    "                    s = eval(s_match.group(1)) if s_match else None\n",
    "                    m = eval(m_match.group(1)) if m_match else None\n",
    "                    \n",
    "                    if isinstance(t, str):\n",
    "                        t = [t]\n",
    "                    if isinstance(s, str):\n",
    "                        s = [s]\n",
    "                    if isinstance(m, str):\n",
    "                        m = [m]\n",
    "\n",
    "                    t_raw = [mapping['temporal'][t_highlevel] for t_highlevel in t]\n",
    "                    s_raw = [mapping['spatials'][s_highlevel] for s_highlevel in s]\n",
    "                    m_raw = [mapping['modalities'][m_highlevel] for m_highlevel in m]\n",
    "                    s_raw = [item for sublist in s_raw for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "                    sql = DBManager.get_query_strings_v2(\n",
    "                        metadata, m_raw, t_raw, s_raw\n",
    "                    )\n",
    "                    sql = normalize_sql_dates(sql)\n",
    "                    # replace data(...) with sql using regex\n",
    "                    d[\"Response\"][\"Script\"][i] = re.sub(r\"data\\(([^)]+)\\)\", lambda x: f\"\\\"{sql}\\\"\", script)\n",
    "                del d[\"Response\"][\"Mapping\"]\n",
    "            #     # raise NotImplementedError\n",
    "            # elif train_type in [\"woOp\"]:\n",
    "            #     instructions = d[\"Response\"][\"Instructions\"]\n",
    "            #     d[\"Response\"][\"Instructions\"] = [i for i in instructions if i[\"type\"] == \"q\"]\n",
    "\n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "\n",
    "        skip = False\n",
    "        for skip_tag in skip_tags:\n",
    "            if skip_tag in tags:\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Scenarios\": dir.name, \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "        # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "        # print(f\"Read {len(result)} examples from {path}\")\n",
    "        # print(f\"Type of result: {type(result)}\")\n",
    "        # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "        # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "        # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "def sub(name, common_prompt):\n",
    "    # Remove the section between <|name|> ... <|name|> including the tags themselves\n",
    "    # Use re.DOTALL to match newlines with '.'\n",
    "    pattern = rf\"\\n?<\\|{name}\\|>[\\s\\S]*?<\\|{name}\\|>\"\n",
    "    common_prompt = re.sub(pattern, \"\", common_prompt, flags=re.DOTALL)\n",
    "    return common_prompt\n",
    "\n",
    "def run_inference(checkpoint_number, train_type):\n",
    "    # ---\n",
    "    dataset = []\n",
    "    for scenario_dir in [d for d in base_dataset_dir.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]:\n",
    "        data = read_dataset(scenario_dir, \"onlyq_ts.json\", train_type)\n",
    "        print(data)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = scenario_dir.name\n",
    "        dataset.extend(data)\n",
    "\n",
    "    # ---\n",
    "    common_prompt = open(base_dataset_dir / f\"prompt.txt\", \"r\").read()\n",
    "    \n",
    "    sub_targets = []\n",
    "    if train_type == \"ours\":\n",
    "        sub_targets = []\n",
    "    elif train_type == \"BASE\":\n",
    "        sub_targets = [\"Thinking\", \"Expectation\", \"Mapping\", \"Script\", \"Examples\"]\n",
    "    elif train_type in [\"WoThinking\"]:\n",
    "        sub_targets = [\"Thinking\"]\n",
    "    elif train_type in [\"woMetadata\"]:\n",
    "        sub_targets = [\"Metadata\"]\n",
    "    elif train_type in [\"WoMetadata+Thinking\"]:\n",
    "        sub_targets = [\"Metadata\", \"Thinking\"]\n",
    "    elif train_type in [\"woExp\"]:\n",
    "        sub_targets = [\"Expectation\"]\n",
    "\n",
    "    if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "        sub_targets = [\"QM\", \"Mapping\"]\n",
    "    if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "        sub_targets = [\"Script\"]\n",
    "\n",
    "    for sub_target in sub_targets:\n",
    "        common_prompt = sub(sub_target, common_prompt)\n",
    "\n",
    "    # remove all <||>\n",
    "    common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "\n",
    "    if train_type in [\"0SL\",\"5SL\", \"ALLSL\"]:\n",
    "        # n-shot prompting with trainset\n",
    "        \n",
    "        datas = []\n",
    "        for directory in base_dataset_dir.iterdir():\n",
    "            if directory.is_dir():\n",
    "                # Note: metadata ÏïàÎÑ£Ïùå\n",
    "                data = read_json(f\"{directory}/onlyq_ts.json\")\n",
    "                for d in data:\n",
    "                    del d[\"Tags\"]\n",
    "\n",
    "                datas.extend(data)\n",
    "        if \"ALL\" not in train_type:\n",
    "            n = int(train_type.split(\"SL\")[0])\n",
    "            datas = datas[:n]\n",
    "\n",
    "        # n-shot prompting with testset\n",
    "        if train_type != \"0SL\": \n",
    "            data_str = \"\\n[ÏòàÏãú]\\n\" + \"\\n\".join([f\"ÏûÖÎ†•: {d['Input']}\\nÏ∂úÎ†•: {d['Response']}\" for d in datas])\n",
    "            common_prompt = common_prompt + data_str\n",
    "    print(common_prompt)\n",
    "    # ---\n",
    "    model_name = \"sh2orc-Llama-3.1-Korean-8B-Instruct\"\n",
    "    model_dir = Path(f\"/model/{model_name}\")\n",
    "    cache_dir = Path(f\"{model_dir}/cache\")\n",
    "    \n",
    "    checkpoint_dir = None\n",
    "    if checkpoint_number == 0:\n",
    "        checkpoint_dir = \"sh2orc/Llama-3.1-Korean-8B-Instruct\"\n",
    "        output_file = f\"../experiments/r-v7_not_trained_{train_type}_tr27_0623.json\"\n",
    "        max_seq_length = 3000\n",
    "    else:\n",
    "        r = 211\n",
    "        tr_dir = f\"v7_r{r}_a{2*r}_{train_type}_tr27_0623\"\n",
    "\n",
    "        checkpoint_dir = Path(f\"{model_dir}/chkpts/{tr_dir}\")\n",
    "        print(checkpoint_dir)\n",
    "\n",
    "        checkpoint_dir = sorted(checkpoint_dir.iterdir(), key=lambda x: int(x.name.split(\"-\")[-1]))[-1]\n",
    "        # tr_config = f\"{tr_dir}/{checkpoint_dir.name}\"\n",
    "        tr_config = f\"{tr_dir}/checkpoint-{checkpoint_number}\"\n",
    "        print(tr_config)\n",
    "        checkpoint_dir = Path(f\"{model_dir}/chkpts/{tr_config}\")\n",
    "        \n",
    "        print(f\"Model: {model_name}, Config: {tr_config}\")\n",
    "\n",
    "    \n",
    "        # Verify paths exist\n",
    "        if not checkpoint_dir.exists():\n",
    "            raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "\n",
    "        output_file = f\"../experiments/r-{tr_config.replace('/', '-')}.json\"\n",
    "        max_seq_length = 10000\n",
    "    \n",
    "\n",
    "    batch_size = 30\n",
    "    inference = UnslothInference(\n",
    "        checkpoint_dir=str(checkpoint_dir),\n",
    "        cache_dir=str(cache_dir),\n",
    "        batch_size=batch_size,\n",
    "        max_seq_length=max_seq_length\n",
    "    )\n",
    "\n",
    "    open(output_file, \"w\").close()  # Clea\n",
    "\n",
    "    inference.run(\n",
    "        dataset=dataset,\n",
    "        common_prompt=common_prompt,\n",
    "        output_file=output_file\n",
    "    )\n",
    "\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"[\\n\")\n",
    "        f.write(\",\\n\".join(lines))\n",
    "        f.write(\"\\n]\\n\")\n",
    "    \n",
    "    print(f\"Saved to {output_file}\")\n",
    "\n",
    "    del inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ResponseGeneration.update_prompt()\n",
    "# # # Ïó¨Îü¨ Î™®Îç∏Ïóê ÎåÄÌï¥ Ï∂îÎ°† Ïã§Ìñâ (Run inference for multiple models)\n",
    "# model_configs = [\n",
    "#     # (55, \"ours\"),\n",
    "#     (55, \"woExp\"),\n",
    "#     # (55, \"WoMetadata\"),\n",
    "#     # (55, \"WoMetadata+Thinking\"),\n",
    "#     # (55, \"WoThinking\"),\n",
    "#     # (55, \"woQM\"),\n",
    "#     # (55, \"woQM+Script\"),\n",
    "#     # (55, \"woScript\")\n",
    "# ]\n",
    "\n",
    "# for checkpoint_num, config_name in model_configs:\n",
    "#     print(f\"Running inference for {config_name} with checkpoint {checkpoint_num}\")\n",
    "#     run_inference(checkpoint_num, config_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any  # Any ÌÉÄÏûÖ import ÌïÑÏöî\n",
    "\n",
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"QueryTruePositive\"\n",
    "    false_positive = \"QueryFalsePositive\"\n",
    "    false_negative = \"QueryFalseNegative\"\n",
    "    \n",
    "def eval_query(cand_response_filename, db_gt_filename=\"./gts.json\"):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "    response_reports = []\n",
    "    time_reports = []\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            # pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            # if \"ÏòÜÎ∞ò ÏäµÎèÑ ÏïåÎ†§Ï§ò\" not in input:\n",
    "            #     continue\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # Í¥ÄÍ≥Ñ ÏóÜÎäî ÏßàÎ¨∏Îì§ÏùÄ Í±¥ÎÑàÎõ∞Ïûê\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                print(f\"No ground truth found for {input}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            gt_report = gt_report[0]\n",
    "            tags = gt_report[\"Tags\"]\n",
    "            # assert gt_report[\"QueryResults\"] != []\n",
    "            # if gt_report[\"Result\"] == []:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            \n",
    "            \n",
    "            gt_results = [d for d in gt_report[\"QueryResults\"]]\n",
    "            gt_query_results = defaultdict(list)\n",
    "            for gt_result in gt_results:\n",
    "                for col in gt_result[\"result_columns\"]:\n",
    "                    gt_query_results[col].extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            gt_response = gt_report[\"Response\"]\n",
    "            # gt_required_variables = gt_report[\"RequiredVariables\"]\n",
    "            # gt_variables_to_report = gt_report[\"VariablesToReport\"]\n",
    "            user_input = gt_report[\"Input\"]\n",
    "            # print(user_input)\n",
    "            exp_tag = cand_response_filename.split(\"/\")[-1].split(\"_\")[3]\n",
    "\n",
    "            response_report = {\n",
    "                \"Input\": user_input,\n",
    "                \"Metadata\": metadata,\n",
    "                \"GT_Response\": gt_response,\n",
    "                # \"GT_RequiredVariables\": gt_required_variables,\n",
    "                # \"GT_VariablesToReport\": gt_variables_to_report,\n",
    "            }\n",
    "            # evaluation_report ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ± (defaultdict ÏÇ¨Ïö©, Í∏∞Î≥∏Í∞í None)\n",
    "\n",
    "            evaluation_report: dict[str, Any] = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Metadata\"] = metadata\n",
    "            evaluation_report[\"Tags\"] = tags\n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict):\n",
    "                requirements = [\"Thinking\", \"Expectations\", \"Mapping\"]\n",
    "                if exp_tag in [\"WoThinking\", \"WoMetadata+Thinking\"]:\n",
    "                    requirements.remove(\"Thinking\")\n",
    "                elif exp_tag in [\"woExp\"]:\n",
    "                    requirements.remove(\"Expectations\")\n",
    "                elif exp_tag in [\"woQM\", \"woQM+Script\"]:\n",
    "                    requirements.remove(\"Mapping\")\n",
    "                for requirement in requirements:\n",
    "                    if requirement not in cand_response[\"Candidate\"]:\n",
    "                        evaluation_report[EM.json_structure] = False\n",
    "                        break\n",
    "                else:\n",
    "                    evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "            \n",
    "            if not evaluation_report[EM.json_structure]:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "\n",
    "                print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "            \n",
    "            start_time = time.time()\n",
    "\n",
    "            if exp_tag in [\"woExp\"]:\n",
    "                cand_response[\"Candidate\"][\"Expectations\"] = []\n",
    "            if exp_tag in [\"woQM\", \"woQM+Script\"]:\n",
    "                pass\n",
    "            # exp_tag = \\\n",
    "            #     \"woCoTExp\" if \"woCoTExp\" in str(cand_response_filename) else \\\n",
    "            #     \"woOp\" if \"woOp\" in str(cand_response_filename) else \\\n",
    "            #     \"woQM\" if \"woQM\" in str(cand_response_filename) else \\\n",
    "            #     None\n",
    "            try:\n",
    "                mapping, expectations, required_variables, script = InputToInstruction.postprocess_v2(\n",
    "                    deepcopy(cand_response[\"Candidate\"]), \n",
    "                    exp_tag=exp_tag\n",
    "                )\n",
    "            except:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            \n",
    "            response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, mapping, expectations, required_variables, script, exp_tag=exp_tag)\n",
    "            # print(response)\n",
    "            response_report[\"PD_Response\"] = response\n",
    "            # try:\n",
    "            #     # response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, instructions, exp_tag=exp_tag)\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Error: {e}\")\n",
    "            #     # evaluation_report[EM.true_positive] = 0\n",
    "            #     # evaluation_report[EM.false_positive] = 0\n",
    "            #     # evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "            #     # evaluation_reports.append(evaluation_report)\n",
    "\n",
    "            #     # response_reports.append(response_report)\n",
    "                            \n",
    "            #     # pbar.update(1)\n",
    "            #     # continue\n",
    "            time_reports.append(time.time() - start_time)\n",
    "            response_reports.append(response_report)\n",
    "            \n",
    "            # required_variables = summarize_variables_to_report(required_variables)\n",
    "            # print(required_variables)\n",
    "            # required_variables = ResponseGeneration.stringify_variables(required_variables)\n",
    "            \n",
    "            # response_report[\"PD_RequiredVariables\"] = required_variables\n",
    "            # response_report[\"PD_VariablesToReport\"] = variables_to_report\n",
    "\n",
    "            if len(_cand_query_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            cand_query_results = defaultdict(list)\n",
    "            for cand_query_result in _cand_query_results:\n",
    "                for col in cand_query_result[\"result_columns\"]:\n",
    "                    cand_query_results[col].extend(cand_query_result[\"result_indices\"])\n",
    "\n",
    "            cand_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            if len(gt_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = cand_total_combinations\n",
    "                evaluation_report[EM.false_negative] = 0\n",
    "\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "\n",
    "                continue\n",
    "            \n",
    "            # print(gt_total_combinations, cand_total_combinations)\n",
    "            # True Positive: Í≥µÌÜµÎêú Ïª¨ÎüºÍ≥º Î°úÏö∞Ïùò Î™®Îì† Ï°∞Ìï©\n",
    "            true_positive = 0\n",
    "            false_negative = 0\n",
    "            false_positive = 0\n",
    "            for col in set(gt_query_results.keys())&set(cand_query_results.keys()):\n",
    "                s_gt_query_result = set(gt_query_results[col])\n",
    "                s_cand_query_result = set(cand_query_results[col])\n",
    "                true_positive += len(s_gt_query_result & s_cand_query_result)\n",
    "                false_negative += len(s_gt_query_result - s_cand_query_result)\n",
    "                false_positive += len(s_cand_query_result - s_gt_query_result)\n",
    "\n",
    "                # print(true_positive, false_negative, false_positive, len(s_gt_query_result), len(s_cand_query_result))\n",
    "            # assert true_positive + false_positive + false_negative == gt_total_combinations\n",
    "            \n",
    "\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    with open(f\"{cand_response_filename.replace('.json', '_response.json')}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(response_reports, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Time: {time_reports}, {sum(time_reports) / len(time_reports)}\")\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_print = eval_df.drop(columns=[\"Metadata\", \"Tags\"])\n",
    "    print(eval_print)\n",
    "    eval_df[EM.true_positive] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[EM.false_positive] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[EM.false_negative] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # replace nan with 0\n",
    "    # eval_df.fillna(0, inplace=True)\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    print(truepos_sum, falsepos_sum, falseneg_sum)\n",
    "    print(precision, recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tag(cand_response_filename, db_gt_filename=\"./gts.json\"):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "    response_reports = []\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            # pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            # if \"ÏòÜÎ∞ò ÏäµÎèÑ ÏïåÎ†§Ï§ò\" not in input:\n",
    "            #     continue\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # Í¥ÄÍ≥Ñ ÏóÜÎäî ÏßàÎ¨∏Îì§ÏùÄ Í±¥ÎÑàÎõ∞Ïûê\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                print(f\"No ground truth found for {input}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            gt_report = gt_report[0]\n",
    "            tags = gt_report[\"Tags\"]\n",
    "            # assert gt_report[\"QueryResults\"] != []\n",
    "            # if gt_report[\"Result\"] == []:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            \n",
    "            \n",
    "            gt_results = [d for d in gt_report[\"QueryResults\"]]\n",
    "            gt_query_results = defaultdict(list)\n",
    "            for gt_result in gt_results:\n",
    "                for col in gt_result[\"result_columns\"]:\n",
    "                    gt_query_results[col].extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            gt_response = gt_report[\"Response\"]\n",
    "            # gt_required_variables = gt_report[\"RequiredVariables\"]\n",
    "            # gt_variables_to_report = gt_report[\"VariablesToReport\"]\n",
    "            user_input = gt_report[\"Input\"]\n",
    "            # print(user_input)\n",
    "            exp_tag = cand_response_filename.split(\"_\")[3]\n",
    "\n",
    "            response_report = {\n",
    "                \"Input\": user_input,\n",
    "                \"Metadata\": metadata,\n",
    "                \"GT_Response\": gt_response,\n",
    "                # \"GT_RequiredVariables\": gt_required_variables,\n",
    "                # \"GT_VariablesToReport\": gt_variables_to_report,\n",
    "            }\n",
    "            # evaluation_report ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ± (defaultdict ÏÇ¨Ïö©, Í∏∞Î≥∏Í∞í None)\n",
    "\n",
    "            evaluation_report: dict[str, Any] = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Metadata\"] = metadata\n",
    "            evaluation_report[\"Tags\"] = tags\n",
    "\n",
    "            print(cand_response)\n",
    "            evaluation_report[EM.json_structure] = True\n",
    "\n",
    "            sql = cand_response[\"Candidate\"]\n",
    "            if sql == \"\":\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "\n",
    "            # \"SELECT\"ÎùºÎäî Ï≤´ Î≤àÏß∏ Îì±Ïû•Îßå \"SELECT id \"Î°ú ÎåÄÏ≤¥Ìï©ÎãàÎã§.\n",
    "            # sql = sql.replace(\"SELECT\", \"SELECT d.id, \", 1)\n",
    "            try:\n",
    "                df = DBManager.execute_structured_query_string(sql)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                response_report[\"PD_Response\"] = \"ÏøºÎ¶¨Ï§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§\"\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "            # cols = list(df.columns)\n",
    "            # cols.remove(\"id\")\n",
    "            # cols.remove(\"idu_name\")\n",
    "            # cols.remove(\"timestamp\")\n",
    "            # rows = list(df[\"id\"])\n",
    "            # query_results = [{\n",
    "            #     \"result_columns\": cols,\n",
    "            #     \"result_indices\": rows,\n",
    "            # }]\n",
    "            # df = df.drop(columns=['id'])\n",
    "\n",
    "            response = ResponseGeneration.execute_raw(\n",
    "                f\"\"\"\n",
    "                Input: {user_input}\n",
    "                Metadata: {metadata}\n",
    "                Data: {df.to_json(orient=\"records\")}\n",
    "                \"\"\",\n",
    "                prompt = \"\"\"\n",
    "                ÏßàÎ¨∏ÏùÑ Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú ÎãµÎ≥ÄÌï¥Ï§ò.\n",
    "                \"\"\"\n",
    "            )\n",
    "            response = extract_content(response)\n",
    "            response_report[\"PD_Response\"] = response\n",
    "            # try:\n",
    "            #     # response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, instructions, exp_tag=exp_tag)\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Error: {e}\")\n",
    "            #     # evaluation_report[EM.true_positive] = 0\n",
    "            #     # evaluation_report[EM.false_positive] = 0\n",
    "            #     # evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "            #     # evaluation_reports.append(evaluation_report)\n",
    "\n",
    "            #     # response_reports.append(response_report)\n",
    "                            \n",
    "            #     # pbar.update(1)\n",
    "            #     # continue\n",
    "            \n",
    "            response_reports.append(response_report)\n",
    "            \n",
    "            # cand_query_results = defaultdict(list)\n",
    "            # for cand_query_result in query_results:\n",
    "            #     for col in cand_query_result[\"result_columns\"]:\n",
    "            #         cand_query_results[col].extend(cand_query_result[\"result_indices\"])\n",
    "\n",
    "            # cand_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            # if len(gt_results) == 0:\n",
    "            #     evaluation_report[EM.true_positive] = 0\n",
    "            #     evaluation_report[EM.false_positive] = cand_total_combinations\n",
    "            #     evaluation_report[EM.false_negative] = 0\n",
    "\n",
    "            #     evaluation_reports.append(evaluation_report)\n",
    "            #     pbar.update(1)\n",
    "\n",
    "            #     continue\n",
    "            \n",
    "            # # print(gt_total_combinations, cand_total_combinations)\n",
    "            # # True Positive: Í≥µÌÜµÎêú Ïª¨ÎüºÍ≥º Î°úÏö∞Ïùò Î™®Îì† Ï°∞Ìï©\n",
    "            # true_positive = 0\n",
    "            # false_negative = 0\n",
    "            # false_positive = 0\n",
    "            # for col in set(gt_query_results.keys())&set(cand_query_results.keys()):\n",
    "            #     s_gt_query_result = set(gt_query_results[col])\n",
    "            #     s_cand_query_result = set(cand_query_results[col])\n",
    "            #     true_positive += len(s_gt_query_result & s_cand_query_result)\n",
    "            #     false_negative += len(s_gt_query_result - s_cand_query_result)\n",
    "            #     false_positive += len(s_cand_query_result - s_gt_query_result)\n",
    "\n",
    "            #     # print(true_positive, false_negative, false_positive, len(s_gt_query_result), len(s_cand_query_result))\n",
    "            # # assert true_positive + false_positive + false_negative == gt_total_combinations\n",
    "            \n",
    "\n",
    "            # evaluation_report[EM.true_positive] = true_positive\n",
    "            # evaluation_report[EM.false_positive] = false_positive\n",
    "            # evaluation_report[EM.false_negative] = false_negative\n",
    "\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    with open(f\"{cand_response_filename.replace('.json', '_response.json')}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(response_reports, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # # print(eval_df)\n",
    "\n",
    "    # eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    # final_result = {}\n",
    "\n",
    "    # for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "    #     # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "    #     final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # # normalize per query\n",
    "    # eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    # eval_print = eval_df.drop(columns=[\"Metadata\", \"Tags\"])\n",
    "    # print(eval_print)\n",
    "    # eval_df[EM.true_positive] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    # eval_df[EM.false_positive] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    # eval_df[EM.false_negative] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # # replace nan with 0\n",
    "    # # eval_df.fillna(0, inplace=True)\n",
    "\n",
    "    # # # F1 score except nans.\n",
    "    # truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    # precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    # recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    # print(truepos_sum, falsepos_sum, falseneg_sum)\n",
    "    # print(precision, recall)\n",
    "    # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # # print(f\"F1: {f1}\")\n",
    "    # final_result[\"F1\"] = f1\n",
    "    # final_result[\"Recall\"] = recall\n",
    "\n",
    "    # for col in final_result:\n",
    "    #     print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    # return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_tag(\n",
    "#     cand_response_filename=\"../experiments/r-v7_r211_a422_TAG_tr27_0629-step-0.json\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse input:  Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïò®ÎèÑ ÏïåÎ†§Ï§ò {\"Thinking\": \"ÏÇ¨Ïö©ÏûêÎäî Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏïåÍ≥†Ïã∂Ïñ¥Ìï®. Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏøºÎ¶¨Ìïú ÌõÑ Í∞Å Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ Î∞è Í∑∏ Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôòÌïòÎ©¥ Îê®.\", \"Expectations\": [\"Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî {{Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†}}‚ÑÉÏù¥Í≥†, ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî {{Ïù¥Î≤àÏ£º_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†}}‚ÑÉÏûÖÎãàÎã§. Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ({{Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†}}‚ÑÉ)Îäî ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ({{Ïù¥Î≤àÏ£º_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†}}‚ÑÉ)Î≥¥Îã§ {{Ïù¥Î≤àÏ£º_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†Ïùò_Ï∞®Ïù¥}}‚ÑÉ ÎÜíÏäµÎãàÎã§.\"}, \"Mapping\": {\"temporal\": {\"Ïù¥Î≤àÏ£º\": \"[DATE_TRUNC('week', DATE 'CURRENT_DATE')), DATE_TRUNC('week', DATE 'CURRENT_DATE' + INTERVAL '1 week'))\"}, \"spatials\": {\"Ïö∞Î¶¨Î∞ò\": \"02_I81\", \"ÏïûÎ∞ò\": \"01_IB7\"}, \"modalities\": {\"Ïã§ÎÇ¥Ïò®ÎèÑ\": \"roomtemp\"}}, \"Script\": [\"v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='Ïù¥Î≤àÏ£º',s='Ïö∞Î¶¨Î∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_Ïù¥Î≤àÏ£º_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='Ïù¥Î≤àÏ£º',s='ÏïûÎ∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑† = v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].mean()\", \"v_Ïù¥Î≤àÏ£º_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑† = v_Ïù¥Î≤àÏ£º_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].mean()\", \"v_Ïù¥Î≤àÏ£º_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†Ïùò_Ï∞®Ïù¥ = v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑† - v_Ïù¥Î≤àÏ£º_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†\"]}\n",
      "Failed to parse input:  ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑÎûë Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò. {\"Thinking\": \"ÏÇ¨Ïö©ÏûêÎäî ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑÏùò Ï∞®Ïù¥Î•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏøºÎ¶¨Ìïú ÌõÑ Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôòÌïòÎ©¥ Îê®.\", \"Expectations\": [\"ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑ({{ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_ÏÑ§Ï†ïÏò®ÎèÑ}}‚ÑÉ)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ({{ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ}}‚ÑÉ)Ïùò Ï∞®Ïù¥Îäî {{ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_ÏÑ§Ï†ïÏò®ÎèÑ-ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ}}‚ÑÉÏûÖÎãàÎã§.\"], \"Mapping\": {\"temporal\": {\"ÌòÑÏû¨\": \"LAST_RECORD\"}, \"spatials\": {\"Ïö∞Î¶¨Î∞ò\": \"02_I81\"}, \"modalities\": {\"ÏÑ§Ï†ïÏò®ÎèÑ\": \"settemp\", \"Ïã§ÎÇ¥Ïò®ÎèÑ\": \"roomtemp\"}}, \"Script\": [\"v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_df = data(t='ÌòÑÏû¨',s='Ïö∞Î¶¨Î∞ò',m=['ÏÑ§Ï†ïÏò®ÎèÑ','Ïã§ÎÇ¥Ïò®ÎèÑ'])\", \"v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò = {'ÏÑ§Ï†ïÏò®ÎèÑ': v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_df['ÏÑ§Ï†ïÏò®ÎèÑ'].values[0], 'Ïã§ÎÇ¥Ïò®ÎèÑ': v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].values[0]}\", \"v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_Ï∞®Ïù¥ = v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò['ÏÑ§Ï†ïÏò®ÎèÑ'] - v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò['Ïã§ÎÇ¥Ïò®ÎèÑ']\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 3/12 [00:00<00:00, 11.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in executing script: v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_ÏÑ§Ï†ïÏò®ÎèÑ = v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_df['ÏÑ§Ï†ïÏò®ÎèÑ'].values[0]\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "Error in running query_v2: index 0 is out of bounds for axis 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:13<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: [0.26526427268981934, 4.9041900634765625, 0.00010156631469726562, 1.9550323486328125e-05, 1.0520789623260498, 0.6002411842346191, 5.003904104232788, 0.8187286853790283, 0.5806748867034912], 1.4694670306311712\n",
      "                               Input  JsonStructureCorrectness  \\\n",
      "0             Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïò®ÎèÑ ÏïåÎ†§Ï§ò                     False   \n",
      "1              ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑÎûë Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò.                     False   \n",
      "2   ÏßÄÎÇúÎã¨Ïóê ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÎßéÏù¥ ÎÇ¨Îçò ÎÇ†ÏùÄ?                      True   \n",
      "3        Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò                      True   \n",
      "4                   2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ† ÏïåÎ†§Ï§ò                      True   \n",
      "5                      ÌôîÏÑ±Ïùò ÏÑ§Ï†ïÏò®ÎèÑ ÌôïÏù∏Ìï¥Ï§ò                      True   \n",
      "6                          ÏòÜÎ∞ò ÏäµÎèÑ ÏïåÎ†§Ï§ò                      True   \n",
      "7       ÏßÄÎÇú 3Ïùº ÎèôÏïà Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥ Ïò®ÎèÑ ÌèâÍ∑† Í∞í ÏïåÎ†§Ï§ò.                      True   \n",
      "8           Ïò§Îäò Ïò§ÌõÑ 5ÏãúÏóê ÏòÜÎ∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÎäî Ïñ¥Îï†Ïñ¥?                      True   \n",
      "9        Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÍ≥º ÏµúÏÜåÍ∞í ÏïåÎ†§Ï§ò                      True   \n",
      "10               Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞ò Ï§ë Í∞ÄÏû• ÎçîÏö¥ Î∞©ÏùÄ?                      True   \n",
      "11                 ÏßÄÍ∏à 4Ï∏µ ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ ÏïåÎ†§Ï§ò                      True   \n",
      "\n",
      "    QueryTruePositive  QueryFalsePositive  QueryFalseNegative  ExactMatch  \\\n",
      "0                   0                   0               14214           0   \n",
      "1                   0                   0                   2           0   \n",
      "2                   0                   0               88160           0   \n",
      "3               14201                   0                   0           1   \n",
      "4                   0                   0                9935           0   \n",
      "5                   0                   0                   0           1   \n",
      "6                   0                   0                   0           1   \n",
      "7                4268                   0                   0           1   \n",
      "8                   1                   0                   0           1   \n",
      "9               44080              131041                   0           0   \n",
      "10                  2                   0                   0           1   \n",
      "11                  3                   0                   0           1   \n",
      "\n",
      "     Total  \n",
      "0    14214  \n",
      "1        2  \n",
      "2    88160  \n",
      "3    14201  \n",
      "4     9935  \n",
      "5        0  \n",
      "6        0  \n",
      "7     4268  \n",
      "8        1  \n",
      "9   175121  \n",
      "10       2  \n",
      "11       3  \n",
      "5.251711673642795 0.7482883263572044 4.0\n",
      "0.8752852789404658 0.5676475725680469\n",
      "JsonStructureCorrectness: 0.83\n",
      "ExactMatch: 0.58\n",
      "F1: 0.69\n",
      "Recall: 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 21068.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse input:  Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïò®ÎèÑ ÏïåÎ†§Ï§ò {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏïåÍ≥†Ïã∂Ïñ¥Ìï®. Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏøºÎ¶¨Ìïú ÌõÑ ÌèâÍ∑†Í∞íÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'Ïù¥Î≤àÏ£º': \"[DATE_TRUNC('week', DATE 'CURRENT_DATE'), DATE_TRUNC('week', DATE 'CURRENT_DATE' + INTERVAL '1 week'))\"}, 'spatials': {'Ïö∞Î¶¨Î∞ò': '02_I81', 'ÏïûÎ∞ò': '01_IB7'}, 'modalities': {'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞òÍ≥ºÏïûÎ∞òÏùòÏã§ÎÇ¥Ïò®ÎèÑ_df = data(t='Ïù¥Î≤àÏ£º',s=['Ïö∞Î¶¨Î∞ò','ÏïûÎ∞ò'],m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞òÍ≥ºÏïûÎ∞òÏùòÏã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑† = v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞òÍ≥ºÏïûÎ∞òÏùòÏã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].mean()\"]}\n",
      "Failed to parse input:  ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑÎûë Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò. {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Î•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑÍ∞íÏùÑ ÏøºÎ¶¨Ìïú ÌõÑ Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'ÌòÑÏû¨': 'LAST_RECORD'}, 'spatials': {'Ïö∞Î¶¨Î∞ò': '02_I81'}, 'modalities': {'ÏÑ§Ï†ïÏò®ÎèÑ': 'settemp', 'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_df = data(t='ÌòÑÏû¨',s='Ïö∞Î¶¨Î∞ò',m=['ÏÑ§Ï†ïÏò®ÎèÑ','Ïã§ÎÇ¥Ïò®ÎèÑ'])\", \"v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_df['Ïò®ÎèÑÏ∞®Ïù¥'] = v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_df['ÏÑ§Ï†ïÏò®ÎèÑ'] - v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_df['Ïã§ÎÇ¥Ïò®ÎèÑ']\"]}\n",
      "Failed to parse input:  ÏßÄÎÇúÎã¨Ïóê ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÎßéÏù¥ ÎÇ¨Îçò ÎÇ†ÏùÄ? {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî ÏßÄÎÇúÎã¨ Ïö∞Î¶¨Î∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• Ïª∏Îçò ÎÇ†ÏßúÎ•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. Ìï¥Îãπ Í∏∞Í∞ÑÏùò ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏøºÎ¶¨Ìïú ÌõÑ Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞ÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'ÏßÄÎÇúÎã¨': \"[DATE_TRUNC('month', DATE 'CURRENT_DATE') - INTERVAL '1 month', DATE_TRUNC('month', DATE 'CURRENT_DATE'))\"}, 'spatials': {'Ïö∞Î¶¨Î∞ò': '02_I81'}, 'modalities': {'ÏÑ§Ï†ïÏò®ÎèÑ': 'settemp', 'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_df = data(t='ÏßÄÎÇúÎã¨',s='Ïö∞Î¶¨Î∞ò',m=['ÏÑ§Ï†ïÏò®ÎèÑ','Ïã§ÎÇ¥Ïò®ÎèÑ'])\", \"v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_df['Ïò®ÎèÑÏ∞®Ïù¥'] = v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_df['ÏÑ§Ï†ïÏò®ÎèÑ'] - v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_df['Ïã§ÎÇ¥Ïò®ÎèÑ']\", \"v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥† = v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_df['Ïò®ÎèÑÏ∞®Ïù¥'].max()\", \"v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_df = v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_df[v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_df['Ïò®ÎèÑÏ∞®Ïù¥'] == v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†]\", \"v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_ÎÇ†Ïßú = get_time(v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_df, fmt='date')\", \"v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_ÏÑ§Ï†ïÏò®ÎèÑ = v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_df['ÏÑ§Ï†ïÏò®ÎèÑ'].values[0]\", \"v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_Ïã§ÎÇ¥Ïò®ÎèÑ = v_ÏßÄÎÇúÎã¨_Ïö∞Î¶¨Î∞ò_Ïò®ÎèÑÏ∞®Ïù¥_ÏµúÍ≥†_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].values[0]\"]}\n",
      "Failed to parse input:  Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏòÜÎ∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏøºÎ¶¨Ìïú ÌõÑ Í∞Å Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ Î∞è Í∑∏ Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'Ïù¥Î≤àÏ£º': \"[DATE_TRUNC('week', DATE 'CURRENT_DATE'), DATE_TRUNC('week', DATE 'CURRENT_DATE' + INTERVAL '1 week'))\"}, 'spatials': {'Ïö∞Î¶¨Î∞ò': '02_I81', 'ÏòÜÎ∞ò': '01_IB5'}, 'modalities': {'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='Ïù¥Î≤àÏ£º',s='Ïö∞Î¶¨Î∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_Ïù¥Î≤àÏ£º_ÏòÜÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='Ïù¥Î≤àÏ£º',s='ÏòÜÎ∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_Ïù¥Î≤àÏ£º_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑† = {'Ïö∞Î¶¨Î∞ò': v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].mean(), 'ÏòÜÎ∞ò': v_Ïù¥Î≤àÏ£º_ÏòÜÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].mean()}\", \"v_Ïù¥Î≤àÏ£º_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†Ïùò_Ï∞®Ïù¥ = v_Ïù¥Î≤àÏ£º_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†['Ïö∞Î¶¨Î∞ò'] - v_Ïù¥Î≤àÏ£º_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†['ÏòÜÎ∞ò']\"]}\n",
      "Failed to parse input:  2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ† ÏïåÎ†§Ï§ò {'Thinking': '2Ï£ºÏ†ÑÏùÄ Ï†ïÌôïÌûà ÎßêÌïòÎ©¥ 2Ï£ºÏ†Ñ ÌïúÏ£º (Ïõî~Ïùº)ÎèôÏïàÏûÑ. ÏÇ¨Ïö©ÏûêÎäî 2Ï£ºÏ†Ñ ÏõîÏöîÏùºÎ∂ÄÌÑ∞ ÏùºÏöîÏùºÍπåÏßÄ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ†ÏßúÎ•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. 2Ï£ºÏ†Ñ ÏõîÏöîÏùºÎ∂ÄÌÑ∞ ÏùºÏöîÏùºÍπåÏßÄÏùò Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏøºÎ¶¨Ìïú ÌõÑ Í∞ÄÏû• ÎÜíÏùÄ Ïã§ÎÇ¥Ïò®ÎèÑÎ•º Í∞ÄÏßÑ ÎÇ†ÏùÑ Ï∞æÏïÑ Í∑∏ ÎÇ†ÏßúÏôÄ Ïò®ÎèÑÎ•º Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'2Ï£ºÏ†Ñ': \"[DATE_TRUNC('week', DATE 'CURRENT_DATE' - INTERVAL '2 week'), DATE_TRUNC('week', DATE 'CURRENT_DATE' - INTERVAL '1 week'))\"}, 'spatials': {'Ïö∞Î¶¨Î∞ò': '02_I81'}, 'modalities': {'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='2Ï£ºÏ†Ñ',s='Ïö∞Î¶¨Î∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†_Ïò®ÎèÑ = v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].max()\", \"v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†_df = v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df[v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'] == v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†_Ïò®ÎèÑ]\", \"v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†_ÎÇ†Ïßú = get_time(v_2Ï£ºÏ†Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†_df, fmt='date')\"]}\n",
      "Failed to parse input:  ÌôîÏÑ±Ïùò ÏÑ§Ï†ïÏò®ÎèÑ ÌôïÏù∏Ìï¥Ï§ò {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî ÌôîÏÑ±Ïùò ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑÎ•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. ÌòÑÏû¨ ÌôîÏÑ±Ïùò ÏÑ§Ï†ïÏò®ÎèÑÎ•º ÏøºÎ¶¨Ìïú ÌõÑ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'ÌòÑÏû¨': 'LAST_RECORD'}, 'spatials': {'ÌôîÏÑ±': 'Unknown'}, 'modalities': {'ÏÑ§Ï†ïÏò®ÎèÑ': 'settemp'}}}\n",
      "Failed to parse input:  ÏòÜÎ∞ò ÏäµÎèÑ ÏïåÎ†§Ï§ò {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî ÏòÜÎ∞òÏùò ÌòÑÏû¨ ÏäµÎèÑÎ•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. ÌòÑÏû¨ ÏòÜÎ∞òÏùò ÏäµÎèÑÎ•º ÏøºÎ¶¨Ìïú ÌõÑ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'ÌòÑÏû¨': 'LAST_RECORD'}, 'spatials': {'ÏòÜÎ∞ò': '01_IB5'}, 'modalities': {'ÏäµÎèÑ': 'Unknown'}}}\n",
      "Failed to parse input:  ÏßÄÎÇú 3Ïùº ÎèôÏïà Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥ Ïò®ÎèÑ ÌèâÍ∑† Í∞í ÏïåÎ†§Ï§ò. {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî ÏßÄÎÇú 3Ïùº Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÌèâÍ∑†ÏùÑ ÏïåÍ≥†Ïã∂Ïñ¥Ìï®. Ìï¥Îãπ Í∏∞Í∞ÑÏùò Ïã§ÎÇ¥Ïò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏøºÎ¶¨Ìïú ÌõÑ ÌèâÍ∑†Í∞íÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'ÏßÄÎÇú3Ïùº': \"[DATE_TRUNC('day', DATE 'CURRENT_DATE') - INTERVAL '3 day', DATE_TRUNC('day', DATE 'CURRENT_DATE'))\"}, 'spatials': {'Ïö∞Î¶¨Î∞ò': '02_I81'}, 'modalities': {'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_ÏßÄÎÇú3Ïùº_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='ÏßÄÎÇú3Ïùº',s='Ïö∞Î¶¨Î∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_ÏßÄÎÇú3Ïùº_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑† = v_ÏßÄÎÇú3Ïùº_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].mean()\"]}\n",
      "Failed to parse input:  Ïò§Îäò Ïò§ÌõÑ 5ÏãúÏóê ÏòÜÎ∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÎäî Ïñ¥Îï†Ïñ¥? {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî Ïò§Îäò Ïò§ÌõÑ 5ÏãúÏóê ÏòÜÎ∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÎ•º ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. Ìï¥Îãπ ÏãúÍ∞ÑÎåÄÏùò ÏÑ§Ï†ïÏò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏøºÎ¶¨Ìïú ÌõÑ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'Ïò§ÌõÑ5Ïãú': \"DATE_TRUNC('day', DATE 'CURRENT_DATE') + INTERVAL '17 hours'\"}, 'spatials': {'ÏòÜÎ∞ò': '01_IB5'}, 'modalities': {'ÏÑ§Ï†ïÏò®ÎèÑ': 'settemp'}}, 'Script': [\"v_Ïò§ÌõÑ5Ïãú_ÏòÜÎ∞ò_ÏÑ§Ï†ïÏò®ÎèÑ_df = data(t='Ïò§ÌõÑ5Ïãú',s='ÏòÜÎ∞ò',m='ÏÑ§Ï†ïÏò®ÎèÑ')\", \"v_Ïò§ÌõÑ5Ïãú_ÏòÜÎ∞ò_ÏÑ§Ï†ïÏò®ÎèÑ = v_Ïò§ÌõÑ5Ïãú_ÏòÜÎ∞ò_ÏÑ§Ï†ïÏò®ÎèÑ_df['ÏÑ§Ï†ïÏò®ÎèÑ']\"]}\n",
      "Failed to parse input:  Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÍ≥º ÏµúÏÜåÍ∞í ÏïåÎ†§Ï§ò {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî Ïò¨Ìï¥ 6Ïõî ~ 8Ïõî Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÏôÄ ÏµúÏÜåÍ∞íÏùÑ ÏïåÍ≥† Ïã∂Ïñ¥Ìï®. Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏøºÎ¶¨Ìïú ÌõÑ ÏµúÎåÄÏôÄ ÏµúÏÜå Ïã§ÎÇ¥Ïò®ÎèÑÎ•º Ï∞æÏïÑ Í∑∏ Í∞íÏùÑ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'Ïò¨Ìï¥Ïó¨Î¶Ñ': \"[DATE 'CURRENT_YEAR-06-01', DATE 'CURRENT_YEAR-09-01')\"}, 'spatials': {'Ïö∞Î¶¨Î∞ò': '02_I81'}, 'modalities': {'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_Ïò¨Ìï¥Ïó¨Î¶Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='Ïò¨Ìï¥Ïó¨Î¶Ñ',s='Ïö∞Î¶¨Î∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_Ïò¨Ìï¥Ïó¨Î¶Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÎåÄ = v_Ïò¨Ìï¥Ïó¨Î¶Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].max()\", \"v_Ïò¨Ìï¥Ïó¨Î¶Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÏÜå = v_Ïò¨Ìï¥Ïó¨Î¶Ñ_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].min()\"]}\n",
      "Failed to parse input:  Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞ò Ï§ë Í∞ÄÏû• ÎçîÏö¥ Î∞©ÏùÄ? {\"Thinking\": \"ÏÇ¨Ïö©ÏûêÎäî ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞ò Ï§ë Ïã§ÎÇ¥Ïò®ÎèÑÍ∞Ä Îçî ÎÜíÏùÄ Î∞©ÏùÑ ÏïåÍ≥†Ïã∂Ïñ¥Ìï®. ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏøºÎ¶¨Ìïú ÌõÑ Îçî ÎÜíÏùÄ Ïã§ÎÇ¥Ïò®ÎèÑÎ•º Í∞ÄÏßÑ Î∞©Í≥º Í∑∏ Ïò®ÎèÑÎ•º Î∞òÌôòÌïòÎ©¥ Îê®.\", \"Mapping\": {\"temporal\": {\"ÌòÑÏû¨\": \"LAST_RECORD\"}, \"spatials\": {\"Ïö∞Î¶¨Î∞ò\": \"02_I81\", \"ÏïûÎ∞ò\": \"01_IB7\"}, \"modalities\": {\"Ïã§ÎÇ¥Ïò®ÎèÑ\": \"roomtemp\"}}, \"Script\": [\"v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='ÌòÑÏû¨',s='Ïö∞Î¶¨Î∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_ÌòÑÏû¨_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='ÌòÑÏû¨',s='ÏïûÎ∞ò',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ = {'Ïö∞Î¶¨Î∞ò': v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].values[0], 'ÏïûÎ∞ò': v_ÌòÑÏû¨_ÏïûÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].values[0]}\", \"v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥† = max(v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ.values())\", \"v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†Ïùò_Í≥µÍ∞Ñ, v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†Ïùò_Ïã§ÎÇ¥Ïò®ÎèÑ = ('Ïö∞Î¶¨Î∞ò', v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†) if v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥† == v_ÌòÑÏû¨_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].values[0] else ('ÏïûÎ∞ò', v_ÌòÑÏû¨_Ïã§ÎÇ¥Ïò®ÎèÑ_ÏµúÍ≥†)\")\n",
      "Failed to parse input:  ÏßÄÍ∏à 4Ï∏µ ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ ÏïåÎ†§Ï§ò {'Thinking': 'ÏÇ¨Ïö©ÏûêÎäî ÌòÑÏû¨ 4Ï∏µÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎ•º ÏïåÍ≥†Ïã∂Ïñ¥Ìï®. ÌòÑÏû¨ 4Ï∏µÏóê ÏÜçÌïú Î™®Îì† Í≥µÍ∞ÑÏùò Ïã§ÎÇ¥Ïò®ÎèÑÏùò ÌèâÍ∑†Í∞íÏùÑ Î∞òÌôòÌïòÎ©¥ Îê®.', 'Mapping': {'temporal': {'ÌòÑÏû¨': 'LAST_RECORD'}, 'spatials': {'4Ï∏µ': ['01_IB5', '01_IB7', '02_I81']}, 'modalities': {'Ïã§ÎÇ¥Ïò®ÎèÑ': 'roomtemp'}}, 'Script': [\"v_ÌòÑÏû¨_4Ï∏µ_Ïã§ÎÇ¥Ïò®ÎèÑ_df = data(t='ÌòÑÏû¨',s='4Ï∏µ',m='Ïã§ÎÇ¥Ïò®ÎèÑ')\", \"v_ÌòÑÏû¨_4Ï∏µ_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑† = v_ÌòÑÏû¨_4Ï∏µ_Ïã§ÎÇ¥Ïò®ÎèÑ_df['Ïã§ÎÇ¥Ïò®ÎèÑ'].mean()\"]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m\n\u001b[1;32m     23\u001b[0m names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# \"../r-v7_r211_a422_TAG_tr27_0629-step-0.json\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr-v7_r211_a422_ours_tr27_0623-step-54\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr-v7_r211_a422_woScript_tr27_0623-step-55\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m ]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43meval_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../experiments/result_23/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 213\u001b[0m, in \u001b[0;36meval_query\u001b[0;34m(cand_response_filename, db_gt_filename)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcand_response_filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_response.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    211\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(response_reports, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_reports\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime_reports\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime_reports\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(evaluation_reports)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# print(eval_df)\u001b[39;00m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# name = \"r-v7_r256_a512_ours_tr6_0503-checkpoint-63\"\n",
    "# name = \"r-v7_r256_a512_ours_tr18_0503-checkpoint-52\"\n",
    "# name = \"r-v7_r256_a512_ours_tr30_0503-checkpoint-54\"\n",
    "# name = \"r-v7_r256_a512_ours_tr45_0503-checkpoint-95\"\n",
    "# name = \"r-v7_r256_a512_ours_tr60_0503-checkpoint-108\"\n",
    "\n",
    "# name = \"r-v7_r256_a512_woall_tr6_0503-checkpoint-28\"\n",
    "# name = \"r-v7_r256_a512_woall_tr18_0503-checkpoint-70\"\n",
    "# name = \"r-v7_r256_a512_woall_tr30_0503-checkpoint-57\"\n",
    "# name = \"r-v7_r256_a512_woall_tr45_0503-checkpoint-95\"\n",
    "# name = \"r-v7_r256_a512_woall_tr60_0503-checkpoint-90\"\n",
    "\n",
    "# names = [\n",
    "# \"r-v7_r211_a422_ours_tr27_0629-step-53\",\n",
    "# \"r-v7_r211_a422_woExp_tr27_0623-step-55\",\n",
    "# \"r-v7_r211_a422_WoMetadata+Thinking_tr27_0629-step-55\",\n",
    "# \"r-v7_r211_a422_WoThinking_tr27_0629-step-54\",\n",
    "# \"r-v7_r211_a422_woQM_tr27_0629-step-46\",\n",
    "# \"r-v7_r211_a422_woQM+Script_tr27_0629-step-55\",\n",
    "# \"r-v7_r211_a422_woScript_tr27_0629-step-55\"\n",
    "# ]\n",
    "\n",
    "names = [\n",
    "# \"../r-v7_r211_a422_TAG_tr27_0629-step-0.json\"\n",
    "\"r-v7_r211_a422_ours_tr27_0623-step-54\",\n",
    "\"r-v7_r211_a422_woExp_tr27_0623-step-55\",\n",
    "\"r-v7_r211_a422_WoMetadata_tr27_0623-step-60\",\n",
    "\"r-v7_r211_a422_WoMetadata+Thinking_tr27_0623-step-55\",\n",
    "\"r-v7_r211_a422_WoThinking_tr27_0623-step-51\",\n",
    "\"r-v7_r211_a422_woQM_tr27_0623-step-53\",\n",
    "\"r-v7_r211_a422_woQM+Script_tr27_0623-step-60\",\n",
    "\"r-v7_r211_a422_woScript_tr27_0623-step-55\"\n",
    "]\n",
    "\n",
    "for name in names:\n",
    "    exp_tag = name.split(\"\n",
    "    eval_query(\n",
    "        f\"../experiments/result_23/{name}.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïò®ÎèÑ ÏïåÎ†§Ï§ò': {\"('WoThinking', 'ours')\": 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.98¬∞CÏù¥Í≥†, ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.11¬∞CÏûÖÎãàÎã§.', 'woExp': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïò®ÎèÑÎäî 25.55¬∞CÏûÖÎãàÎã§.', 'WoMetadata': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.67¬∞C ÏûÖÎãàÎã§.', 'WoMetadata+Thinking': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïò®ÎèÑÎäî 25.39¬∞C ÏûÖÎãàÎã§.', 'woQM': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.55¬∞C ÏûÖÎãàÎã§.', 'woQM+Script': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 27.00¬∞CÏù¥Í≥†, ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 26.50¬∞CÏûÖÎãàÎã§. Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎ≥¥Îã§ 0.50¬∞C ÎÜíÏäµÎãàÎã§.', 'woScript': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(27.00¬∞C)Îäî ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(26.50¬∞C)Î≥¥Îã§ 0.50¬∞C ÎÜíÏäµÎãàÎã§.', 'GT_Response': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.98¬∞CÏù¥Í≥†, ÏïûÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.11¬∞CÏûÖÎãàÎã§.'}, 'ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑÎûë Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò.': {'ours': 'ÌòÑÏû¨ Ïö∞Î¶¨Î∞ò ÏÑ§Ï†ïÏò®ÎèÑ Îç∞Ïù¥ÌÑ∞ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.', 'woExp': 'ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ(27.00¬∞C)Ïùò Ï∞®Ïù¥Îäî 4.00¬∞CÏûÖÎãàÎã§.', \"('WoMetadata', 'WoMetadata+Thinking')\": 'Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.', 'WoThinking': 'ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)Îäî Ïã§ÎÇ¥Ïò®ÎèÑ(27.00¬∞C)Î≥¥Îã§ -4.00¬∞C ÎÇÆÏäµÎãàÎã§.', 'woQM': 'ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ(28.50¬∞C)Ïùò Ï∞®Ïù¥Îäî -5.50¬∞CÏûÖÎãàÎã§.', 'woQM+Script': 'ÌòÑÏû¨ Ïö∞Î¶¨Î∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)Îäî Ïã§ÎÇ¥Ïò®ÎèÑ(27.00¬∞C)Î≥¥Îã§ -4.00¬∞C ÎÇÆÏäµÎãàÎã§.', 'woScript': 'ÌòÑÏû¨ ÏòÜÎ∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ(28.50¬∞C) Ï∞®Ïù¥Îäî 5.50¬∞CÏûÖÎãàÎã§.', 'GT_Response': 'ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ(27.00¬∞C)Ïùò Ï∞®Ïù¥Îäî 4.00¬∞CÏûÖÎãàÎã§.'}, 'ÏßÄÎÇúÎã¨Ïóê ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÎßéÏù¥ ÎÇ¨Îçò ÎÇ†ÏùÄ?': {\"('WoMetadata', 'WoMetadata+Thinking', 'ours')\": 'ÏßÄÎÇúÎã¨ 8Ïõî 19ÏùºÏóê ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ(22.00¬∞C) Ï∞®Ïù¥Í∞Ä 1.00¬∞CÎ°ú Í∞ÄÏû• Ïª∏ÏäµÎãàÎã§.', 'woExp': 'ÏßÄÎÇúÎã¨Ïóê ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÎßéÏù¥ ÎÇ¨Îçò ÎÇ†ÏùÄ 2022-08-02, 2022-08-04, 2022-08-05, 2022-08-07, 2022-08-08, 2022-08-10, 2022-08-13, 2022-08-16, 2022-08-18, 2022-08-21, 2022-08-24, 2022-08-26, 2022-08-27, 2022-08-29, 2022-08-30Î°ú, Í∞ÅÍ∞Å 0.5¬∞CÏùò Ï∞®Ïù¥Î•º Î≥¥ÏòÄÏäµÎãàÎã§.', 'WoThinking': 'ÏßÄÎÇúÎã¨ 8Ïõî 2Ïùº, 4Ïùº, 5Ïùº, 7Ïùº, 8Ïùº, 10Ïùº, 13Ïùº, 16Ïùº, 18Ïùº, 21Ïùº, 24Ïùº, 26Ïùº, 27Ïùº, 29Ïùº, 30ÏùºÏóê ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ(22.50¬∞C) Ï∞®Ïù¥Í∞Ä 0.50¬∞CÎ°ú Í∞ÄÏû• Ïª∏ÏäµÎãàÎã§.', 'woQM': 'ÏßÄÎÇúÎã¨ ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• Ïª∏Îçò ÎÇ†ÏùÄ 2022-08-19Î°ú 1.00¬∞CÏòÄÏäµÎãàÎã§.', 'woQM+Script': 'ÏßÄÎÇúÎã¨ ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• Ïª∏Îçò ÎÇ† Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§. Î™®Îì† ÎÇ†ÏßúÏùò Ï∞®Ïù¥Í∞Ä ÎèôÏùºÌïòÍ≤å 10.00¬∞CÎ°ú Í∏∞Î°ùÎêòÏñ¥ ÏûàÏäµÎãàÎã§.', 'woScript': 'ÏßÄÎÇúÎã¨ ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÎßéÏù¥ ÎÇ¨Îçò ÎÇ†Ïóê ÎåÄÌïú Ï†ïÎ≥¥Í∞Ä Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Î™ÖÌôïÌûà Íµ¨Î∂ÑÎêòÏßÄ ÏïäÏäµÎãàÎã§. Ï†úÍ≥µÎêú Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏÑ§Ï†ïÏò®ÎèÑÍ∞Ä ÏùºÏ†ïÌïòÍ≤å Ïú†ÏßÄÎêòÍ≥† Ïã§ÎÇ¥Ïò®ÎèÑÎèÑ Î≥ÄÎèôÏù¥ Í±∞Ïùò ÏóÜÏñ¥ Ï∞®Ïù¥Í∞Ä ÌÅ¨Í≤å Î∞úÏÉùÌïú ÌäπÏ†ï ÎÇ†ÏßúÎ•º ÌäπÏ†ïÌï† Ïàò ÏóÜÏäµÎãàÎã§. \\n\\nÏ†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ ÏúÑÌï¥ÏÑúÎäî ÏÑ§Ï†ïÏò®ÎèÑÏùò Î≥ÄÎèôÍ≥º Ïã§ÎÇ¥Ïò®ÎèÑÏùò Î≥ÄÌôîÍ∞Ä Î™ÖÌôïÌûà Í∏∞Î°ùÎêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞Î°úÎäî Îã§ÏùåÍ≥º Í∞ôÏù¥ ÌëúÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§:\\n\\nÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä ÌÅ¨Í≤å Î∞úÏÉùÌïú ÌäπÏ†ï ÎÇ†ÏßúÎ•º ÌôïÏù∏Ìï† Ïàò ÏûàÎäî Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§.', 'GT_Response': 'ÏßÄÎÇúÎã¨ 8Ïõî 2Ïùº, 4Ïùº, 5Ïùº, 7Ïùº, 8Ïùº, 10Ïùº, 13Ïùº, 16Ïùº, 18Ïùº, 21Ïùº, 24Ïùº, 26Ïùº, 27Ïùº, 29Ïùº, 30ÏùºÏóê ÏÑ§Ï†ïÏò®ÎèÑ(23.00¬∞C)ÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ(22.50¬∞C) Ï∞®Ïù¥Í∞Ä 0.50¬∞CÎ°ú Í∞ÄÏû• Ïª∏ÏäµÎãàÎã§.'}, 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò': {'ours': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.98¬∞CÏù¥Í≥†, ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 25.67¬∞CÏûÖÎãàÎã§. Ï∞®Ïù¥Îäî 0.31¬∞CÏûÖÎãàÎã§.', 'woExp': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞ò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.98¬∞C)ÏôÄ ÏòÜÎ∞ò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.67¬∞C)Ïùò Ï∞®Ïù¥Îäî 0.31¬∞CÏûÖÎãàÎã§.', 'WoMetadata': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.67¬∞C)Îäî ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.11¬∞C)Î≥¥Îã§ 0.56¬∞C ÎÜíÏäµÎãàÎã§.', 'WoMetadata+Thinking': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Îäî 25.67¬∞C ÏûÖÎãàÎã§.', 'WoThinking': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞ò(25.98¬∞C)ÏùÄ ÏòÜÎ∞ò(25.67¬∞C)Î≥¥Îã§ 0.31¬∞C ÎÜíÏäµÎãàÎã§.', 'woQM': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.98¬∞C)Îäî ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.67¬∞C)Î≥¥Îã§ 0.31¬∞C ÎÜíÏäµÎãàÎã§.', 'woQM+Script': 'Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.', 'woScript': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ({{v_Ïù¥Î≤àÏ£º_Ïö∞Î¶¨Î∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†}}¬∞C)ÏôÄ ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ({{v_Ïù¥Î≤àÏ£º_ÏòÜÎ∞ò_Ïã§ÎÇ¥Ïò®ÎèÑ_ÌèâÍ∑†}}¬∞C)Ïùò Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞íÏù¥ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÏßÅÏ†ëÏ†ÅÏúºÎ°ú Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Îî∞ÎùºÏÑú Ï†ïÌôïÌïú ÌèâÍ∑† Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞Ìï† Ïàò ÏóÜÏäµÎãàÎã§. \\n\\nÌïòÏßÄÎßå, ÏòàÏãú Ìè¨Îß∑Ïóê ÎßûÏ∂îÏñ¥ ÌëúÌòÑÌïòÏûêÎ©¥:\\n\\nÏù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(nan¬∞C)Îäî ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(nan¬∞C)Î≥¥Îã§ Îç∞Ïù¥ÌÑ∞ Î∂ÄÏ°±ÏúºÎ°ú Ïù∏Ìï¥ Ï∞®Ïù¥Î•º Í≥ÑÏÇ∞Ìï† Ïàò ÏóÜÏäµÎãàÎã§.', 'GT_Response': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞ò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.98¬∞C)ÏôÄ ÏòÜÎ∞ò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ(25.67¬∞C) Ï∞®Ïù¥Îäî 0.31¬∞CÏûÖÎãàÎã§.'}, '2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ† ÏïåÎ†§Ï§ò': {'ours': '2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ†ÏùÄ 2022-09-12, 2022-09-14Î°ú 29.00¬∞CÏòÄÏäµÎãàÎã§.', 'woExp': '2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ†ÏùÄ 2022-09-12ÏôÄ 2022-09-14Î°ú, Í∞ÅÍ∞Å Ïã§ÎÇ¥Ïò®ÎèÑÍ∞Ä 29.00¬∞CÏòÄÏäµÎãàÎã§.', 'WoMetadata': 'Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.', 'WoMetadata+Thinking': \"Ï£ÑÏÜ°Ìï©ÎãàÎã§, ['Unknown']Îäî Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≥µÍ∞ÑÏù¥ÎÇò Î™®Îã¨Î¶¨Ìã∞ ÏûÖÎãàÎã§.\", 'WoThinking': 'Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±ÌïòÏó¨ 2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ†ÏùÑ ÏïåÎ†§ÎìúÎ¶¥ Ïàò ÏóÜÏäµÎãàÎã§.', 'woQM': '2Ï£ºÏ†Ñ Ïö∞Î¶¨Î∞òÏùÄ 2022-09-12, 2022-09-14Ïóê 29.00¬∞CÎ°ú Í∞ÄÏû• ÎçîÏõ†ÏäµÎãàÎã§.', 'woQM+Script': '2Ï£ºÏ†Ñ Ïö∞Î¶¨Î∞òÏùÄ 2022-09-12 ÏõîÏöîÏùºÏóê Ïã§ÎÇ¥Ïò®ÎèÑ 29.00¬∞CÎ°ú Í∞ÄÏû• ÎçîÏõ†ÏäµÎãàÎã§. (Ï£º: Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Í∞ÄÏû• ÎÜíÏùÄ Ïò®ÎèÑÍ∞Ä 2022-09-12Ïóê Í∏∞Î°ùÎêòÏóàÏúºÎ©∞, Ïù¥ÌõÑ ÎÇ†ÏßúÏù∏ 2022-09-18Ïùò Ïò®ÎèÑÎäî 25.00¬∞CÎ°ú Í∞ÄÏû• ÎÜíÏùÄ Ïò®ÎèÑ Í∏∞Î°ùÏù¥ ÏïÑÎãôÎãàÎã§.) \\n\\nÏ†ïÌôïÌïú Ìè¨Îß∑Ïóê ÎßûÏ∂îÏñ¥ Îã®Ïùº ÎãµÎ≥ÄÏúºÎ°ú Ï†úÍ≥µÌïòÏûêÎ©¥:\\n\\n2Ï£ºÏ†Ñ Ïö∞Î¶¨Î∞òÏùÄ ÏõîÏöîÏùºÏù∏ 2022-09-12Ïóê Ïã§ÎÇ¥Ïò®ÎèÑ 29.00¬∞CÎ°ú Í∞ÄÏû• ÎçîÏõ†ÏäµÎãàÎã§.', 'woScript': '2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ†ÏùÄ 2022-09-12Î°ú 29.00¬∞CÏòÄÏäµÎãàÎã§.', 'GT_Response': '2Ï£ºÏ†Ñ 9Ïõî 12ÏùºÍ≥º 9Ïõî 14ÏùºÏóê Ïã§ÎÇ¥Ïò®ÎèÑ(29.00¬∞C)Í∞Ä Í∞ÄÏû• ÎÜíÏïòÏäµÎãàÎã§.'}, 'ÌôîÏÑ±Ïùò ÏÑ§Ï†ïÏò®ÎèÑ ÌôïÏù∏Ìï¥Ï§ò': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woExp', 'woScript')\": \"Ï£ÑÏÜ°Ìï©ÎãàÎã§, ['ÌôîÏÑ±']Îäî Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≥µÍ∞ÑÏù¥ÎÇò Î™®Îã¨Î¶¨Ìã∞ ÏûÖÎãàÎã§.\", 'WoMetadata': 'Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.', 'woQM': 'ÌôîÏÑ±Ïùò ÏÑ§Ï†ïÏò®ÎèÑÎäî 23.00¬∞CÏûÖÎãàÎã§.', 'woQM+Script': 'ÌôîÏÑ±Ïùò ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑÎäî 23.00¬∞CÏûÖÎãàÎã§.', 'GT_Response': \"Ï£ÑÏÜ°Ìï©ÎãàÎã§, ['ÌôîÏÑ±']Îäî Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≥µÍ∞ÑÏù¥ÎÇò Î™®Îã¨Î¶¨Ìã∞ ÏûÖÎãàÎã§.\"}, 'ÏòÜÎ∞ò ÏäµÎèÑ ÏïåÎ†§Ï§ò': {\"('ours', 'woExp', 'woScript')\": \"Ï£ÑÏÜ°Ìï©ÎãàÎã§, ['ÏäµÎèÑ']Îäî Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≥µÍ∞ÑÏù¥ÎÇò Î™®Îã¨Î¶¨Ìã∞ ÏûÖÎãàÎã§.\", \"('WoMetadata', 'WoMetadata+Thinking')\": 'Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.', 'WoThinking': 'Ï£ÑÏÜ°Ìï©ÎãàÎã§, []Îäî Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≥µÍ∞ÑÏù¥ÎÇò Î™®Îã¨Î¶¨Ìã∞ ÏûÖÎãàÎã§.', 'woQM': 'ÏòÜÎ∞òÏùò ÌòÑÏû¨ ÏäµÎèÑÎäî 28.50%ÏûÖÎãàÎã§.', 'woQM+Script': 'ÏòÜÎ∞ò ÏäµÎèÑ Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.', 'GT_Response': \"Ï£ÑÏÜ°Ìï©ÎãàÎã§, ['ÏäµÎèÑ']Îäî Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Í≥µÍ∞ÑÏù¥ÎÇò Î™®Îã¨Î¶¨Ìã∞ ÏûÖÎãàÎã§.\"}, 'ÏßÄÎÇú 3Ïùº ÎèôÏïà Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥ Ïò®ÎèÑ ÌèâÍ∑† Í∞í ÏïåÎ†§Ï§ò.': {'ours': 'ÏßÄÎÇú 3Ïùº Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÌèâÍ∑†ÏùÄ 26.52¬∞C ÏûÖÎãàÎã§.', 'woExp': 'ÏßÄÎÇú 3Ïùº ÎèôÏïà Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥ Ïò®ÎèÑ ÌèâÍ∑† Í∞íÏùÄ 26.52¬∞CÏûÖÎãàÎã§.', \"('WoMetadata', 'WoMetadata+Thinking')\": 'ÏßÄÎÇú 3Ïùº Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÌèâÍ∑†ÏùÄ 25.15¬∞CÏòÄÏäµÎãàÎã§.', 'WoThinking': 'ÏßÄÎÇú 3Ïùº Ïö∞Î¶¨Î∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑÎäî 26.52¬∞C ÏûÖÎãàÎã§.', 'woQM': 'ÏßÄÎÇú 3Ïùº Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÌèâÍ∑†ÏùÄ 24.56¬∞CÏòÄÏäµÎãàÎã§.', \"('woQM+Script', 'woScript')\": 'ÏßÄÎÇú 3Ïùº Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÌèâÍ∑†ÏùÄ 29.00¬∞CÏòÄÏäµÎãàÎã§.', 'GT_Response': 'ÏßÄÎÇú 3ÏùºÍ∞Ñ Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥Ïò®ÎèÑ ÌèâÍ∑†Í∞íÏùÄ 26.52¬∞CÏòÄÏäµÎãàÎã§.'}, 'Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÍ≥º ÏµúÏÜåÍ∞í ÏïåÎ†§Ï§ò': {'ours': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑÎäî 30.50¬∞CÎ°ú Í∞ÄÏû• ÎçîÏõ†Í≥†, 22.50¬∞CÎ°ú Í∞ÄÏû• Ï∂îÏõ†ÏäµÎãàÎã§.', 'woExp': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÏùÄ 30.50¬∞C, ÏµúÏÜåÍ∞íÏùÄ 22.50¬∞CÏûÖÎãàÎã§.', 'WoMetadata': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑÎäî 27.50¬∞CÎ°ú 22.00¬∞CÎ°ú Í∞ÄÏû• Îã¨ÎûêÏäµÎãàÎã§.', 'WoMetadata+Thinking': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑÎäî 27.50¬∞CÎ°ú ÏµúÎåÄÏòÄÏäµÎãàÎã§. 22.00¬∞CÎ°ú ÏµúÏÜåÏòÄÏäµÎãàÎã§.', 'WoThinking': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥Ïò®ÎèÑÎäî 30.50¬∞CÎ°ú Í∞ÄÏû• ÎçîÏõ†Í≥†, 22.50¬∞CÎ°ú Í∞ÄÏû• Ï∂îÏõ†ÏäµÎãàÎã§.', 'woQM': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ(6Ïõî ~ 8Ïõî) Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÏùÄ 30.50¬∞CÏù¥Í≥†, ÏµúÏÜåÍ∞íÏùÄ 22.50¬∞CÏòÄÏäµÎãàÎã§.', 'woQM+Script': 'Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.', 'woScript': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ(2022-06-01 ~ 2022-08-31) Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÏùÄ 28.00¬∞CÏù¥Í≥†, ÏµúÏÜåÍ∞íÏùÄ 23.00¬∞CÏûÖÎãàÎã§.', 'GT_Response': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ(6Ïõî ~ 8Ïõî) Ïö∞Î¶¨Î∞òÏùò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÍ≥º ÏµúÏÜåÍ∞íÏùÄ Í∞ÅÍ∞Å 30.50¬∞CÏôÄ 22.50¬∞CÏûÖÎãàÎã§.'}, 'Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞ò Ï§ë Í∞ÄÏû• ÎçîÏö¥ Î∞©ÏùÄ?': {'ours': 'ÏïûÎ∞ò(27.00¬∞C)Í≥º Ïö∞Î¶¨Î∞ò(27.00¬∞C)Ïùò Ïò®ÎèÑ Ï∞®Ïù¥Îäî 0.00¬∞CÏûÖÎãàÎã§.', \"('woExp', 'woQM+Script')\": 'Ïã§ÌñâÏ§ë ÏóêÎü¨ Î∞úÏÉù', \"('WoMetadata', 'WoThinking')\": 'Ïã§ÌñâÏ§ë ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.', 'WoMetadata+Thinking': 'ÌòÑÏû¨ ÏÑ∏ Î∞© Ï§ë Ïö∞Î¶¨Î∞òÏù¥ Í∞ÄÏû• ÎçîÏö¥ Î∞©Ïù¥Î©∞ Ïã§ÎÇ¥Ïò®ÎèÑÎäî 20.00¬∞CÏûÖÎãàÎã§. (Ï∞∏Í≥†Î°ú ÏïûÎ∞òÏùò Ïò®ÎèÑ Ï†ïÎ≥¥Í∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.)', 'woQM': 'ÌòÑÏû¨ Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÍ≥† Î∞©Í≥º Ïò®ÎèÑ Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.', 'woScript': 'Ïö∞Î¶¨Î∞ò(27.00¬∞C)Í≥º ÏïûÎ∞ò(27.00¬∞C)Ïùò Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Îäî 0.00¬∞CÏûÖÎãàÎã§. Îëê Î∞©Ïùò Ïò®ÎèÑÍ∞Ä ÎèôÏùºÌï©ÎãàÎã§.', 'GT_Response': 'ÏïûÎ∞òÏù¥(Í∞Ä) 27.00¬∞CÎ°ú Í∞ÄÏû• ÎçîÏö¥ Î∞©Ïù¥ÏóêÏöî.'}}\n"
     ]
    }
   ],
   "source": [
    "responses = {}\n",
    "\n",
    "for name in names:\n",
    "    # print(name, exp_tag)\n",
    "    exp_tag = name.split(\"_\")[3]\n",
    "    with open(f\"../experiments/result_23/{name}_response.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            input = item[\"Input\"]\n",
    "            if input not in responses:\n",
    "                responses[input] = {\n",
    "                    \"GT_Response\": item[\"GT_Response\"],\n",
    "                }\n",
    "            # if \"GT_Response\" in item:\n",
    "            #     print(item[\"GT_Response\"])\n",
    "            if \"PD_Response\" in item:\n",
    "                pd_response = item[\"PD_Response\"]\n",
    "            else:\n",
    "                pd_response = \"Ïã§ÌñâÏ§ë ÏóêÎü¨ Î∞úÏÉù\"\n",
    "            \n",
    "            responses[input][f\"{exp_tag}\"] = pd_response\n",
    "\n",
    "# if the response is exactly equal, then merge them and make in to one, key is then tuple\n",
    "for input, response in responses.items():\n",
    "    if len(response) == 1:\n",
    "        continue\n",
    "    \n",
    "    # merge every matching pd_response (not only first one but every combination)\n",
    "    # create groups of responses with same values\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # group responses by their values (excluding GT_Response)\n",
    "    value_groups = defaultdict(list)\n",
    "    \n",
    "    for key, value in response.items():\n",
    "        if key != \"GT_Response\":\n",
    "            value_groups[value].append(key)\n",
    "    \n",
    "    # merge keys that have the same response values\n",
    "    merged_responses = {}\n",
    "    for value, keys in value_groups.items():\n",
    "        if len(keys) > 1:\n",
    "            # create tuple key for merged responses\n",
    "            merged_key = str(tuple(sorted(keys)))\n",
    "            merged_responses[merged_key] = value\n",
    "        else:\n",
    "            # keep single responses as is\n",
    "            merged_responses[keys[0]] = value\n",
    "    \n",
    "    # add back GT_Response\n",
    "    merged_responses[\"GT_Response\"] = response[\"GT_Response\"]\n",
    "    \n",
    "    # for key in merged_responses:\n",
    "    #     if isinstance(merged_responses[key], list):\n",
    "    #         merged_responses[key] = \" \".join(merged_responses[key])\n",
    "\n",
    "    # update responses dict\n",
    "    responses[input] = merged_responses\n",
    "\n",
    "# delete input = Ïò§Îäò Ïò§ÌõÑ 5ÏãúÏóê ÏòÜÎ∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÎäî Ïñ¥Îï†Ïñ¥?\n",
    "del responses[\"Ïò§Îäò Ïò§ÌõÑ 5ÏãúÏóê ÏòÜÎ∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÎäî Ïñ¥Îï†Ïñ¥?\"]\n",
    "del responses[\"ÏßÄÍ∏à 4Ï∏µ ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ ÏïåÎ†§Ï§ò\"]\n",
    "\n",
    "# import pprint\n",
    "# pprint.pprint(responses)\n",
    "\n",
    "# save to json\n",
    "with open(\"responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, ensure_ascii=False, indent=4)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
