{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.db.manager import DBManager\n",
    "from src.input_to_instructions.load_and_execute import *\n",
    "from src.input_to_instructions.types import *\n",
    "from src.operation.execute import *\n",
    "from src.response_generation.load_and_execute import *\n",
    "from src.dateutils import normalize_sql_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "# from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\"\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_implementation: flash_attention_2, torch_dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "print(f\"attn_implementation: {attn_implementation}, torch_dtype: {torch_dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unsloth_zoo.log:*** Could not find model_type for config = ExaoneConfig {\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"ExaoneForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_exaone.ExaoneConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_exaone.ExaoneForCausalLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_exaone.ExaoneForSequenceClassification\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"exaone\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      " ***\n",
      "INFO:unsloth_zoo.log:*** Could not find model_type for config = ExaoneConfig {\n",
      "  \"activation_function\": \"silu\",\n",
      "  \"architectures\": [\n",
      "    \"ExaoneForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_exaone.ExaoneConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_exaone.ExaoneForCausalLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"modeling_exaone.ExaoneForSequenceClassification\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"embed_dropout\": 0.0,\n",
      "  \"eos_token_id\": 361,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"exaone\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      " ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.10.7: Fast Exaone patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA H100 NVL. Num GPUs = 1. Max memory: 93.016 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Exaone does not support SDPA - switching to fast eager.\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168acdb9531a4f5085a911f0f5fbf8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct', vocab_size=102400, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '[BOS]', 'eos_token': '[|endofturn|]', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t5: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t6: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t7: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t8: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t9: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t10: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t11: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t12: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t13: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t14: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t15: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t16: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t17: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t18: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t19: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t20: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t21: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t22: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t23: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t24: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t25: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t26: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t27: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t28: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t29: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t30: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t31: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t33: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t34: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t35: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t36: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t37: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t38: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t39: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t40: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t41: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t42: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t43: AddedToken(\"<|c|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t44: AddedToken(\"<|c++|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t45: AddedToken(\"<|python|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t46: AddedToken(\"<|javascript|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t47: AddedToken(\"<|markdown|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t48: AddedToken(\"<|html|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t49: AddedToken(\"<|css|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50: AddedToken(\"<|vue|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t51: AddedToken(\"<|java|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t52: AddedToken(\"PI:URL\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53: AddedToken(\"PI:EMAIL\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t54: AddedToken(\"PI:ACCOUNT_NUM\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t55: AddedToken(\"PI:PHONE_NUM\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t56: AddedToken(\"PI:BUSINESS_NUM\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t57: AddedToken(\"PI:ANNON\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t58: AddedToken(\"PI:KEY\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t59: AddedToken(\"PI:ID\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t60: AddedToken(\"PI:IP_ADDRESS\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t61: AddedToken(\"PI:USER\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t62: AddedToken(\"[unused0]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t63: AddedToken(\"[unused1]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t64: AddedToken(\"[unused2]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t65: AddedToken(\"[unused3]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t66: AddedToken(\"[unused4]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t67: AddedToken(\"[unused5]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t68: AddedToken(\"[unused6]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t69: AddedToken(\"[unused7]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t70: AddedToken(\"[unused8]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t71: AddedToken(\"[unused9]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t72: AddedToken(\"[unused10]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t73: AddedToken(\"[unused11]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t74: AddedToken(\"[unused12]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t75: AddedToken(\"[unused13]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t76: AddedToken(\"[unused14]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t77: AddedToken(\"[unused15]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t78: AddedToken(\"[unused16]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t79: AddedToken(\"[unused17]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t80: AddedToken(\"[unused18]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t81: AddedToken(\"[unused19]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t82: AddedToken(\"[unused20]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t83: AddedToken(\"[unused21]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t84: AddedToken(\"[unused22]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t85: AddedToken(\"[unused23]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t86: AddedToken(\"[unused24]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t87: AddedToken(\"[unused25]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t88: AddedToken(\"[unused26]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t89: AddedToken(\"[unused27]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t90: AddedToken(\"[unused28]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t91: AddedToken(\"[unused29]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t92: AddedToken(\"[unused30]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t93: AddedToken(\"[unused31]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t94: AddedToken(\"[unused32]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t95: AddedToken(\"[unused33]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t96: AddedToken(\"[unused34]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t97: AddedToken(\"[unused35]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t98: AddedToken(\"[unused36]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t99: AddedToken(\"[unused37]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[unused38]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[unused39]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[unused40]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[unused41]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t104: AddedToken(\"[unused42]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t105: AddedToken(\"[unused43]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t106: AddedToken(\"[unused44]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t107: AddedToken(\"[unused45]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t108: AddedToken(\"[unused46]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t109: AddedToken(\"[unused47]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t110: AddedToken(\"[unused48]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t111: AddedToken(\"[unused49]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t112: AddedToken(\"[unused50]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t113: AddedToken(\"[unused51]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t114: AddedToken(\"[unused52]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t115: AddedToken(\"[unused53]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t116: AddedToken(\"[unused54]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t117: AddedToken(\"[unused55]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t118: AddedToken(\"[unused56]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t119: AddedToken(\"[unused57]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t120: AddedToken(\"[unused58]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t121: AddedToken(\"[unused59]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t122: AddedToken(\"[unused60]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t123: AddedToken(\"[unused61]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t124: AddedToken(\"[unused62]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t125: AddedToken(\"[unused63]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t126: AddedToken(\"[unused64]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t127: AddedToken(\"[unused65]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128: AddedToken(\"[unused66]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t129: AddedToken(\"[unused67]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t130: AddedToken(\"[unused68]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t131: AddedToken(\"[unused69]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t132: AddedToken(\"[unused70]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t133: AddedToken(\"[unused71]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t134: AddedToken(\"[unused72]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t135: AddedToken(\"[unused73]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t136: AddedToken(\"[unused74]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t137: AddedToken(\"[unused75]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t138: AddedToken(\"[unused76]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t139: AddedToken(\"[unused77]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t140: AddedToken(\"[unused78]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t141: AddedToken(\"[unused79]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t142: AddedToken(\"[unused80]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t143: AddedToken(\"[unused81]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t144: AddedToken(\"[unused82]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t145: AddedToken(\"[unused83]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t146: AddedToken(\"[unused84]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t147: AddedToken(\"[unused85]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t148: AddedToken(\"[unused86]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t149: AddedToken(\"[unused87]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t150: AddedToken(\"[unused88]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151: AddedToken(\"[unused89]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t152: AddedToken(\"[unused90]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t153: AddedToken(\"[unused91]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t154: AddedToken(\"[unused92]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t155: AddedToken(\"[unused93]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t156: AddedToken(\"[unused94]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t157: AddedToken(\"[unused95]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t158: AddedToken(\"[unused96]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t159: AddedToken(\"[unused97]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t160: AddedToken(\"[unused98]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t161: AddedToken(\"[unused99]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t162: AddedToken(\"[extra_id_0]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t163: AddedToken(\"[extra_id_1]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t164: AddedToken(\"[extra_id_2]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t165: AddedToken(\"[extra_id_3]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t166: AddedToken(\"[extra_id_4]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t167: AddedToken(\"[extra_id_5]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t168: AddedToken(\"[extra_id_6]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t169: AddedToken(\"[extra_id_7]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t170: AddedToken(\"[extra_id_8]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t171: AddedToken(\"[extra_id_9]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t172: AddedToken(\"[extra_id_10]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t173: AddedToken(\"[extra_id_11]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t174: AddedToken(\"[extra_id_12]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t175: AddedToken(\"[extra_id_13]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t176: AddedToken(\"[extra_id_14]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t177: AddedToken(\"[extra_id_15]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t178: AddedToken(\"[extra_id_16]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t179: AddedToken(\"[extra_id_17]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t180: AddedToken(\"[extra_id_18]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t181: AddedToken(\"[extra_id_19]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t182: AddedToken(\"[extra_id_20]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t183: AddedToken(\"[extra_id_21]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t184: AddedToken(\"[extra_id_22]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t185: AddedToken(\"[extra_id_23]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t186: AddedToken(\"[extra_id_24]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t187: AddedToken(\"[extra_id_25]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t188: AddedToken(\"[extra_id_26]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t189: AddedToken(\"[extra_id_27]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t190: AddedToken(\"[extra_id_28]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t191: AddedToken(\"[extra_id_29]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t192: AddedToken(\"[extra_id_30]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t193: AddedToken(\"[extra_id_31]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t194: AddedToken(\"[extra_id_32]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t195: AddedToken(\"[extra_id_33]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t196: AddedToken(\"[extra_id_34]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t197: AddedToken(\"[extra_id_35]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t198: AddedToken(\"[extra_id_36]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t199: AddedToken(\"[extra_id_37]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t200: AddedToken(\"[extra_id_38]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t201: AddedToken(\"[extra_id_39]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t202: AddedToken(\"[extra_id_40]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t203: AddedToken(\"[extra_id_41]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t204: AddedToken(\"[extra_id_42]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t205: AddedToken(\"[extra_id_43]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t206: AddedToken(\"[extra_id_44]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t207: AddedToken(\"[extra_id_45]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t208: AddedToken(\"[extra_id_46]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t209: AddedToken(\"[extra_id_47]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t210: AddedToken(\"[extra_id_48]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t211: AddedToken(\"[extra_id_49]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t212: AddedToken(\"[extra_id_50]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t213: AddedToken(\"[extra_id_51]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t214: AddedToken(\"[extra_id_52]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t215: AddedToken(\"[extra_id_53]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t216: AddedToken(\"[extra_id_54]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t217: AddedToken(\"[extra_id_55]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t218: AddedToken(\"[extra_id_56]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t219: AddedToken(\"[extra_id_57]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t220: AddedToken(\"[extra_id_58]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t221: AddedToken(\"[extra_id_59]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t222: AddedToken(\"[extra_id_60]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t223: AddedToken(\"[extra_id_61]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t224: AddedToken(\"[extra_id_62]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t225: AddedToken(\"[extra_id_63]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t226: AddedToken(\"[extra_id_64]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t227: AddedToken(\"[extra_id_65]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t228: AddedToken(\"[extra_id_66]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t229: AddedToken(\"[extra_id_67]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t230: AddedToken(\"[extra_id_68]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t231: AddedToken(\"[extra_id_69]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t232: AddedToken(\"[extra_id_70]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t233: AddedToken(\"[extra_id_71]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t234: AddedToken(\"[extra_id_72]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t235: AddedToken(\"[extra_id_73]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t236: AddedToken(\"[extra_id_74]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t237: AddedToken(\"[extra_id_75]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t238: AddedToken(\"[extra_id_76]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t239: AddedToken(\"[extra_id_77]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t240: AddedToken(\"[extra_id_78]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t241: AddedToken(\"[extra_id_79]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t242: AddedToken(\"[extra_id_80]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t243: AddedToken(\"[extra_id_81]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t244: AddedToken(\"[extra_id_82]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t245: AddedToken(\"[extra_id_83]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t246: AddedToken(\"[extra_id_84]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t247: AddedToken(\"[extra_id_85]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t248: AddedToken(\"[extra_id_86]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t249: AddedToken(\"[extra_id_87]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t250: AddedToken(\"[extra_id_88]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t251: AddedToken(\"[extra_id_89]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t252: AddedToken(\"[extra_id_90]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t253: AddedToken(\"[extra_id_91]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t254: AddedToken(\"[extra_id_92]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t255: AddedToken(\"[extra_id_93]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t256: AddedToken(\"[extra_id_94]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t257: AddedToken(\"[extra_id_95]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t258: AddedToken(\"[extra_id_96]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t259: AddedToken(\"[extra_id_97]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t260: AddedToken(\"[extra_id_98]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t261: AddedToken(\"[extra_id_99]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t262: AddedToken(\"[extra_id_100]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t263: AddedToken(\"[extra_id_101]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t264: AddedToken(\"[extra_id_102]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t265: AddedToken(\"[extra_id_103]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t266: AddedToken(\"[extra_id_104]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t267: AddedToken(\"[extra_id_105]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t268: AddedToken(\"[extra_id_106]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t269: AddedToken(\"[extra_id_107]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t270: AddedToken(\"[extra_id_108]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t271: AddedToken(\"[extra_id_109]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t272: AddedToken(\"[extra_id_110]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t273: AddedToken(\"[extra_id_111]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t274: AddedToken(\"[extra_id_112]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t275: AddedToken(\"[extra_id_113]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t276: AddedToken(\"[extra_id_114]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t277: AddedToken(\"[extra_id_115]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t278: AddedToken(\"[extra_id_116]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t279: AddedToken(\"[extra_id_117]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t280: AddedToken(\"[extra_id_118]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t281: AddedToken(\"[extra_id_119]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t282: AddedToken(\"[extra_id_120]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t283: AddedToken(\"[extra_id_121]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t284: AddedToken(\"[extra_id_122]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t285: AddedToken(\"[extra_id_123]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t286: AddedToken(\"[extra_id_124]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t287: AddedToken(\"[extra_id_125]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t288: AddedToken(\"[extra_id_126]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t289: AddedToken(\"[extra_id_127]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t290: AddedToken(\"[extra_id_128]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t291: AddedToken(\"[extra_id_129]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t292: AddedToken(\"[extra_id_130]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t293: AddedToken(\"[extra_id_131]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t294: AddedToken(\"[extra_id_132]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t295: AddedToken(\"[extra_id_133]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t296: AddedToken(\"[extra_id_134]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t297: AddedToken(\"[extra_id_135]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t298: AddedToken(\"[extra_id_136]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t299: AddedToken(\"[extra_id_137]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t300: AddedToken(\"[extra_id_138]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t301: AddedToken(\"[extra_id_139]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t302: AddedToken(\"[extra_id_140]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t303: AddedToken(\"[extra_id_141]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t304: AddedToken(\"[extra_id_142]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t305: AddedToken(\"[extra_id_143]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t306: AddedToken(\"[extra_id_144]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t307: AddedToken(\"[extra_id_145]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t308: AddedToken(\"[extra_id_146]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t309: AddedToken(\"[extra_id_147]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t310: AddedToken(\"[extra_id_148]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t311: AddedToken(\"[extra_id_149]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t312: AddedToken(\"[extra_id_150]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t313: AddedToken(\"[extra_id_151]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t314: AddedToken(\"[extra_id_152]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t315: AddedToken(\"[extra_id_153]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t316: AddedToken(\"[extra_id_154]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t317: AddedToken(\"[extra_id_155]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t318: AddedToken(\"[extra_id_156]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t319: AddedToken(\"[extra_id_157]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t320: AddedToken(\"[extra_id_158]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t321: AddedToken(\"[extra_id_159]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t322: AddedToken(\"[extra_id_160]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t323: AddedToken(\"[extra_id_161]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t324: AddedToken(\"[extra_id_162]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t325: AddedToken(\"[extra_id_163]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t326: AddedToken(\"[extra_id_164]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t327: AddedToken(\"[extra_id_165]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t328: AddedToken(\"[extra_id_166]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t329: AddedToken(\"[extra_id_167]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t330: AddedToken(\"[extra_id_168]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t331: AddedToken(\"[extra_id_169]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t332: AddedToken(\"[extra_id_170]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t333: AddedToken(\"[extra_id_171]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t334: AddedToken(\"[extra_id_172]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t335: AddedToken(\"[extra_id_173]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t336: AddedToken(\"[extra_id_174]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t337: AddedToken(\"[extra_id_175]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t338: AddedToken(\"[extra_id_176]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t339: AddedToken(\"[extra_id_177]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t340: AddedToken(\"[extra_id_178]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t341: AddedToken(\"[extra_id_179]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t342: AddedToken(\"[extra_id_180]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t343: AddedToken(\"[extra_id_181]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t344: AddedToken(\"[extra_id_182]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t345: AddedToken(\"[extra_id_183]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t346: AddedToken(\"[extra_id_184]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t347: AddedToken(\"[extra_id_185]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t348: AddedToken(\"[extra_id_186]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t349: AddedToken(\"[extra_id_187]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t350: AddedToken(\"[extra_id_188]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t351: AddedToken(\"[extra_id_189]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t352: AddedToken(\"[extra_id_190]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t353: AddedToken(\"[extra_id_191]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t354: AddedToken(\"[extra_id_192]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t355: AddedToken(\"[extra_id_193]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t356: AddedToken(\"[extra_id_194]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t357: AddedToken(\"[extra_id_195]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t358: AddedToken(\"[extra_id_196]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t359: AddedToken(\"[extra_id_197]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t360: AddedToken(\"[extra_id_198]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t361: AddedToken(\"[|endofturn|]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ResponseGeneration.update_prompt()\n",
    "\n",
    "ResponseGeneration.initialize(\n",
    "    log_output=False,\n",
    "    instance_type=\"unsloth\"\n",
    ")\n",
    "tokenizer = ResponseGeneration.tokenizer\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from src.input_to_instructions.types import InstructionQ_raw\n",
    "def get_time(df, fmt=\"datetime\"):\n",
    "    # from df get 'timestamp' column and return them in format\n",
    "    if fmt == \"date\":\n",
    "        fmt = '%Y-%m-%d'\n",
    "    elif fmt == \"month\":\n",
    "        fmt = '%Y-%m'\n",
    "    elif fmt == \"year\":\n",
    "        fmt = '%Y'\n",
    "    else:\n",
    "        fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    if isinstance(df['timestamp'], pd.Timestamp):\n",
    "        result = df['timestamp'].strftime(fmt)\n",
    "    else:\n",
    "        result = df['timestamp'].apply(lambda x: x.strftime(fmt))\n",
    "    return sorted(list(set(result)))\n",
    "\n",
    "def get_spatials(df):\n",
    "    return pd.unique(df['idu_name'])\n",
    "\n",
    "def get_tv(df, col:str|list[str], fmt=\"datetime\"):\n",
    "    if isinstance(col, str):\n",
    "        col = [col]\n",
    "    \n",
    "    timestamps = get_time(df, fmt)\n",
    "    return_tuple = tuple([timestamps] + [df[c] for c in col])\n",
    "    return return_tuple\n",
    "\n",
    "def data_(metadata, mapping, query_results, t=str|list[str], s=str|list[str], m=str|list[str]):\n",
    "    if isinstance(t, str):\n",
    "        t = [t]\n",
    "    if isinstance(s, str):\n",
    "        s = [s]\n",
    "    if isinstance(m, str):\n",
    "        m = [m]\n",
    "\n",
    "    t_raw = [mapping.temporal[t_highlevel] for t_highlevel in t]\n",
    "    s_raw = [mapping.spatials[s_highlevel] for s_highlevel in s]\n",
    "    m_raw = [mapping.modalities[m_highlevel] for m_highlevel in m]\n",
    "    \n",
    "    # flatten s_raw into a list of strings\n",
    "    # flattened = [item for sublist in data for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    s_raw = [item for sublist in s_raw for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    # print(s_raw)\n",
    "    result_df = DBManager.structured_query_data_t_v2(metadata, m_raw, t_raw, s_raw, get_rowids=True)\n",
    "    \n",
    "    cols = list(result_df.columns)\n",
    "    cols.remove(\"id\")\n",
    "    cols.remove(\"idu_name\")\n",
    "    cols.remove(\"timestamp\")\n",
    "    rows = list(result_df[\"id\"])\n",
    "    query_results.append({\n",
    "        \"result_columns\": cols,\n",
    "        \"result_indices\": rows,\n",
    "    })\n",
    "    # print(cols, rows)\n",
    "\n",
    "    # For demo, drop rows where any value is -1\n",
    "    result_df = result_df.loc[(result_df != -1).all(axis=1)]\n",
    "\n",
    "    # drop \"id\" from result_df\n",
    "    result_df = result_df.drop(columns=['id'])\n",
    "\n",
    "    # change column names to high level\n",
    "    inverse_mapping = {v: k for k, v in mapping.modalities.items()}\n",
    "    result_df.columns = [inverse_mapping[col] if col in inverse_mapping else col for col in result_df.columns]\n",
    "\n",
    "    # change idu_name raw values to high level\n",
    "    inverse_mapping = {}\n",
    "    for k, v in mapping.spatials.items():\n",
    "        if isinstance(v, list):\n",
    "            for v_ in v:\n",
    "                inverse_mapping[v_] = k\n",
    "        else:\n",
    "            inverse_mapping[v] = k\n",
    "\n",
    "    result_df[\"idu_name\"] = result_df[\"idu_name\"].map(inverse_mapping)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def run_query_v2(user_input, metadata, mapping, expectations, required_variables, scripts, exp_tag=None):\n",
    "    query_results = []\n",
    "    variables = {}\n",
    "    # print(f\"exp_tag: {exp_tag}\")\n",
    "    if scripts is not None:\n",
    "\n",
    "        # search data(t=~~, ...,)\n",
    "        globals()['metadata'] = metadata\n",
    "        globals()['mapping'] = mapping\n",
    "        globals()['query_results'] = query_results\n",
    "        for name in list(globals()):\n",
    "            if name.startswith(\"v_\"):\n",
    "                del globals()[name]\n",
    "        try:\n",
    "            query_time = 0\n",
    "            process_time = 0\n",
    "            \n",
    "            for script in scripts:\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    if \"data\" in script:\n",
    "                        script = script.replace(\"data(\", \"data_(metadata, mapping, query_results, \")\n",
    "                    \n",
    "                    if \"SELECT\" in script:\n",
    "                        # split only at the first '=' to avoid issues with '=' in SQL\n",
    "                        variable, sql = script.split(\"=\", 1)\n",
    "                        variable = variable.strip()\n",
    "                        sql = sql.strip()\n",
    "                        # get all between \\\" and \\\"\n",
    "                        sql = re.findall(r'\"(.*)\"', sql)\n",
    "                        sql = sql[0]\n",
    "                        # \"SELECT\"ë¼ëŠ” ì²« ë²ˆì§¸ ë“±ìž¥ë§Œ \"SELECT id \"ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "                        sql = sql.replace(\"SELECT\", \"SELECT id, \", 1)\n",
    "                        df = DBManager.execute_structured_query_string(sql)\n",
    "                        cols = list(df.columns)\n",
    "                        cols.remove(\"id\")\n",
    "                        cols.remove(\"idu_name\")\n",
    "                        cols.remove(\"timestamp\")\n",
    "                        rows = list(df[\"id\"])\n",
    "                        query_results.append({\n",
    "                            \"result_columns\": cols,\n",
    "                            \"result_indices\": rows,\n",
    "                        })\n",
    "                        df = df.drop(columns=['id'])\n",
    "                        globals()[variable] = df\n",
    "                    else:\n",
    "                        exec(script, globals())\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    if \"data\" in script:\n",
    "                        query_time += end_time - start_time\n",
    "                    else:\n",
    "                        process_time += end_time - start_time\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in executing script: {script}\")\n",
    "                    print(e)\n",
    "                    raise e\n",
    "\n",
    "            start_time = time.time()\n",
    "            variables = {name:globals()[name] for name in globals() if name.startswith(\"v_\")}\n",
    "            response, required_variables = ResponseGeneration.execute_v2(expectations, required_variables, variables, user_input, exp_tag=exp_tag)\n",
    "            # rg_last_input_token_length = measure_token_count(ResponseGeneration.last_input_str)\n",
    "            # rg_last_output_token_length = measure_token_count(response)\n",
    "            # print(\"rg_last_input_token_length,\", rg_last_input_token_length, \",rg_last_output_token_length,\", rg_last_output_token_length)\n",
    "            \n",
    "            response_generation_time = time.time() - start_time\n",
    "\n",
    "            # print(f\"ì§ˆë¬¸: {user_input}, ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„: {query_time:.4f}ì´ˆ, í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì‹œê°„: {process_time:.4f}ì´ˆ, ì‘ë‹µ ìƒì„± ì‹œê°„: {response_generation_time:.4f}ì´ˆ\")\n",
    "            return response, variables, required_variables, query_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error in running query_v2: {e}\")\n",
    "            return \"ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\", variables, None, query_results\n",
    "    else:\n",
    "        if exp_tag in [\"woQM\", \"woQM+Script\"]:\n",
    "            response, required_variables = ResponseGeneration.execute_v2(expectations, required_variables, variables, user_input, exp_tag=exp_tag)\n",
    "            return response, variables, required_variables, query_results\n",
    "        else:\n",
    "            variables = {}\n",
    "            unknown_spatials = [k for k, v in mapping.spatials.items() if v == \"Unknown\"]\n",
    "            unknown_modalities = [k for k, v in mapping.modalities.items() if v == \"Unknown\"]\n",
    "            \n",
    "            response_unknown = f\"ì£„ì†¡í•©ë‹ˆë‹¤, {unknown_spatials + unknown_modalities}ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"\n",
    "            return response_unknown, variables, [], query_results\n",
    "\n",
    "\n",
    "def run_query(user_input, metadata, instructions, exp_tag=None):\n",
    "    variables = {\n",
    "        \"Metadata\": metadata,\n",
    "    }\n",
    "    query_results = []\n",
    "        \n",
    "    \n",
    "    for instruction in instructions:\n",
    "        # logger.debug(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        # print(f\"Executing instruction: {instruction.__class__.__name__}\")\n",
    "        \n",
    "        if type(instruction) == InstructionQ:\n",
    "            # Execute query\n",
    "            result_df = DBManager.structured_query_data_t(metadata, instruction.args, get_rowids=True)\n",
    "            # if result_df is None:\n",
    "                # print(\"ì£„ì†¡í•©ë‹ˆë‹¤, ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\", \"response\")\n",
    "                # return\n",
    "\n",
    "            cols = list(result_df.columns)\n",
    "            cols.remove(\"id\")\n",
    "            cols.remove(\"idu\")\n",
    "            rows = list(result_df[\"id\"])\n",
    "\n",
    "            query_results.append({\n",
    "                \"result_columns\": cols,\n",
    "                \"result_indices\": rows,\n",
    "            })\n",
    "\n",
    "            # For demo, drop rows where any value is -1\n",
    "            result_df = result_df.loc[(result_df != -1).all(axis=1)]\n",
    "\n",
    "            # drop \"id\" from result_df\n",
    "            result_df = result_df.drop(columns=['id'])\n",
    "           \n",
    "            #pd.set_option('display.max_rows', 10000)        \n",
    "            #pd.set_option('display.max_columns', 1000)\n",
    "            #pd.set_option('display.width', 1000)\n",
    "            #pd.set_option('display.max_colwidth', 1000)\n",
    "            #print(f\"QueryResult: {result_df}\")\n",
    "\n",
    "            variables[instruction.result_name] = result_df\n",
    "        elif type(instruction) == InstructionQ_raw:\n",
    "            instruction.query = instruction.query.replace(\" FROM \\\"data_t\\\"\", \", \\\"id\\\" FROM \\\"data_t\\\"\")\n",
    "            result_df = DBManager.execute_structured_query_string(\n",
    "                instruction.query\n",
    "            )\n",
    "            # rename idu_name to idu\n",
    "            result_df = result_df.rename(columns={'idu_name': 'idu'})\n",
    "            \n",
    "            cols = list(result_df.columns)\n",
    "            cols.remove(\"id\")\n",
    "            cols.remove(\"idu\")\n",
    "            rows = list(result_df[\"id\"])\n",
    "\n",
    "            query_results.append({\n",
    "                \"result_columns\": cols,\n",
    "                \"result_indices\": rows,\n",
    "            })\n",
    "\n",
    "            # drop \"id\" from result_df\n",
    "            result_df = result_df.drop(columns=['id'])\n",
    "            \n",
    "            variables[instruction.result_name] = result_df\n",
    "            # print(result_df, flush=True)\n",
    "\n",
    "        elif type(instruction) == InstructionO:\n",
    "            # Execute operation\n",
    "            # variables_to_report = {k: v for k, v in variables.items() if k not in [\"Metadata\"]}\n",
    "            # print(variables_to_report)\n",
    "            result_dict = OperationExecutor.execute(variables, instruction.scripts)\n",
    "            # print(instruction.scripts, instruction.returns, result_dict)\n",
    "            variables.update(result_dict)\n",
    "            pass\n",
    "            # print(fig, \"graph\")\n",
    "        elif type(instruction) == InstructionR:\n",
    "            # Execute response generation\n",
    "            variables_to_report = {k: v for k, v in variables.items() if k not in [\"Metadata\"]}\n",
    "            # print(variables_to_report)\n",
    "            # variables_to_report = ResponseGeneration.stringify_variables(variables_to_report)\n",
    "            # variables_to_report = summarize_variables_to_report(variables_to_report)\n",
    "\n",
    "            # print(f\"Variables: {variables_to_report}\")\n",
    "\n",
    "            keys_to_leave = [\"modality_mapping\", \"idu_mapping\"]\n",
    "            metadata_ = {}\n",
    "            for key in metadata.keys():\n",
    "                if key in keys_to_leave:\n",
    "                    metadata_[key] = metadata[key]\n",
    "\n",
    "            response, required_variables = ResponseGeneration.execute(instruction, variables, user_input, metadata_, exp_tag=exp_tag)\n",
    "            # print(f\"Required variables: {required_variables}\")\n",
    "            \n",
    "            # response = instruction.expectations[0] # \"{{var}}...\"\n",
    "            # for var_name, var_value in required_variables.items():\n",
    "            #     placeholder = f\"{{{{{var_name}}}}}\"\n",
    "            #     if placeholder in response:\n",
    "            #         response = response.replace(placeholder, str(var_value))\n",
    "\n",
    "            \n",
    "            return response, variables_to_report, required_variables, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dataset_name}\")\n",
    "\n",
    "def build_query_groundtruth():\n",
    "    \n",
    "    def read(path):\n",
    "        data = read_json(path)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = directory.name\n",
    "            if \"v7\" in dataset_name:\n",
    "                data[i][\"Metadata\"] = metadata\n",
    "        return data\n",
    "\n",
    "    ds_ts = []\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir():\n",
    "            if \"v7\" in dataset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            \n",
    "            # d = read(f\"{directory}/onlyq_ts.json\")\n",
    "            \n",
    "            ds_ts.extend(read(f\"{directory}/onlyq_ts.json\"))\n",
    "            ds_ts.extend(read(f\"{directory}/onlyq_tr.json\"))\n",
    "            # ds_tr.extend(read(f\"{directory}/graph.json\"))\n",
    "    \n",
    "    ds = ds_ts\n",
    "    print(len(ds))\n",
    "    \n",
    "    gts = []\n",
    "\n",
    "    for d in ds:\n",
    "        cont = False\n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "        for st in skip_tags:\n",
    "            if st in tags:\n",
    "                cont = True\n",
    "                break\n",
    "        if cont:\n",
    "            continue\n",
    "\n",
    "        # pbar.set_description(f\"Processing {d['Input']}\")\n",
    "        # print(\"--\")\n",
    "        exp_tag = \"v2\"\n",
    "        # print(f\"Warning! exp_tag is v2\")\n",
    "\n",
    "        # expertLLM_output_token_length = measure_token_count(d['Response'])\n",
    "        # expertLLM_input_token_length = measure_token_count(d['Input'])\n",
    "        # print(\"Input,\", d['Input'], \",expertLLM_output_tlen,\",  expertLLM_output_token_length, \",expertLLM_input_tlen,\", expertLLM_input_token_length)\n",
    "        mapping, expectations, required_variables, scripts = InputToInstruction.postprocess_v2(deepcopy(d['Response']), exp_tag=exp_tag)\n",
    "        user_input, tags, metadata, scenario = d[\"Input\"], d[\"Tags\"], d[\"Metadata\"], d[\"Scenario\"]\n",
    "        # if user_input != \"ì§€ê¸ˆ ëª‡ì‹œì•¼?\":\n",
    "        #     continue\n",
    "\n",
    "        response, variables_to_report, required_variables, query_results = run_query_v2(\n",
    "            user_input, metadata, mapping, expectations, required_variables, scripts, exp_tag=exp_tag\n",
    "        )\n",
    "        print(f\"ì¶œë ¥: {response}\")\n",
    "        # print({k: (v, type(v)) for k, v in variables_to_report.items()})\n",
    "        gts.append({\n",
    "            \"Input\": user_input,\n",
    "            \"Metadata\": metadata,\n",
    "            \"Scenario\": scenario,\n",
    "            \"Tags\": tags,\n",
    "            \"GT\": d['Response'],\n",
    "            \"Response\": response,\n",
    "            # \"RequiredVariables\": required_variables,\n",
    "            \"QueryResults\": query_results,\n",
    "            # \"VariablesToReport\": variables_to_report,\n",
    "        })\n",
    "\n",
    "    # save to json\n",
    "    with open(f\"./gts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(gts, f, ensure_ascii=False, indent=4)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_token_count(input: str) -> int:\n",
    "    return len(tokenizer.encode(str(input)))\n",
    "\n",
    "# # ResponseGeneration.update_prompt()\n",
    "# build_query_groundtruth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class UnslothInference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint_dir: str,\n",
    "        cache_dir: str,\n",
    "        max_seq_length: int = 3500,\n",
    "        attn_implementation: str = attn_implementation,\n",
    "        batch_size: int = 8  # ë°°ì¹˜ í¬ê¸° ë§¤ê°œë³€ìˆ˜ ì¶”ê°€\n",
    "    ):\n",
    "        if 'checkpoint' in checkpoint_dir:\n",
    "            self.checkpoint_dir = Path(checkpoint_dir)\n",
    "            if not self.checkpoint_dir.exists():\n",
    "                raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "        else:\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.attn_implementation = attn_implementation\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Verify model files exist\n",
    "        # if not (self.checkpoint_dir / \"config.json\").exists():\n",
    "        #     raise ValueError(f\"config.json not found in {checkpoint_dir}\")\n",
    "        \n",
    "        # Set torch dtype based on GPU capability\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize model and tokenizer for the given rank.\"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            try:\n",
    "                if isinstance(self.checkpoint_dir, Path):\n",
    "                    checkpoint_dir = self.checkpoint_dir.as_posix()\n",
    "                else:\n",
    "                    checkpoint_dir = self.checkpoint_dir\n",
    "\n",
    "                self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "                    checkpoint_dir,\n",
    "                    dtype = self.torch_dtype,\n",
    "                    load_in_4bit = False,\n",
    "                    load_in_8bit = False,\n",
    "                    attn_implementation=self.attn_implementation,\n",
    "                    cache_dir=self.cache_dir.as_posix(),\n",
    "                    local_files_only=True,\n",
    "                    device_map=\"cuda\",\n",
    "                )\n",
    "                FastLanguageModel.for_inference(self.model)\n",
    "            \n",
    "                self.tokenizer.padding_side = \"left\"\n",
    "                print(f\"Model loaded from {self.checkpoint_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in setup_model {str(e)}\")\n",
    "                raise\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_content(text: str):\n",
    "        \"\"\"Extract content from model output.\"\"\"\n",
    "        if \"start_header_id\" in text:\n",
    "            pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "        elif \"start_of_turn\" in text:\n",
    "            pattern = r\"<start_of_turn>model\\n(.*?)<eos>\"\n",
    "        elif \"im_start\" in text:\n",
    "            # <|im_start|>assistant{\"Thinking\": \"ì‚¬ìš©ìžëŠ” ì˜¤ëŠ˜ 4ì¸µì— ìžˆëŠ” ëª¨ë“  ë°©ì˜ ì„¤ì •ì˜¨ë„ì˜ í‰ê· ê°’ì„ ì•Œê³  ì‹¶ì–´í•©ë‹ˆë‹¤. 4ì¸µì— í•´ë‹¹í•˜ëŠ” iduë“¤(01_IB7, 02_I84, 02_I85)ì˜ ì˜¤ëŠ˜ ì„¤ì •ì˜¨ë„ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•œ í›„ í‰ê· ê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•˜ë©´ ë©ë‹ˆë‹¤.\", \"Expectations\": [\"ì˜¤ëŠ˜ 4ì¸µì˜ í‰ê·  ì„¤ì •ì˜¨ë„ëŠ” {{settemp_avg}}â„ƒ ìž…ë‹ˆë‹¤.\"], \"Instructions\": [{\"type\": \"q\", \"args\": {\"table_name\": \"data_t\", \"columns\": [\"settemp\"], \"temporal\": \"[DATE_TRUNC('day', DATE 'CURRENT_DATE'), DATE_TRUNC('day', DATE 'CURRENT_DATE' + INTERVAL '1 day'))\", \"spatials\": [\"01_IB7\", \"02_I84\", \"02_I85\"]}, \"result_name\": \"qr\"}, {\"type\": \"o\", \"script\": \"settemp_avg = qr['settemp'].mean();\", \"returns\": [\"settemp_avg\"]}]}<|im_end|>\n",
    "            pattern = r\"<\\|im_start\\|>assistant\\n(.*?)<\\|im_end\\|>\"\n",
    "        elif \"|endofturn|\" in text:\n",
    "            pattern = r\"\\[\\|assistant\\|\\](.*?)\\[\\|endofturn\\|\\]\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    def process_batch(\n",
    "        self,\n",
    "        batch_data,\n",
    "        common_prompt,\n",
    "    ):\n",
    "        try:\n",
    "            batch_data = Dataset.from_list(batch_data)\n",
    "            model: AutoModelForCausalLM = self.model\n",
    "            tokenizer: AutoTokenizer = self.tokenizer\n",
    "\n",
    "            convos = []\n",
    "            for metadata, input in zip(batch_data[\"Metadata\"], batch_data[\"Input\"]):\n",
    "                if \"llama\" in model.config.architectures[0].lower():\n",
    "                    chat = [\n",
    "                        {\"role\": \"system\", \"content\": common_prompt},\n",
    "                        {\"role\": \"user\", \"content\": f\"Metadata:{metadata};Input:{input};\"},\n",
    "                    ]\n",
    "                elif \"gemma\" in model.config.architectures[0].lower():\n",
    "                    chat = [\n",
    "                        {\"role\": \"user\", \"content\": f\"{common_prompt};{json.dumps(metadata)};{input}\"},\n",
    "                    ]\n",
    "                else:\n",
    "                    chat = [\n",
    "                        {\"role\": \"system\", \"content\": common_prompt},\n",
    "                        {\"role\": \"user\", \"content\": f\"Metadata:{metadata};Input:{input};\"},\n",
    "                    ]\n",
    "                    # raise ValueError(f\"Unsupported model architecture: {model.config.architectures[0]}\")\n",
    "                \n",
    "                chat = tokenizer.apply_chat_template(\n",
    "                    chat,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(model.device)\n",
    "                convos.append(chat)\n",
    "            \n",
    "            max_length = max(inputs.size(1) for inputs in convos)\n",
    "        \n",
    "            # íŒ¨ë”© ì ìš©í•˜ì—¬ ìž…ë ¥ ì¤€ë¹„\n",
    "            padded_inputs = []\n",
    "            attention_masks = []\n",
    "            \n",
    "            for inputs in convos:\n",
    "                pad_length = max_length - inputs.size(1)\n",
    "                \n",
    "                if pad_length > 0:\n",
    "                    # íŒ¨ë”© ì¶”ê°€\n",
    "                    padded = torch.cat([\n",
    "                        torch.full((1, pad_length), tokenizer.pad_token_id, device=model.device),\n",
    "                        inputs,\n",
    "                    ], dim=1)\n",
    "                    \n",
    "                    # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (ì›ë³¸ ì‹œí€€ìŠ¤ëŠ” 1, íŒ¨ë”©ì€ 0)\n",
    "                    mask = torch.cat([\n",
    "                        torch.zeros(1, pad_length, device=model.device),\n",
    "                        torch.ones(1, inputs.size(1), device=model.device),\n",
    "                    ], dim=1)\n",
    "                else:\n",
    "                    padded = inputs\n",
    "                    mask = torch.ones(1, inputs.size(1), device=model.device)\n",
    "                \n",
    "                padded_inputs.append(padded)\n",
    "                attention_masks.append(mask)\n",
    "            \n",
    "            # ë°°ì¹˜ í…ì„œ ìƒì„±\n",
    "            batch_tensor = torch.cat(padded_inputs, dim=0)\n",
    "            attention_mask = torch.cat(attention_masks, dim=0)\n",
    "            # print(batch_tensor)\n",
    "            # ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch_tensor,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=self.max_seq_length,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False  # ê²°ì •ë¡ ì  ìƒì„±\n",
    "            )\n",
    "            \n",
    "            # ê²°ê³¼ ë””ì½”ë”© ë° íŒŒì‹±\n",
    "            responses = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "            # print(responses)\n",
    "            parsed_responses = []\n",
    "            for response in responses:\n",
    "                parsed = self.extract_content(response)\n",
    "                if parsed is None:\n",
    "                    print(f\"Error parsing response: {response[:100]}...\")\n",
    "                    parsed_responses.append(None)\n",
    "                else:\n",
    "                    parsed_responses.append(parsed)\n",
    "            # print(f\"Elapsed time: {end_time - start_time:.2f}s\")\n",
    "            return parsed_responses\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_batch: {str(e)}\")\n",
    "            return [None] * len(batch_data)\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        dataset,\n",
    "        common_prompt: str,\n",
    "        output_file: str\n",
    "    ):\n",
    "        \"\"\"Run inference in batches.\"\"\"\n",
    "            \n",
    "        # Setup model and tokenizer\n",
    "        self.setup_model()\n",
    "\n",
    "        self.model\n",
    "        # ë°°ì¹˜ ì²˜ë¦¬\n",
    "        \n",
    "        with tqdm(total=len(dataset)) as pbar:\n",
    "            for batch_start in range(0, len(dataset), self.batch_size):\n",
    "                batch_end = min(batch_start + self.batch_size, len(dataset))\n",
    "                batch_data = dataset[batch_start:batch_end]\n",
    "                \n",
    "                # ë°°ì¹˜ ì²˜ë¦¬\n",
    "                start_time = time.time()\n",
    "                # print(self.batch_size, batch_start, batch_end)\n",
    "                responses = self.process_batch(\n",
    "                    batch_data, common_prompt\n",
    "                )\n",
    "                time_taken = time.time() - start_time\n",
    "                print(f\"Time taken: {time_taken:.2f}s, Input: {batch_data[0]['Input']}\")\n",
    "                \n",
    "                # ê²°ê³¼ ì €ìž¥\n",
    "                for i, response in enumerate(responses):\n",
    "                    sample = batch_data[i]\n",
    "                    \n",
    "                    if response is not None:\n",
    "                        try:\n",
    "                            response = eval(response)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error in eval: {str(e)}\")\n",
    "                        \n",
    "                        result = {\n",
    "                            \"Input\": sample[\"Input\"],\n",
    "                            \"Scenario\": sample[\"Scenario\"],\n",
    "                            \"Metadata\": sample[\"Metadata\"],\n",
    "                            \"Candidate\": response,\n",
    "                        }\n",
    "                        \n",
    "                        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                            try:\n",
    "                                line = json.dumps(result, ensure_ascii=False) + \"\\n\"\n",
    "                                f.write(line)\n",
    "                            except Exception as e:\n",
    "                                print(result)\n",
    "                                response = \"ì˜¤ë¥˜ë°œìƒ\"\n",
    "                                result = {\n",
    "                                    \"Input\": sample[\"Input\"],\n",
    "                                    \"Scenario\": sample[\"Scenario\"],\n",
    "                                    \"Metadata\": sample[\"Metadata\"],\n",
    "                                    \"Candidate\": response,\n",
    "                                }\n",
    "                                f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "                    else:\n",
    "                        print(f\"Error in response for sample {batch_start + i}\")\n",
    "                \n",
    "                pbar.update(batch_end - batch_start)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_dataset(dir, path, train_type):\n",
    "\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    result = []\n",
    "    for d in data:\n",
    "        if train_type in [\"WoThinking\", \"WoMetadata+Thinking\"]:\n",
    "            del d[\"Response\"][\"Thinking\"]\n",
    "        elif train_type in [\"woExp\"]:\n",
    "            del d[\"Response\"][\"Expectations\"]\n",
    "        \n",
    "        if \"Script\" in d[\"Response\"]:\n",
    "            if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "                new_scripts = []\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" in script:\n",
    "                        new_scripts.append(script)\n",
    "                d[\"Response\"][\"Script\"] = new_scripts\n",
    "\n",
    "            if train_type in [\"woQM\"]:\n",
    "                new_scripts = []\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" in script:\n",
    "                        new_scripts.append(script)\n",
    "                    else:\n",
    "                        for m, n in [(\"ì‹¤ë‚´ì˜¨ë„\", \"roomtemp\"), (\"ì„¤ì •ì˜¨ë„\", \"settemp\")]:\n",
    "                            script = script.replace(f\"'{m}'\", f\"'{n}'\")\n",
    "                        \n",
    "                        new_scripts.append(script)\n",
    "                d[\"Response\"][\"Script\"] = new_scripts\n",
    "\n",
    "            if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "                mapping = d[\"Response\"][\"Mapping\"]\n",
    "                for i, script in enumerate(d[\"Response\"][\"Script\"]):\n",
    "                    if \"data\" not in script:\n",
    "                        continue\n",
    "\n",
    "                    t_match = re.search(r\"t=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    s_match = re.search(r\"s=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    m_match = re.search(r\"m=('[^']+'|\\[[^\\]]+\\])\", script)\n",
    "                    t = eval(t_match.group(1)) if t_match else None\n",
    "                    s = eval(s_match.group(1)) if s_match else None\n",
    "                    m = eval(m_match.group(1)) if m_match else None\n",
    "                    \n",
    "                    if isinstance(t, str):\n",
    "                        t = [t]\n",
    "                    if isinstance(s, str):\n",
    "                        s = [s]\n",
    "                    if isinstance(m, str):\n",
    "                        m = [m]\n",
    "\n",
    "                    t_raw = [mapping['temporal'][t_highlevel] for t_highlevel in t]\n",
    "                    s_raw = [mapping['spatials'][s_highlevel] for s_highlevel in s]\n",
    "                    m_raw = [mapping['modalities'][m_highlevel] for m_highlevel in m]\n",
    "                    s_raw = [item for sublist in s_raw for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "                    sql = DBManager.get_query_strings_v2(\n",
    "                        metadata, m_raw, t_raw, s_raw\n",
    "                    )\n",
    "                    sql = normalize_sql_dates(sql)\n",
    "                    # replace data(...) with sql using regex\n",
    "                    d[\"Response\"][\"Script\"][i] = re.sub(r\"data\\(([^)]+)\\)\", lambda x: f\"\\\"{sql}\\\"\", script)\n",
    "                del d[\"Response\"][\"Mapping\"]\n",
    "            #     # raise NotImplementedError\n",
    "            # elif train_type in [\"woOp\"]:\n",
    "            #     instructions = d[\"Response\"][\"Instructions\"]\n",
    "            #     d[\"Response\"][\"Instructions\"] = [i for i in instructions if i[\"type\"] == \"q\"]\n",
    "\n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "\n",
    "        skip_tags = [\"Reason\", \"Graph\", \"Unrelated\", \"Prediction\"]\n",
    "\n",
    "        skip = False\n",
    "        for skip_tag in skip_tags:\n",
    "            if skip_tag in tags:\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Scenarios\": dir.name, \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "        # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "        # print(f\"Read {len(result)} examples from {path}\")\n",
    "        # print(f\"Type of result: {type(result)}\")\n",
    "        # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "        # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "        # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "def sub(name, common_prompt):\n",
    "    # Remove the section between <|name|> ... <|name|> including the tags themselves\n",
    "    # Use re.DOTALL to match newlines with '.'\n",
    "    pattern = rf\"\\n?<\\|{name}\\|>[\\s\\S]*?<\\|{name}\\|>\"\n",
    "    common_prompt = re.sub(pattern, \"\", common_prompt, flags=re.DOTALL)\n",
    "    return common_prompt\n",
    "\n",
    "def run_inference(checkpoint_number, train_type):\n",
    "    # ---\n",
    "    dataset = []\n",
    "    for scenario_dir in [d for d in base_dataset_dir.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]:\n",
    "        data = read_dataset(scenario_dir, \"onlyq_ts.json\", train_type)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = scenario_dir.name\n",
    "        dataset.extend(data)\n",
    "\n",
    "        data = read_dataset(scenario_dir, \"onlyq_tr.json\", train_type)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = scenario_dir.name\n",
    "        dataset.extend(data)\n",
    "\n",
    "    # ---\n",
    "    common_prompt = open(base_dataset_dir / f\"prompt.txt\", \"r\").read()\n",
    "    \n",
    "    sub_targets = []\n",
    "    if train_type == \"ours\":\n",
    "        sub_targets = []\n",
    "    elif train_type == \"BASE\":\n",
    "        sub_targets = [\"Thinking\", \"Expectation\", \"Mapping\", \"Script\", \"Examples\"]\n",
    "    elif train_type in [\"WoThinking\"]:\n",
    "        sub_targets = [\"Thinking\"]\n",
    "    elif train_type in [\"woMetadata\"]:\n",
    "        sub_targets = [\"Metadata\"]\n",
    "    elif train_type in [\"WoMetadata+Thinking\"]:\n",
    "        sub_targets = [\"Metadata\", \"Thinking\"]\n",
    "    elif train_type in [\"woExp\"]:\n",
    "        sub_targets = [\"Expectation\"]\n",
    "\n",
    "    if train_type in [\"woQM\", \"woQM+Script\"]:\n",
    "        sub_targets = [\"QM\", \"Mapping\"]\n",
    "    if train_type in [\"woScript\", \"woQM+Script\"]:\n",
    "        sub_targets = [\"Script\"]\n",
    "\n",
    "    for sub_target in sub_targets:\n",
    "        common_prompt = sub(sub_target, common_prompt)\n",
    "\n",
    "    # remove all <||>\n",
    "    common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "\n",
    "    if train_type in [\"0SL\",\"5SL\", \"ALLSL\"]:\n",
    "        # n-shot prompting with trainset\n",
    "        \n",
    "        datas = []\n",
    "        for directory in base_dataset_dir.iterdir():\n",
    "            if directory.is_dir():\n",
    "                # Note: metadata ì•ˆë„£ìŒ\n",
    "                data = read_json(f\"{directory}/onlyq_tr.json\")\n",
    "                for d in data:\n",
    "                    del d[\"Tags\"]\n",
    "                datas.extend(data)\n",
    "\n",
    "                data = read_json(f\"{directory}/onlyq_ts.json\")\n",
    "                for d in data:\n",
    "                    del d[\"Tags\"]\n",
    "                datas.extend(data)\n",
    "        if \"ALL\" not in train_type:\n",
    "            n = int(train_type.split(\"SL\")[0])\n",
    "            datas = datas[:n]\n",
    "\n",
    "        # n-shot prompting with testset\n",
    "        if train_type != \"0SL\": \n",
    "            data_str = \"\\n[ì˜ˆì‹œ]\\n\" + \"\\n\".join([f\"ìž…ë ¥: {d['Input']}\\nì¶œë ¥: {d['Response']}\" for d in datas])\n",
    "            common_prompt = common_prompt + data_str\n",
    "    print(common_prompt)\n",
    "    # ---\n",
    "    model_name = \"sh2orc-Llama-3.1-Korean-8B-Instruct\"\n",
    "    model_dir = Path(f\"../model/{model_name}\")\n",
    "    cache_dir = Path(f\"{model_dir}/cache\")\n",
    "    \n",
    "    checkpoint_dir = None\n",
    "    if checkpoint_number == 0:\n",
    "        checkpoint_dir = \"sh2orc/Llama-3.1-Korean-8B-Instruct\"\n",
    "        output_file = f\"../experiments/r-v7_not_trained_{train_type}_tr27_0629.json\"\n",
    "        max_seq_length = 3000\n",
    "    else:\n",
    "        r = 211\n",
    "        tr_dir = f\"v7_r{r}_a{2*r}_{train_type}_tr27_0629\"\n",
    "\n",
    "        checkpoint_dir = Path(f\"{model_dir}/chkpts/{tr_dir}\")\n",
    "        print(checkpoint_dir)\n",
    "\n",
    "        checkpoint_dir = checkpoint_dir.iterdir()\n",
    "        checkpoint_dir = [c for c in checkpoint_dir if c.is_dir()]\n",
    "        checkpoint_dir = sorted(checkpoint_dir, key=lambda x: int(x.name.split(\"-\")[-1]))[-1]\n",
    "        # tr_config = f\"{tr_dir}/{checkpoint_dir.name}\"\n",
    "        tr_config = f\"{tr_dir}/checkpoint-{checkpoint_number}\"\n",
    "        print(tr_config)\n",
    "        checkpoint_dir = Path(f\"{model_dir}/chkpts/{tr_config}\")\n",
    "        \n",
    "        print(f\"Model: {model_name}, Config: {tr_config}\")\n",
    "\n",
    "    \n",
    "        # Verify paths exist\n",
    "        if not checkpoint_dir.exists():\n",
    "            raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "\n",
    "        output_file = f\"../experiments/revision/r-{tr_config.replace('/', '-').replace('checkpoint', 'step')}.json\"\n",
    "        max_seq_length = 10000\n",
    "    \n",
    "\n",
    "    batch_size = 22\n",
    "    inference = UnslothInference(\n",
    "        checkpoint_dir=str(checkpoint_dir),\n",
    "        cache_dir=str(cache_dir),\n",
    "        batch_size=batch_size,\n",
    "        max_seq_length=max_seq_length\n",
    "    )\n",
    "\n",
    "    open(output_file, \"w\").close()  # Clea\n",
    "\n",
    "    inference.run(\n",
    "        dataset=dataset,\n",
    "        common_prompt=common_prompt,\n",
    "        output_file=output_file\n",
    "    )\n",
    "\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"[\\n\")\n",
    "        f.write(\",\\n\".join(lines))\n",
    "        f.write(\"\\n]\\n\")\n",
    "    \n",
    "    print(f\"Saved to {output_file}\")\n",
    "\n",
    "    del inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for WoThinking with checkpoint 31\n",
      "ë„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \n",
      "ì‚¬ìš©ìžì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„ Agentì˜ Instructionsë¥¼ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\n",
      "Expectationì—ì„œëŠ” ìœ ì €ê°€ ê¸°ëŒ€í• ë§Œí•œ ë‹µë³€ì„ ì¶”ì¸¡í•´ì•¼í•¨.\n",
      "Mappingì—ì„œëŠ” ì§ˆë¬¸ì— ì‚¬ìš©ëœ high-level taxonomyë¥¼ metadataë¥¼ ë°”íƒ•ìœ¼ë¡œ low-level taxonomyë¡œì˜ mappingì„ ê³„ì‚°í•˜ì—¬ì•¼ í•¨.\n",
      "ì´ë•Œ metadataì— ì—†ëŠ” ì •ë³´ë¥¼ ê¸°ìˆ í•˜ëŠ” ë“± ê±°ì§“ëœ ì¶œë ¥ì„ í•˜ë©´ ì•ˆë˜ê³ , ëª¨ë¥´ëŠ” ì •ë³´ê°€ ìžˆìœ¼ë©´ Unknownì´ë¼ ë‹µë³€í•´ì•¼í•¨.\n",
      "Scriptì—ì„œëŠ” data í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ pandas dataframe í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•˜ì—¬ ë‹µë³€ì— í•„ìš”í•œ ì—°ì‚°ì„ python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§œ ìˆ˜í–‰í•œë‹¤. ì´ë•Œ ì‹¤í–‰ ì—ëŸ¬ì— ì¡°ì‹¬í•œë‹¤. python\n",
      "Expectationì˜ ëª…ì‹œëœ ëª¨ë“  variableì´ scriptì—ì„œ ê³„ì‚°ë˜ì•¼ í•œë‹¤.\n",
      "jsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«íž˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.\n",
      "../model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r211_a422_WoThinking_tr27_0629\n",
      "v7_r211_a422_WoThinking_tr27_0629/checkpoint-31\n",
      "Model: sh2orc-Llama-3.1-Korean-8B-Instruct, Config: v7_r211_a422_WoThinking_tr27_0629/checkpoint-31\n",
      "==((====))==  Unsloth 2025.10.7: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA H100 NVL. Num GPUs = 1. Max memory: 93.016 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88b6057313d4afc96a16ee61628b9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../model/sh2orc-Llama-3.1-Korean-8B-Instruct/chkpts/v7_r211_a422_WoThinking_tr27_0629/checkpoint-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:11<00:22,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.99s, Input: ì–´ì œ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [00:33<00:16,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 21.67s, Input: ìž‘ë…„ ì˜†ë°˜ ê°€ìž¥ ë”ì› ë˜ ë‹¬ì€?\n",
      "Error in eval: unmatched '}' (<string>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:51<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 17.33s, Input: í™”ì„±ì˜ ì„¤ì •ì˜¨ë„ í™•ì¸í•´ì¤˜\n",
      "{'Input': '3ì£¼ì „ ì˜¨ë„ê°€ 22â„ƒ ì´ìƒì´ì—ˆë˜ ë‚ ì§œ ì•Œë ¤ì¤˜', 'Scenario': 'scenario3', 'Metadata': {'idu_mapping': {'01_IB5': ['ì•žë°˜', '4ì¸µ'], '01_IB7': ['ìš°ë¦¬ë°˜', '4ì¸µ'], '02_I81': ['ì˜†ë°˜', '4ì¸µ']}, 'modality_mapping': {'roomtemp': ['ì‹¤ë‚´ì˜¨ë„'], 'settemp': ['ì„¤ì •ì˜¨ë„']}, 'current_datetime': '2022-09-28 22:30:00'}, 'Candidate': {'Expectations': ['3ì£¼ì „ ì˜¨ë„ê°€ 22â„ƒ ì´ìƒì´ì—ˆë˜ ë‚ ì€ {{3ì£¼ì „_ì˜¨ë„22â„ƒì´ìƒ_ë‚ ì§œ}}ì˜€ìŠµë‹ˆë‹¤.'], 'Mapping': {'temporal': {'3ì£¼ì „': \"[DATE_TRUNC('week', DATE 'CURRENT_DATE' - INTERVAL '3 week'), DATE_TRUNC('week', DATE 'CURRENT_DATE' - INTERVAL '2 week'))\"}, 'spatials': {'Unknown'}, 'modalities': {'roomtemp'}}}}\n",
      "Saved to ../experiments/revision/r-v7_r211_a422_WoThinking_tr27_0629-step-31.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # ResponseGeneration.update_prompt()\n",
    "# # ì—¬ëŸ¬ ëª¨ë¸ì— ëŒ€í•´ ì¶”ë¡  ì‹¤í–‰ (Run inference for multiple models)\n",
    "model_configs = [\n",
    "    # (30, \"ours\"),\n",
    "    # (30, \"woExp\"),\n",
    "    # (30, \"WoMetadata+Thinking\"),\n",
    "    (31, \"WoThinking\"),\n",
    "    # (40, \"woQM\"),\n",
    "\n",
    "    # (30, \"woQM+Script\"),\n",
    "    # (30, \"woScript\")\n",
    "]\n",
    "\n",
    "for checkpoint_num, config_name in model_configs:\n",
    "    print(f\"Running inference for {config_name} with checkpoint {checkpoint_num}\")\n",
    "    run_inference(checkpoint_num, config_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any  # Any íƒ€ìž… import í•„ìš”\n",
    "\n",
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"QueryTruePositive\"\n",
    "    false_positive = \"QueryFalsePositive\"\n",
    "    false_negative = \"QueryFalseNegative\"\n",
    "    \n",
    "def eval_query(cand_response_filename, db_gt_filename=\"./gts.json\"):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "    response_reports = []\n",
    "    time_reports = []\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            # pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            # if \"ì˜†ë°˜ ìŠµë„ ì•Œë ¤ì¤˜\" not in input:\n",
    "            #     continue\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # ê´€ê³„ ì—†ëŠ” ì§ˆë¬¸ë“¤ì€ ê±´ë„ˆë›°ìž\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                print(f\"No ground truth found for {input}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            gt_report = gt_report[0]\n",
    "            tags = gt_report[\"Tags\"]\n",
    "            # assert gt_report[\"QueryResults\"] != []\n",
    "            # if gt_report[\"Result\"] == []:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            \n",
    "            \n",
    "            gt_results = [d for d in gt_report[\"QueryResults\"]]\n",
    "            gt_query_results = defaultdict(list)\n",
    "            for gt_result in gt_results:\n",
    "                for col in gt_result[\"result_columns\"]:\n",
    "                    gt_query_results[col].extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            gt_response = gt_report[\"Response\"]\n",
    "            # gt_required_variables = gt_report[\"RequiredVariables\"]\n",
    "            # gt_variables_to_report = gt_report[\"VariablesToReport\"]\n",
    "            user_input = gt_report[\"Input\"]\n",
    "            # print(user_input)\n",
    "            exp_tag = cand_response_filename.split(\"/\")[-1].split(\"_\")[3]\n",
    "\n",
    "            response_report = {\n",
    "                \"Input\": user_input,\n",
    "                \"Metadata\": metadata,\n",
    "                \"GT_Response\": gt_response,\n",
    "                # \"GT_RequiredVariables\": gt_required_variables,\n",
    "                # \"GT_VariablesToReport\": gt_variables_to_report,\n",
    "            }\n",
    "            # evaluation_report ë”•ì…”ë„ˆë¦¬ ìƒì„± (defaultdict ì‚¬ìš©, ê¸°ë³¸ê°’ None)\n",
    "\n",
    "            evaluation_report: dict[str, Any] = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Metadata\"] = metadata\n",
    "            evaluation_report[\"Tags\"] = tags\n",
    "\n",
    "            \n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict):\n",
    "                requirements = [\"Thinking\", \"Expectations\", \"Mapping\"]\n",
    "                if exp_tag in [\"WoThinking\", \"WoMetadata+Thinking\"]:\n",
    "                    requirements.remove(\"Thinking\")\n",
    "                elif exp_tag in [\"woExp\"]:\n",
    "                    requirements.remove(\"Expectations\")\n",
    "                elif exp_tag in [\"woQM\", \"woQM+Script\"]:\n",
    "                    requirements.remove(\"Mapping\")\n",
    "                for requirement in requirements:\n",
    "                    if requirement not in cand_response[\"Candidate\"]:\n",
    "                        evaluation_report[EM.json_structure] = False\n",
    "                        break\n",
    "                else:\n",
    "                    evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "            \n",
    "            if not evaluation_report[EM.json_structure]:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "\n",
    "                print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "            \n",
    "            start_time = time.time()\n",
    "            # expertLLM_output_token_length = measure_token_count(cand_response[\"Candidate\"])\n",
    "            # print(\"Input,\", cand_response[\"Input\"], \",expertLLM_output_tlen,\",  expertLLM_output_token_length)\n",
    "\n",
    "            if exp_tag in [\"woExp\"]:\n",
    "                cand_response[\"Candidate\"][\"Expectations\"] = []\n",
    "            if exp_tag in [\"woQM\", \"woQM+Script\"]:\n",
    "                pass\n",
    "            # exp_tag = \\\n",
    "            #     \"woCoTExp\" if \"woCoTExp\" in str(cand_response_filename) else \\\n",
    "            #     \"woOp\" if \"woOp\" in str(cand_response_filename) else \\\n",
    "            #     \"woQM\" if \"woQM\" in str(cand_response_filename) else \\\n",
    "            #     None\n",
    "            try:\n",
    "                mapping, expectations, required_variables, script = InputToInstruction.postprocess_v2(\n",
    "                    deepcopy(cand_response[\"Candidate\"]), \n",
    "                    exp_tag=exp_tag\n",
    "                )\n",
    "            except:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            \n",
    "            response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, mapping, expectations, required_variables, script, exp_tag=exp_tag)\n",
    "            # print(response)\n",
    "            response_report[\"PD_Response\"] = response\n",
    "            # try:\n",
    "            #     # response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, instructions, exp_tag=exp_tag)\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Error: {e}\")\n",
    "            #     # evaluation_report[EM.true_positive] = 0\n",
    "            #     # evaluation_report[EM.false_positive] = 0\n",
    "            #     # evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "            #     # evaluation_reports.append(evaluation_report)\n",
    "\n",
    "            #     # response_reports.append(response_report)\n",
    "                            \n",
    "            #     # pbar.update(1)\n",
    "            #     # continue\n",
    "            time_reports.append(time.time() - start_time)\n",
    "            response_reports.append(response_report)\n",
    "            \n",
    "            # required_variables = summarize_variables_to_report(required_variables)\n",
    "            # print(required_variables)\n",
    "            # required_variables = ResponseGeneration.stringify_variables(required_variables)\n",
    "            \n",
    "            # response_report[\"PD_RequiredVariables\"] = required_variables\n",
    "            # response_report[\"PD_VariablesToReport\"] = variables_to_report\n",
    "\n",
    "            if len(_cand_query_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            cand_query_results = defaultdict(list)\n",
    "            for cand_query_result in _cand_query_results:\n",
    "                for col in cand_query_result[\"result_columns\"]:\n",
    "                    cand_query_results[col].extend(cand_query_result[\"result_indices\"])\n",
    "\n",
    "            cand_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            if len(gt_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = cand_total_combinations\n",
    "                evaluation_report[EM.false_negative] = 0\n",
    "\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "\n",
    "                continue\n",
    "            \n",
    "            # print(gt_total_combinations, cand_total_combinations)\n",
    "            # True Positive: ê³µí†µëœ ì»¬ëŸ¼ê³¼ ë¡œìš°ì˜ ëª¨ë“  ì¡°í•©\n",
    "            true_positive = 0\n",
    "            false_negative = 0\n",
    "            false_positive = 0\n",
    "            for col in set(gt_query_results.keys())&set(cand_query_results.keys()):\n",
    "                s_gt_query_result = set(gt_query_results[col])\n",
    "                s_cand_query_result = set(cand_query_results[col])\n",
    "                true_positive += len(s_gt_query_result & s_cand_query_result)\n",
    "                false_negative += len(s_gt_query_result - s_cand_query_result)\n",
    "                false_positive += len(s_cand_query_result - s_gt_query_result)\n",
    "\n",
    "                # print(true_positive, false_negative, false_positive, len(s_gt_query_result), len(s_cand_query_result))\n",
    "            # assert true_positive + false_positive + false_negative == gt_total_combinations\n",
    "            \n",
    "\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    with open(f\"{cand_response_filename.replace('.json', '_response.json')}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(response_reports, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # print(f\"Time: {time_reports}, {sum(time_reports) / len(time_reports)}\")\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_print = eval_df.drop(columns=[\"Metadata\", \"Tags\"])\n",
    "    print(f\"Results for {cand_response_filename}:\")\n",
    "    # print(eval_print)\n",
    "    eval_df[EM.true_positive] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[EM.false_positive] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[EM.false_negative] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # replace nan with 0\n",
    "    # eval_df.fillna(0, inplace=True)\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    # print(truepos_sum, falsepos_sum, falseneg_sum)\n",
    "    # print(precision, recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    # final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "    final_result[\"Precision\"] = precision\n",
    "\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tag(cand_response_filename, db_gt_filename=\"./gts.json\"):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "    response_reports = []\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            # pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            # if \"ì˜†ë°˜ ìŠµë„ ì•Œë ¤ì¤˜\" not in input:\n",
    "            #     continue\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # ê´€ê³„ ì—†ëŠ” ì§ˆë¬¸ë“¤ì€ ê±´ë„ˆë›°ìž\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                print(f\"No ground truth found for {input}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            gt_report = gt_report[0]\n",
    "            tags = gt_report[\"Tags\"]\n",
    "            # assert gt_report[\"QueryResults\"] != []\n",
    "            # if gt_report[\"Result\"] == []:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            \n",
    "            \n",
    "            gt_results = [d for d in gt_report[\"QueryResults\"]]\n",
    "            gt_query_results = defaultdict(list)\n",
    "            for gt_result in gt_results:\n",
    "                for col in gt_result[\"result_columns\"]:\n",
    "                    gt_query_results[col].extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            gt_response = gt_report[\"Response\"]\n",
    "            # gt_required_variables = gt_report[\"RequiredVariables\"]\n",
    "            # gt_variables_to_report = gt_report[\"VariablesToReport\"]\n",
    "            user_input = gt_report[\"Input\"]\n",
    "            # print(user_input)\n",
    "            exp_tag = cand_response_filename.split(\"_\")[3]\n",
    "\n",
    "            response_report = {\n",
    "                \"Input\": user_input,\n",
    "                \"Metadata\": metadata,\n",
    "                \"GT_Response\": gt_response,\n",
    "                # \"GT_RequiredVariables\": gt_required_variables,\n",
    "                # \"GT_VariablesToReport\": gt_variables_to_report,\n",
    "            }\n",
    "            # evaluation_report ë”•ì…”ë„ˆë¦¬ ìƒì„± (defaultdict ì‚¬ìš©, ê¸°ë³¸ê°’ None)\n",
    "\n",
    "            evaluation_report: dict[str, Any] = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Metadata\"] = metadata\n",
    "            evaluation_report[\"Tags\"] = tags\n",
    "\n",
    "            print(cand_response)\n",
    "            evaluation_report[EM.json_structure] = True\n",
    "\n",
    "            sql = cand_response[\"Candidate\"]\n",
    "            if sql == \"\":\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "\n",
    "            # \"SELECT\"ë¼ëŠ” ì²« ë²ˆì§¸ ë“±ìž¥ë§Œ \"SELECT id \"ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "            # sql = sql.replace(\"SELECT\", \"SELECT d.id, \", 1)\n",
    "            try:\n",
    "                df = DBManager.execute_structured_query_string(sql)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                response_report[\"PD_Response\"] = \"ì¿¼ë¦¬ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤\"\n",
    "                pbar.update(1)\n",
    "                response_reports.append(response_report)\n",
    "                continue\n",
    "            # cols = list(df.columns)\n",
    "            # cols.remove(\"id\")\n",
    "            # cols.remove(\"idu_name\")\n",
    "            # cols.remove(\"timestamp\")\n",
    "            # rows = list(df[\"id\"])\n",
    "            # query_results = [{\n",
    "            #     \"result_columns\": cols,\n",
    "            #     \"result_indices\": rows,\n",
    "            # }]\n",
    "            # df = df.drop(columns=['id'])\n",
    "            print(df.shape)\n",
    "            if df.shape[0] < 10000:\n",
    "                response = ResponseGeneration.execute_raw(\n",
    "                    f\"\"\"\n",
    "                    Input: {user_input}\n",
    "                    Metadata: {metadata}\n",
    "                    Data: {df.to_json(orient=\"records\")}\n",
    "                    \"\"\",\n",
    "                    prompt = \"\"\"\n",
    "                    ì§ˆë¬¸ì„ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\n",
    "                    \"\"\"\n",
    "                )\n",
    "                response = extract_content(response)\n",
    "            else:\n",
    "                response = \"ë°ì´í„°ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤.\"\n",
    "            response_report[\"PD_Response\"] = response\n",
    "            # try:\n",
    "            #     # response, variables_to_report, required_variables, _cand_query_results = run_query_v2(user_input, metadata, instructions, exp_tag=exp_tag)\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Error: {e}\")\n",
    "            #     # evaluation_report[EM.true_positive] = 0\n",
    "            #     # evaluation_report[EM.false_positive] = 0\n",
    "            #     # evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                            \n",
    "            #     # evaluation_reports.append(evaluation_report)\n",
    "\n",
    "            #     # response_reports.append(response_report)\n",
    "                            \n",
    "            #     # pbar.update(1)\n",
    "            #     # continue\n",
    "            \n",
    "            response_reports.append(response_report)\n",
    "            \n",
    "            # cand_query_results = defaultdict(list)\n",
    "            # for cand_query_result in query_results:\n",
    "            #     for col in cand_query_result[\"result_columns\"]:\n",
    "            #         cand_query_results[col].extend(cand_query_result[\"result_indices\"])\n",
    "\n",
    "            # cand_total_combinations = sum(len(v) for v in gt_query_results.values())\n",
    "\n",
    "            # if len(gt_results) == 0:\n",
    "            #     evaluation_report[EM.true_positive] = 0\n",
    "            #     evaluation_report[EM.false_positive] = cand_total_combinations\n",
    "            #     evaluation_report[EM.false_negative] = 0\n",
    "\n",
    "            #     evaluation_reports.append(evaluation_report)\n",
    "            #     pbar.update(1)\n",
    "\n",
    "            #     continue\n",
    "            \n",
    "            # # print(gt_total_combinations, cand_total_combinations)\n",
    "            # # True Positive: ê³µí†µëœ ì»¬ëŸ¼ê³¼ ë¡œìš°ì˜ ëª¨ë“  ì¡°í•©\n",
    "            # true_positive = 0\n",
    "            # false_negative = 0\n",
    "            # false_positive = 0\n",
    "            # for col in set(gt_query_results.keys())&set(cand_query_results.keys()):\n",
    "            #     s_gt_query_result = set(gt_query_results[col])\n",
    "            #     s_cand_query_result = set(cand_query_results[col])\n",
    "            #     true_positive += len(s_gt_query_result & s_cand_query_result)\n",
    "            #     false_negative += len(s_gt_query_result - s_cand_query_result)\n",
    "            #     false_positive += len(s_cand_query_result - s_gt_query_result)\n",
    "\n",
    "            #     # print(true_positive, false_negative, false_positive, len(s_gt_query_result), len(s_cand_query_result))\n",
    "            # # assert true_positive + false_positive + false_negative == gt_total_combinations\n",
    "            \n",
    "\n",
    "            # evaluation_report[EM.true_positive] = true_positive\n",
    "            # evaluation_report[EM.false_positive] = false_positive\n",
    "            # evaluation_report[EM.false_negative] = false_negative\n",
    "\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    with open(f\"{cand_response_filename.replace('.json', '_response.json')}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(response_reports, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # # print(eval_df)\n",
    "\n",
    "    # eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    # final_result = {}\n",
    "\n",
    "    # for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "    #     # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "    #     final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # # normalize per query\n",
    "    # eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    # eval_print = eval_df.drop(columns=[\"Metadata\", \"Tags\"])\n",
    "    # print(eval_print)\n",
    "    # eval_df[EM.true_positive] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    # eval_df[EM.false_positive] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    # eval_df[EM.false_negative] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # # replace nan with 0\n",
    "    # # eval_df.fillna(0, inplace=True)\n",
    "\n",
    "    # # # F1 score except nans.\n",
    "    # truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    # precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    # recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    # print(truepos_sum, falsepos_sum, falseneg_sum)\n",
    "    # print(precision, recall)\n",
    "    # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # # print(f\"F1: {f1}\")\n",
    "    # final_result[\"F1\"] = f1\n",
    "    # final_result[\"Recall\"] = recall\n",
    "\n",
    "    # for col in final_result:\n",
    "    #     print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    # return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_tag(\n",
    "#     cand_response_filename=\"../experiments/r-v7_r211_a422_TAG_tr27_revision-step-0.json\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_query(\n",
    "#     f\"../experiments/result_29/r-v7_r211_a422_WoThinking_tr27_0629-step-54.json\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [01:03<00:35,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in executing script: v_í˜„ìž¬_ìš°ë¦¬ë°˜_df = data_(metadata, mapping, query_results, t='í˜„ìž¬',s='ìš°ë¦¬ë°˜')\n",
      "'types.UnionType' object is not iterable\n",
      "Error in running query_v2: 'types.UnionType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 50/64 [01:08<00:11,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in executing script: v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ê³µê°„({{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ê³µê°„}})ì´ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ê³µê°„åˆ¥}}ë³´ë‹¤ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ì°¨ì´}}â„ƒ ë” ë†’ìŠµë‹ˆë‹¤.\n",
      "invalid character 'â„ƒ' (U+2103) (<string>, line 1)\n",
      "Error in running query_v2: invalid character 'â„ƒ' (U+2103) (<string>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [01:24<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in executing script: v_3ì£¼ì „_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_22â„ƒì´ìƒ_ë‚ ì§œ = []\n",
      "invalid character 'â„ƒ' (U+2103) (<string>, line 1)\n",
      "Error in running query_v2: invalid character 'â„ƒ' (U+2103) (<string>, line 1)\n",
      "Results for ../experiments/revision/r-v7_r211_a422_ours_tr27_0629-step-45.json:\n",
      "JsonStructureCorrectness: 1.00\n",
      "ExactMatch: 0.84\n",
      "Recall: 0.85\n",
      "Precision: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# name = \"r-v7_r256_a512_ours_tr6_0503-checkpoint-63\"\n",
    "# name = \"r-v7_r256_a512_ours_tr18_0503-checkpoint-52\"\n",
    "# name = \"r-v7_r256_a512_ours_tr30_0503-checkpoint-54\"\n",
    "# name = \"r-v7_r256_a512_ours_tr45_0503-checkpoint-95\"\n",
    "# name = \"r-v7_r256_a512_ours_tr60_0503-checkpoint-108\"\n",
    "\n",
    "# name = \"r-v7_r256_a512_woall_tr6_0503-checkpoint-28\"\n",
    "# name = \"r-v7_r256_a512_woall_tr18_0503-checkpoint-70\"\n",
    "# name = \"r-v7_r256_a512_woall_tr30_0503-checkpoint-57\"\n",
    "# name = \"r-v7_r256_a512_woall_tr45_0503-checkpoint-95\"\n",
    "# name = \"r-v7_r256_a512_woall_tr60_0503-checkpoint-90\"\n",
    "\n",
    "names = [\n",
    "\"r-v7_r211_a422_ours_tr27_0629-step-45\",\n",
    "# \"r-v7_r211_a422_woExp_tr27_0629-step-30\",\n",
    "# \"r-v7_r211_a422_WoMetadata+Thinking_tr27_0629-step-30\",\n",
    "# \"r-v7_r211_a422_WoThinking_tr27_0629-step-31\",\n",
    "# \"r-v7_r211_a422_woQM_tr27_0629-step-30\",\n",
    "# \"r-v7_r211_a422_woQM+Script_tr27_0629-step-30\",\n",
    "# \"r-v7_r211_a422_woScript_tr27_0629-step-30\"\n",
    "]\n",
    "\n",
    "for name in names:\n",
    "    eval_query(\n",
    "        f\"../experiments/revision/{name}.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WoThinking</th>\n",
       "      <th>woQM</th>\n",
       "      <th>woScript</th>\n",
       "      <th>woQM+Script</th>\n",
       "      <th>ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExactMatch</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            WoThinking  woQM  woScript  woQM+Script  ours\n",
       "ExactMatch        0.88  0.69      0.81         0.73  0.84\n",
       "Recall            0.86  0.82      0.80         0.86  0.85\n",
       "Precision         0.99  0.81      0.98         0.87  0.96"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from reivision_qresult.txt\n",
    "\n",
    "\n",
    "# parse something like using regex\n",
    "# Results for ../experiments/revision/r-v7_r211_a422_WoMetadata+Thinking_tr27_0629-step-30.json:\n",
    "# JsonStructureCorrectness: 0.98\n",
    "# ExactMatch: 0.78\n",
    "# Recall: 0.81\n",
    "# Precision: 0.90\n",
    "\n",
    "# parse\n",
    "import re\n",
    "pattern = r\"Results for (.+):\\nJsonStructureCorrectness: ([0-9.]+)\\nExactMatch: ([0-9.]+)\\nRecall: ([0-9.]+)\\nPrecision: ([0-9.]+)\"\n",
    "data = {}\n",
    "with open(\"./revision_qresult.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    matches = re.findall(pattern, content)\n",
    "    for match in matches:\n",
    "        filename, json_structure, exact_match, recall, precision = match\n",
    "        # print(f\"Filename: {filename}\")\n",
    "        # parse exp_tag from filename\n",
    "        exp_tag = filename.split(\"_\")[3]\n",
    "        # print(f\"ExpTag: {exp_tag}\")\n",
    "        # print(f\"JsonStructureCorrectness: {json_structure}\")\n",
    "        # print(f\"ExactMatch: {exact_match}\")\n",
    "        # print(f\"Recall: {recall}\")\n",
    "        # print(f\"Precision: {precision}\")\n",
    "        # print()\n",
    "        data[exp_tag] = {\n",
    "            # \"JsonStructureCorrectness\": float(json_structure),\n",
    "            \"ExactMatch\": float(exact_match),\n",
    "            \"Recall\": float(recall),\n",
    "            \"Precision\": float(precision),\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[[\"WoThinking\", \"woQM\", \"woScript\", \"woQM+Script\", \"ours\"]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë‹µë³€ merge for laj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-v7_r211_a422_ours_tr27_0629-step-45_response.json ours\n",
      "r-v7_r211_a422_ours_tr27_0629-step-45_response.json\n",
      "r-v7_r211_a422_woExp_tr27_0629-step-30_response.json woExp\n",
      "r-v7_r211_a422_woExp_tr27_0629-step-30_response.json\n",
      "r-v7_r211_a422_WoMetadata+Thinking_tr27_0629-step-30_response.json WoMetadata+Thinking\n",
      "r-v7_r211_a422_WoMetadata+Thinking_tr27_0629-step-30_response.json\n",
      "r-v7_r211_a422_woQM_tr27_0629-step-30_response.json woQM\n",
      "r-v7_r211_a422_woQM_tr27_0629-step-30_response.json\n",
      "r-v7_r211_a422_woQM+Script_tr27_0629-step-30_response.json woQM+Script\n",
      "r-v7_r211_a422_woQM+Script_tr27_0629-step-30_response.json\n",
      "r-v7_r211_a422_woScript_tr27_0629-step-30_response.json woScript\n",
      "r-v7_r211_a422_woScript_tr27_0629-step-30_response.json\n",
      "r-v7_r211_a422_WoThinking_tr27_0629-step-31_response.json WoThinking\n",
      "r-v7_r211_a422_WoThinking_tr27_0629-step-31_response.json\n",
      "{'ì–´ì œ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM', 'woQM+Script')\": 'ì–´ì œ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ëŠ” ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ë³´ë‹¤ 0.00Â°C ë†’ìŠµë‹ˆë‹¤.', 'woExp': 'ì–´ì œ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ ì°¨ì´ëŠ” 0.00Â°Cìž…ë‹ˆë‹¤.', 'woScript': 'ì–´ì œ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ëŠ” ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤. (0.00Â°C)', 'GT_Response': 'ì–´ì œ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ëŠ” ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ë³´ë‹¤ 0.00Â°C ë†’ìŠµë‹ˆë‹¤.'}, 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì˜¨ë„ì°¨ì´ ì•Œë ¤ì¤˜': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM')\": 'ìš°ë¦¬ë°˜(27.45Â°C)ì´ ì˜†ë°˜(26.65Â°C)ë³´ë‹¤ 0.80Â°C ë†’ìŠµë‹ˆë‹¤.', 'woExp': 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ í‰ê·  ì˜¨ë„(27.45Â°C)ì™€ ì˜†ë°˜ í‰ê·  ì˜¨ë„(26.65Â°C)ì˜ ì°¨ì´ëŠ” 0.80Â°Cìž…ë‹ˆë‹¤.', \"('woQM+Script', 'woScript')\": 'ìš°ë¦¬ë°˜(28.50Â°C)ì´ ì˜†ë°˜(27.00Â°C)ë³´ë‹¤ 1.50Â°C ë†’ìŠµë‹ˆë‹¤.', 'GT_Response': 'ìš°ë¦¬ë°˜(27.45Â°C)ì´ ì˜†ë°˜(26.65Â°C)ë³´ë‹¤ 0.80Â°C ë†’ìŠµë‹ˆë‹¤.'}, 'ìž‘ë…„ ê²¨ìš¸ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours')\": 'ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woExp': 'ìž‘ë…„ ê²¨ìš¸ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ í‰ê·  ì„¤ì •ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woQM': 'ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” -1.00Â°C ìž…ë‹ˆë‹¤.', 'woQM+Script': 'ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” nanÂ°C ìž…ë‹ˆë‹¤. \\n\\n(ì°¸ê³ : ì œê³µëœ ë°ì´í„°ì—ì„œ í‰ê·  ì˜¨ë„ ê°’ì´ ê³„ì‚°ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ëˆ„ë½ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.)', 'woScript': 'ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': 'ìž‘ë…„ ê²¨ìš¸(2021-12 ~ 2022-02) ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, 'ì˜¬í•´ ì—¬ë¦„ ì•žë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woQM')\": 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” 26.11Â°C ìž…ë‹ˆë‹¤.', 'woExp': 'ì˜¬í•´ ì—¬ë¦„ ì•žë°˜ í‰ê· ì˜¨ë„ëŠ” 26.11Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 26.11Â°C ìž…ë‹ˆë‹¤.', \"('woQM+Script', 'woScript')\": 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” 23.00Â°C ìž…ë‹ˆë‹¤.', 'GT_Response': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” 26.11Â°C ìž…ë‹ˆë‹¤.'}, 'ì˜¬í•´ ë´„ ì˜†ë°˜ ì œì¼ ì¶”ì› ë˜ ë‚  ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours')\": 'ì˜¬í•´ ë´„ ì˜†ë°˜ ì‹¤ë‚´ ìµœì € ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woExp': 'ì˜¬í•´ ë´„ ì˜†ë°˜ì—ì„œ ê°€ìž¥ ì¶”ì› ë˜ ë‚ ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì˜¬í•´ ë´„ ì˜†ë°˜(01_IB7)ì˜ ìµœì € ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woQM': 'ì˜¬í•´ ë´„ ì˜†ë°˜ì€ 2022-03-01ì— -1.00Â°Cë¡œ ì œì¼ ì¶”ì› ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì˜¬í•´ ë´„(3ì›” ~ 5ì›”) ì˜†ë°˜ì€ 2022-05-31 00:00:00ì— -1.00Â°Cë¡œ ì œì¼ ì¶”ì› ìŠµë‹ˆë‹¤.', 'woScript': 'ì˜¬í•´ ë´„ ì˜†ë°˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜¬í•´ ë´„ ì˜†ë°˜ ì‹¤ë‚´ ìµœì € ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, '4ì›” ì•žë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woScript')\": '4ì›” ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woExp': '4ì›” ì•žë°˜ í‰ê· ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': '4ì›” ì•žë°˜ì˜ í‰ê·  ì„¤ì •ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woQM': '4ì›” ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” -1.00Â°C ìž…ë‹ˆë‹¤.', 'woQM+Script': '4ì›” ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” nanÂ°C ìž…ë‹ˆë‹¤. \\n\\n(ë°ì´í„°ì—ì„œ í‰ê·  ì˜¨ë„ ê°’ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)', 'GT_Response': '4ì›” ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, 'ì´ë²ˆë‹¬ ì¤‘ ìš°ë¦¬ë°˜ ì˜¨ë„ê°€ ê°€ìž¥ ëœ ë”ìš´ë‚ ì´ ì–¸ì œì•¼?': {\"('WoThinking', 'ours', 'woQM')\": 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì˜¨ë„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.', 'woExp': 'ì´ë²ˆë‹¬ ì¤‘ ìš°ë¦¬ë°˜ ì˜¨ë„ê°€ ê°€ìž¥ ëœ ë”ìš´ ë‚ ì€ 2022-09-07ê³¼ 2022-09-27ìž…ë‹ˆë‹¤. ìµœì†Œ ì˜¨ë„ëŠ” 22.50Â°Cì˜€ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-07ê³¼ 2022-09-27ì— ì‹¤ë‚´ì˜¨ë„ 22.50Â°Cë¡œ ê°€ìž¥ ëœ ë”ì› ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-01ì— 25.00Â°Cë¡œ ê°€ìž¥ ëœ ë”ì› ìŠµë‹ˆë‹¤. (ì´ë²ˆ ë°ì´í„°ì—ì„œ ê°€ìž¥ ë‚®ì€ ì˜¨ë„ê°€ 2022-09-01ì— ê¸°ë¡ë˜ì—ˆìœ¼ë©°, ì´í›„ ë‚ ì§œë“¤ì€ ëª¨ë‘ 28.50Â°Cë¡œ ë™ì¼í•©ë‹ˆë‹¤.) \\n\\në‹¨, ì œê³µëœ ë°ì´í„° ì˜ˆì‹œì—ì„œ íŠ¹ì • ë‚ ì§œë³„ ìµœì†Œ ì˜¨ë„ì˜ ë‹¤ì–‘ì„±ì„ ëª…í™•ížˆ ë³´ì—¬ì£¼ì§€ ëª»í•´, ê°€ìž¥ ëœ ë”ìš´ ë‚ ë¡œ 2022-09-01ì„ ì§€ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ì •í™•í•œ ë‹¤ì–‘í•œ ë‚ ì§œë³„ ìµœì†Œ ì˜¨ë„ ì •ë³´ê°€ í•„ìš”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ í¬ë§·ì— ì •í™•ížˆ ë§žì¶”ì–´ ë‹¨ì¼ ë‹µë³€ì„ ì œê³µí•˜ì˜€ìŠµë‹ˆë‹¤. \\n\\n**ìµœì¢… ë‹µë³€:**\\nì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-01ì— 25.00Â°Cë¡œ ê°€ìž¥ ëœ ë”ì› ìŠµë‹ˆë‹¤.', 'woScript': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-01ì— 25.00Â°Cë¡œ ê°€ìž¥ ëœ ë”ì› ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì˜¨ë„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.'}, '2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ë‚ ì´ ì–¸ì œì•¼?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM')\": '2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì€ 2022-09-12, 2022-09-13, 2022-09-14, 2022-09-15, 2022-09-16, 2022-09-17, 2022-09-18 ì¤‘ í•˜ë‚˜ë¡œ 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woExp': 'ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì€ 2022-09-12ë¶€í„° 2022-09-18ê¹Œì§€ ë§¤ì¼ 23.00Â°Cë¡œ ë™ì¼í•˜ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë‚ ì§œ: 2022-09-12, 2022-09-13, 2022-09-14, 2022-09-15, 2022-09-16, 2022-09-17, 2022-09-18.', 'woQM+Script': '2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚  ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.', 'woScript': \"2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì€ ë°ì´í„°ì—ì„œ ëª¨ë“  ì„¤ì •ì˜¨ë„ê°€ 23.0Â°Cë¡œ ì¼ì •í•˜ì—¬ íŠ¹ì • ë‚ ì§œë¥¼ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ë‚ ì§œì—ì„œ ì„¤ì •ì˜¨ë„ëŠ” {{v_2ì£¼ì „_ìš°ë¦¬ë°˜ê³¼ì˜†ë°˜_ì„¤ì •ì˜¨ë„_ìµœì†Œ}}23.00Â°Cì˜€ìŠµë‹ˆë‹¤. \\n\\ní•˜ì§€ë§Œ ì£¼ì–´ì§„ í¬ë§·ì— ë§žì¶”ì–´ ì¶œë ¥í•˜ìžë©´:\\n\\n['2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì€ ëª¨ë“  ë‚ ì§œ(2022-09-12 ~ 2022-09-18)ë¡œ 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.']\", 'GT_Response': '2ì£¼ì „ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ í•©ì³ì„œ ì„¤ì •ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì€ 2022-09-12, 2022-09-13, 2022-09-14, 2022-09-15, 2022-09-16, 2022-09-17, 2022-09-18 ì¤‘ í•˜ë‚˜ë¡œ 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì´ë²ˆë‹¬ ì¤‘ ë’·ë°˜ ì˜¨ë„ê°€ ê°€ìž¥ ë”ìš´ë‚ ì´ ì–¸ì œì•¼?': {\"('WoThinking', 'ours', 'woExp', 'woScript')\": \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ë’·ë°˜']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", \"('WoMetadata+Thinking', 'woQM', 'woQM+Script')\": 'ì´ë²ˆë‹¬ ë’·ë°˜ ì˜¨ë„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.', 'GT_Response': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ë’·ë°˜']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"}, 'ìš°ë¦¬ë°˜ì˜ í˜„ìž¬ ì„¤ì • ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM', 'woQM+Script', 'woScript')\": 'ìš°ë¦¬ë°˜ì˜ í˜„ìž¬ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.', 'woExp': 'í˜„ìž¬ ì„¤ì • ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.', 'GT_Response': 'ìš°ë¦¬ë°˜ì˜ í˜„ìž¬ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.'}, '8ì¼ì „ ì„¤ì •ì˜¨ë„ëŠ”?': {\"('WoThinking', 'ours', 'woQM', 'woQM+Script', 'woScript')\": '8ì¼ì „ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woExp': '8ì¼ì „ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': '8ì¼ì „ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'GT_Response': '8ì¼ì „ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.'}, '10ë…„ ì „ ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ ì˜¨ë„ëŠ”?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM', 'woQM+Script', 'woScript')\": '10ë…„ ì „ ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woExp': '10ë…„ ì „ ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': '10ë…„ ì „ ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, 'ë¡¯ë°ìºìŠ¬ì˜ í˜„ìž¬ ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woExp', 'woScript')\": \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ë¡¯ë°ìºìŠ¬']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", 'WoMetadata+Thinking': 'ë¡¯ë°ìºìŠ¬ì˜ í˜„ìž¬ ì‹¤ë‚´ì˜¨ë„ëŠ” 28.50Â°Cìž…ë‹ˆë‹¤.', \"('woQM', 'woQM+Script')\": 'ë¡¯ë°ìºìŠ¬ì˜ í˜„ìž¬ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ë¡¯ë°ìºìŠ¬']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"}, 'ì§€ê¸ˆ 4ì¸µ ìµœëŒ€ ì„¤ì •ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woQM+Script', 'woScript')\": 'ì§€ê¸ˆ 4ì¸µì˜ ìµœëŒ€ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.', 'woExp': '4ì¸µì˜ í˜„ìž¬ ìµœëŒ€ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': '4ì¸µ ìµœëŒ€ ì„¤ì •ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woQM': 'ì§€ê¸ˆ 4ì¸µì˜ ìµœëŒ€ ì„¤ì •ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì§€ê¸ˆ 4ì¸µì˜ ìµœëŒ€ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.'}, 'ìš°ë¦¬ë°˜, ì˜†ë°˜, ì•žë°˜ ì¤‘ ê°€ìž¥ ì¶”ìš´ ë°©ì€?': {\"('WoThinking', 'ours')\": 'í˜„ìž¬ ì„¸ ë°© ì¤‘ ì˜†ë°˜ê³¼ ì•žë°˜ ì¤‘ì—ì„œ ê°€ìž¥ ì¶”ìš´ ë°©ì„ ì •í™•ížˆ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì œê³µëœ ë°ì´í„°ì—ëŠ” ìš°ë¦¬ë°˜ì˜ ì˜¨ë„ ì •ë³´ê°€ ëˆ„ë½ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.', 'woExp': 'ê°€ìž¥ ì¶”ìš´ ë°©ì€ ì˜†ë°˜ê³¼ ì•žë°˜ìœ¼ë¡œ ë™ì¼í•˜ê²Œ ì¸¡ì •ë˜ì—ˆìœ¼ë©°, ìµœì € ì˜¨ë„ëŠ” 27.00Â°Cìž…ë‹ˆë‹¤. íŠ¹ì • ë°©ì„ ì§€ì •í•˜ìžë©´ ë°ì´í„°ì—ì„œ ì˜†ë°˜ê³¼ ì•žë°˜ì˜ ì˜¨ë„ê°€ ë™ì¼í•˜ê²Œ ê°€ìž¥ ë‚®ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. \\n\\nì¶œë ¥: ì˜†ë°˜ê³¼ ì•žë°˜ ëª¨ë‘ ìµœì € ì˜¨ë„ 27.00Â°Cë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. íŠ¹ì • ë°©ì„ ì§€ì •í•˜ìžë©´ ë°ì´í„°ì—ì„œ ì˜†ë°˜ê³¼ ì•žë°˜ì˜ ì˜¨ë„ê°€ ë™ì¼í•˜ê²Œ ê°€ìž¥ ë‚®ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'woQM': \"í˜„ìž¬ ì„¸ ë°© ì¤‘ 01_IB7, 02_I81 ì¤‘ ê°€ìž¥ ì¶”ìš´ ë°© ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (ë°ì´í„°ì—ì„œ êµ¬ì²´ì ì¸ ì˜¨ë„ ì°¨ì´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.) \\n\\ní•˜ì§€ë§Œ ì£¼ì–´ì§„ í¬ë§·ì— ë§žì¶”ì–´ ì œê³µ ê°€ëŠ¥í•œ ì •ë³´ë¡œ ë‹µë³€ ë“œë¦¬ìžë©´:\\n\\n['í˜„ìž¬ ì„¸ ë°© ì¤‘ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœì €_ë°©}}ì´(ê°€) {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœì €}}Â°Cë¡œ ê°€ìž¥ ì¶”ì›Œìš”.']\\n\\nì¶œë ¥: í˜„ìž¬ ì„¸ ë°© ì¤‘ 01_IB7, 02_I81 ì¤‘ ê°€ìž¥ ì¶”ìš´ ë°© ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì •í™•í•œ ë¹„êµë¥¼ ìœ„í•œ ì„¸ë¶€ ì˜¨ë„ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì£¼ì–´ì§„ ë°ì´í„°ë¡œëŠ” ì •í™•í•œ ë‹µë³€ ë¶ˆê°€) \\n\\nì •í™•í•œ í¬ë§·ì— ë”°ë¥¸ ë‹µë³€ ì œí•œìœ¼ë¡œ ì¸í•´, ì‹¤ì œ ì˜¨ë„ ì°¨ì´ ì •ë³´ ì—†ì´ëŠ” ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¡œëŠ” êµ¬ì²´ì ì¸ ë°©ì„ ì§€ì •í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\", 'woQM+Script': \"['ìš°ë¦¬ë°˜(01_IB5)ê³¼ ì˜†ë°˜(01_IB7), ì•žë°˜(02_I81) ì¤‘ì—ì„œ ê°€ìž¥ ì¶”ìš´ ë°©ì€ ì˜†ë°˜(01_IB7)ìœ¼ë¡œ {{v_í˜„ìž¬_ì‹¤ë‚´ì˜¨ë„_ìµœì €}}Â°Cìž…ë‹ˆë‹¤.']\\n\\nì¶œë ¥: ìš°ë¦¬ë°˜(28.50Â°C), ì˜†ë°˜(27.00Â°C), ì•žë°˜(27.00Â°C) ì¤‘ì—ì„œ ê°€ìž¥ ì¶”ìš´ ë°©ì€ ì˜†ë°˜(27.00Â°C)ìž…ë‹ˆë‹¤.\", 'woScript': 'ìš°ë¦¬ë°˜, ì˜†ë°˜, ì•žë°˜ ì¤‘ ê°€ìž¥ ì¶”ìš´ ë°©ì€ ì˜†ë°˜ìœ¼ë¡œ, í˜„ìž¬ ì˜¨ë„ëŠ” 27.00Â°Cìž…ë‹ˆë‹¤.', 'GT_Response': 'í˜„ìž¬ ì„¸ ë°© ì¤‘ ì˜†ë°˜ê³¼ ì•žë°˜ ì¤‘ì—ì„œ ê°€ìž¥ ì¶”ìš´ ë°©ì„ ì •í™•ížˆ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì œê³µëœ ë°ì´í„°ì—ëŠ” ìš°ë¦¬ë°˜ì˜ ì˜¨ë„ ì •ë³´ê°€ ëˆ„ë½ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.'}, 'ì§€ê¸ˆ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woExp', 'woScript')\": \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", 'WoMetadata+Thinking': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', \"('woQM', 'woQM+Script')\": 'í˜„ìž¬ ìš°ë¦¬ë°˜ì˜ ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ì—ë„ˆì§€ ì‚¬ìš©ëŸ‰']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"}, 'ì§€ë‚œë‹¬ ì˜¤ëŠ˜ ì˜¤í›„ 2ì‹œì— ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” ì–´ë• ì–´?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woExp', 'woQM', 'woQM+Script', 'woScript')\": 'ì§€ë‚œë‹¬ ì˜¤ëŠ˜ ì˜¤í›„ 2ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì§€ë‚œë‹¬ ì˜¤ëŠ˜ ì˜¤í›„ 2ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì˜†ë°˜ì˜ í˜„ìž¬ ì˜¨ë„ëž‘ ì„¤ì •ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woQM', 'woQM+Script', 'woScript')\": 'ì˜†ë°˜ì˜ í˜„ìž¬ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cì´ê³ , ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.', 'woExp': 'ì˜†ë°˜ì˜ í˜„ìž¬ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì´ê³ , ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜†ë°˜ì˜ í˜„ìž¬ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cì´ê³ , ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.'}, 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM')\": 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.67Â°C ìž…ë‹ˆë‹¤.', 'woExp': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì˜¨ë„ëŠ” 25.67Â°Cìž…ë‹ˆë‹¤.', 'woQM+Script': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.37Â°C ìž…ë‹ˆë‹¤.', 'woScript': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.21Â°C ìž…ë‹ˆë‹¤.', 'GT_Response': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.67Â°C ìž…ë‹ˆë‹¤.'}, 'ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì„ ì•Œë ¤ì¤˜.': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM')\": 'ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì€ 23.02Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woExp': 'ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì€ 23.02Â°Cìž…ë‹ˆë‹¤.', \"('woQM+Script', 'woScript')\": 'ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì€ 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ í‰ê· ì€ 23.02Â°Cì˜€ìŠµë‹ˆë‹¤.'}, '1ì¸µ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woExp', 'woScript')\": \"ì£„ì†¡í•©ë‹ˆë‹¤, ['1ì¸µ']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", 'WoMetadata+Thinking': '1ì¸µì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 28.50Â°C ìž…ë‹ˆë‹¤.', \"('woQM', 'woQM+Script')\": '1ì¸µì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['1ì¸µ']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"}, 'ìš°ë¦¬ë°˜ ì´ë²ˆë‹¬ ì œì¼ ì¶”ì› ë˜ ë‚ ì€ ì–¸ì œëƒ?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM')\": 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-07, 2022-09-27ì— ì‹¤ë‚´ì˜¨ë„ 22.50Â°Cë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.', 'woExp': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì œì¼ ì¶”ì› ë˜ ë‚ ì€ 2022-09-07ê³¼ 2022-09-27ìž…ë‹ˆë‹¤.', 'woQM+Script': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ì—ì„œ ìµœì†Œ ì˜¨ë„ê°€ 28.5Â°Cë¡œ ê¸°ë¡ëœ ë‚ ì§œê°€ 2022ë…„ 9ì›” 30ì¼ ì „ì²´ ì‹œê°„ëŒ€ì— ê±¸ì³ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì´ë²ˆë‹¬ ì œì¼ ì¶”ì› ë˜ ë‚ ì€ 2022-09-30ìž…ë‹ˆë‹¤. \\n\\nì¶œë ¥: ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-30ì— ì‹¤ë‚´ì˜¨ë„ 28.50Â°Cë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.', 'woScript': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ì—ì„œ ìµœì†Œ ì˜¨ë„ê°€ ê°€ìž¥ ë‚®ì€ ë‚ ì´ ì—¬ëŸ¬ ê°œ ì¡´ìž¬í•©ë‹ˆë‹¤. ìµœì†Œ ì˜¨ë„ëŠ” 25.00Â°Cë¡œ, í•´ë‹¹ ì˜¨ë„ê°€ ê¸°ë¡ëœ ë‚ ì§œëŠ” ì œê³µëœ ë°ì´í„°ì—ì„œ ëª…í™•ížˆ íŠ¹ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë°ì´í„° ì˜ˆì‹œì—ì„œëŠ” 2022ë…„ 9ì›” ë‚´ë‚´ 25.00Â°Cë¡œ ì¼ì •í•˜ê²Œ ê¸°ë¡ë˜ì–´ ìžˆì–´ ìµœì €ì¼ì„ íŠ¹ì • ë‚ ì§œë¡œ ì§€ì •í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤). ì œê³µëœ ì •ë³´ë¡œëŠ” ì •í™•í•œ ìµœì €ì¼ì„ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì€ 2022-09-07, 2022-09-27ì— ì‹¤ë‚´ì˜¨ë„ 22.50Â°Cë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.'}, 'ìž‘ë…„ ì˜†ë°˜ ê°€ìž¥ ë”ì› ë˜ ë‹¬ì€?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours')\": 'ì˜†ë°˜ ìž‘ë…„ ì‹¤ë‚´ì˜¨ë„ ìµœê³  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woExp': 'ìž‘ë…„ ì˜†ë°˜ ê°€ìž¥ ë”ì› ë˜ ë‹¬ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woQM': 'ìž‘ë…„ ì˜†ë°˜ì€ ê°€ìž¥ ë”ì› ë˜ ë‹¬ ì •ë³´ì— í•´ë‹¹í•˜ëŠ” ì˜¨ë„ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.', 'woQM+Script': \"ë°ì´í„°ì—ì„œ ìž‘ë…„ ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœê³  ë‹¬ê³¼ ì˜¨ë„ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ['v_ìž‘ë…„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë‹¬']ê³¼ ['v_ìž‘ë…„_ì˜†ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ì˜¨ë„']ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê°’ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\", 'woScript': 'ì˜†ë°˜ ìž‘ë…„ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜†ë°˜ ìž‘ë…„ ì‹¤ë‚´ì˜¨ë„ ìµœê³  ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, 'ì˜¤ëŠ˜ ì˜¤ì „ 11ì‹œì— ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” ì–´ë• ì–´?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woExp', 'woQM', 'woQM+Script', 'woScript')\": 'ì˜¤ëŠ˜ ì˜¤ì „ 11ì‹œ ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜¤ëŠ˜ ì˜¤ì „ 11ì‹œ ì˜†ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ì‹¤ë‚´ì˜¨ë„ í‰ê·  ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woQM')\": 'ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 28.05Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woExp': 'ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 28.05Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 28.05Â°Cì˜€ìŠµë‹ˆë‹¤.', \"('woQM+Script', 'woScript')\": 'ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 28.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œë¶€í„° 6ì‹œê¹Œì§€ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 28.05Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì§€ë‚œì£¼ì— ì„¤ì •ì˜¨ë„ì™€ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ê°€ ê°€ìž¥ ë§Žì´ ë‚¬ë˜ ë‚ ì€?': {\"('WoThinking', 'ours')\": 'ì§€ë‚œì£¼ 2022-09-23ì— ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(23.50Â°C) ì°¨ì´ê°€ 0.50Â°Cë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.', 'woExp': 'ì§€ë‚œì£¼ì— ì„¤ì •ì˜¨ë„ì™€ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ê°€ ê°€ìž¥ ë§Žì´ ë‚¬ë˜ ë‚ ì€ 2022-09-23ì´ë©°, ê·¸ ì°¨ì´ëŠ” -0.5Â°Cì˜€ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'woQM': 'ì§€ë‚œì£¼ 9ì›” 23ì¼, 24ì¼ì— ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(22.50Â°C) ì°¨ì´ê°€ 0.50Â°Cë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.', \"('woQM+Script', 'woScript')\": 'ì§€ë‚œì£¼ 2022-09-25ì— ì„¤ì •ì˜¨ë„(24.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(26.00Â°C) ì°¨ì´ê°€ 2.00Â°Cë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì§€ë‚œì£¼ 2022-09-23ì— ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(23.50Â°C) ì°¨ì´ê°€ 0.50Â°Cë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.'}, 'ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì¤‘ ë” ì¶”ìš´ê³³ì€ ì–´ë””ì•¼?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woQM')\": 'ì˜†ë°˜(27.00Â°C)ì´ ìš°ë¦¬ë°˜(28.50Â°C)ë³´ë‹¤ 1.50Â°C ë” ë‚®ì•„ìš”.', 'woExp': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ', 'woQM+Script': 'ìš°ë¦¬ë°˜(28.50Â°C)ì´ ì˜†ë°˜(27.00Â°C)ë³´ë‹¤ 1.50Â°C ë” ë”°ëœ»í•©ë‹ˆë‹¤.', 'woScript': 'ì˜†ë°˜(27.00Â°C)ì´ ìš°ë¦¬ë°˜(28.50Â°C)ë³´ë‹¤ 1.50Â°C ë” ë‚®ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜†ë°˜(27.00Â°C)ì´ ìš°ë¦¬ë°˜(28.50Â°C)ë³´ë‹¤ 1.50Â°C ë” ë‚®ì•„ìš”.'}, 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ê³¼ ì•žë°˜ì˜ í‰ê·  ì˜¨ë„ ì•Œë ¤ì¤˜': {'ours': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ê³¼ ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.39Â°C ìž…ë‹ˆë‹¤.', 'woExp': 'ìš°ë¦¬ë°˜ í‰ê·  ì˜¨ë„ëŠ” 25.39Â°C, ì•žë°˜ í‰ê·  ì˜¨ë„ëŠ” 28.50Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜(25.11Â°C)ê³¼ ì•žë°˜(25.67Â°C)ì˜ í‰ê·  ì˜¨ë„ëŠ” 25.40Â°Cìž…ë‹ˆë‹¤.', 'woQM': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.11Â°Cì´ê³ , ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.67Â°Cìž…ë‹ˆë‹¤.', 'woQM+Script': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 26.81Â°Cì´ê³ , ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 28.00Â°Cìž…ë‹ˆë‹¤.', 'woScript': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„({{v_ì´ë²ˆì£¼_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· : 27.27Â°C}})ëŠ” ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„({{v_ì´ë²ˆì£¼_ì•žë°˜_ì‹¤ë‚´ì˜¨ë„_í‰ê· : 27.00Â°C}})ë³´ë‹¤ 0.27Â°C ë†’ìŠµë‹ˆë‹¤. \\n\\n(ì°¸ê³ ë¡œ, ì •í™•í•œ í‰ê·  ê³„ì‚°ì„ ìœ„í•´ ì „ì²´ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ, ì œê³µëœ ë°ì´í„° ì˜ˆì‹œë§Œìœ¼ë¡œëŠ” ì „ì²´ ì£¼ê°„ í‰ê· ì„ ì •í™•ížˆ ì‚°ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì¼ë¶€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì‹œë¥¼ ì œê³µí•˜ì˜€ìŠµë‹ˆë‹¤.) \\n\\n**ìµœì¢… ë‹µë³€ë§Œ:**\\nì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(27.27Â°C)ëŠ” ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(27.00Â°C)ë³´ë‹¤ 0.27Â°C ë†’ìŠµë‹ˆë‹¤.', 'WoThinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜(25.11Â°C)ì€ ì•žë°˜(25.67Â°C)ë³´ë‹¤ -0.56Â°C ë‚®ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.98Â°Cì´ê³ , ì•žë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.11Â°Cìž…ë‹ˆë‹¤.'}, 'í˜„ìž¬ ì„¤ì •ì˜¨ë„ëž‘ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.': {\"('ours', 'woQM+Script')\": 'í˜„ìž¬ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ëŠ” ì‹¤ë‚´ì˜¨ë„(27.00Â°C)ë³´ë‹¤ -4.00Â°C ë‚®ìŠµë‹ˆë‹¤.', 'woExp': 'í˜„ìž¬ ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(27.00Â°C)ì˜ ì°¨ì´ëŠ” -4.00Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'í˜„ìž¬ ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(28.50Â°C) ì°¨ì´ëŠ” -5.50Â°Cìž…ë‹ˆë‹¤.', \"('woQM', 'woScript')\": 'í˜„ìž¬ ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(27.00Â°C) ì°¨ì´ëŠ” 4.00Â°Cìž…ë‹ˆë‹¤.', 'WoThinking': 'í˜„ìž¬ ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(27.00Â°C) ì°¨ì´ëŠ” -4.00Â°Cìž…ë‹ˆë‹¤.', 'GT_Response': 'í˜„ìž¬ ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(27.00Â°C)ì˜ ì°¨ì´ëŠ” 4.00Â°Cìž…ë‹ˆë‹¤.'}, 'ì§€ë‚œë‹¬ì— ì„¤ì •ì˜¨ë„ì™€ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ê°€ ê°€ìž¥ ë§Žì´ ë‚¬ë˜ ë‚ ì€?': {'ours': 'ì§€ë‚œë‹¬ 8ì›” 18ì¼ê³¼ 8ì›” 19ì¼ì— ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(21.00Â°C) ì°¨ì´ê°€ 2.00Â°Cë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.', 'woExp': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ', \"('WoMetadata+Thinking', 'woQM', 'woScript')\": 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì§€ë‚œë‹¬ ì„¤ì •ì˜¨ë„ì™€ ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ê°€ ê°€ìž¥ ë§Žì´ ë‚¬ë˜ ë‚  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•œ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.', 'WoThinking': 'ì§€ë‚œë‹¬ 8ì›” 18ì¼, 19ì¼ì— ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(22.50Â°C) ì°¨ì´ê°€ 0.50Â°Cë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì§€ë‚œë‹¬ 8ì›” 2ì¼, 4ì¼, 5ì¼, 7ì¼, 8ì¼, 10ì¼, 13ì¼, 16ì¼, 18ì¼, 21ì¼, 24ì¼, 26ì¼, 27ì¼, 29ì¼, 30ì¼ì— ì„¤ì •ì˜¨ë„(23.00Â°C)ì™€ ì‹¤ë‚´ì˜¨ë„(22.50Â°C) ì°¨ì´ê°€ 0.50Â°Cë¡œ ê°€ìž¥ ì»¸ìŠµë‹ˆë‹¤.'}, 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜': {'ours': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.11Â°Cì´ê³ , ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 25.98Â°Cìž…ë‹ˆë‹¤. ì°¨ì´ëŠ” -0.87Â°Cìž…ë‹ˆë‹¤.', 'woExp': 'ìš°ë¦¬ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.11Â°C)ì™€ ì˜†ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.98Â°C)ì˜ ì°¨ì´ëŠ” -0.87Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜(25.11Â°C)ê³¼ ì˜†ë°˜(25.98Â°C)ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ëŠ” -0.87Â°Cìž…ë‹ˆë‹¤.', 'woQM': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.11Â°C)ëŠ” ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.98Â°C)ë³´ë‹¤ -0.87Â°C ë‚®ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(26.50Â°C)ëŠ” ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(28.50Â°C)ë³´ë‹¤ -2.00Â°C ë‚®ìŠµë‹ˆë‹¤.', 'woScript': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(26.50Â°C)ëŠ” ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(27.50Â°C)ë³´ë‹¤ -1.00Â°C ë‚®ìŠµë‹ˆë‹¤.', 'WoThinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜(25.11Â°C)ì€ ì˜†ë°˜(25.98Â°C)ë³´ë‹¤ -0.87Â°C ë‚®ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì´ë²ˆì£¼ ìš°ë¦¬ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.98Â°C)ì™€ ì˜†ë°˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.67Â°C) ì°¨ì´ëŠ” 0.31Â°Cìž…ë‹ˆë‹¤.'}, '2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚  ì•Œë ¤ì¤˜': {'ours': '2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì€ 2022-09-15ë¡œ 26.50Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woExp': '2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì€ {{v_2ì£¼ì „_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ë‚ ì§œ}}ì¸ 2022-09-15ë¡œ ì‹¤ë‚´ì˜¨ë„ê°€ {{v_2ì£¼ì „_ìš°ë¦¬ë°˜_ì‹¤ë‚´ì˜¨ë„_ìµœê³ _ì˜¨ë„}}ì¸ 26.50Â°Cì˜€ìŠµë‹ˆë‹¤.\\n\\nì¶œë ¥: 2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì€ 2022-09-15ë¡œ ì‹¤ë‚´ì˜¨ë„(26.50Â°C)ê°€ ê°€ìž¥ ë†’ì•˜ìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': '2ì£¼ì „ ìš°ë¦¬ë°˜ì€ 2022-09-15ì— 26.50Â°Cë¡œ ê°€ìž¥ ë”ì› ìŠµë‹ˆë‹¤.', 'woQM': '2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì€ 2022-09-12, 2022-09-14ë¡œ 29.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woQM+Script': '2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì€ 2022-09-12, ìµœê³  ì˜¨ë„ëŠ” 29.00Â°Cì˜€ìŠµë‹ˆë‹¤. (ì£¼: ë°ì´í„°ì—ì„œ ê°€ìž¥ ë†’ì€ ì˜¨ë„ë¥¼ ê¸°ë¡í•œ ë‚ ì§œê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, ì˜ˆì‹œì—ì„œëŠ” ì²« ë²ˆì§¸ë¡œ ë‚˜íƒ€ë‚œ ë‚ ì§œë¥¼ ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤.) \\n\\nì •í™•í•œ ë‹¨ì¼ ë‹µë³€ìœ¼ë¡œ ì œí•œí•˜ìžë©´:\\n\\n2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì€ 2022-09-12ë¡œ 29.00Â°Cì˜€ìŠµë‹ˆë‹¤. (ë°ì´í„° ë‚´ì—ì„œ ê°€ìž¥ ë†’ì€ ì˜¨ë„ë¥¼ ë³´ì¸ ë‚ ì§œ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤.) \\n\\në‹¨, ì£¼ì–´ì§„ ë°ì´í„° í¬ë§·ê³¼ ì •ë³´ë§Œìœ¼ë¡œëŠ” ëª¨ë“  ë‚ ì§œë¥¼ ì •í™•ížˆ ë‚˜ì—´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì£¼ìš” ì •ë³´ì— ë”°ë¥¸ ë‹µë³€ìž…ë‹ˆë‹¤. \\n\\nìµœì¢… ë‹¨ì¼ ë‹µë³€:\\n2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì€ 2022-09-12ë¡œ 29.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woScript': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['Unknown']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", 'WoThinking': 'ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ 2ì£¼ì „ ê°€ìž¥ ë”ì› ë˜ ë‚ ì˜ ì˜¨ë„ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': '2ì£¼ì „ 9ì›” 12ì¼ê³¼ 9ì›” 14ì¼ì— ì‹¤ë‚´ì˜¨ë„(29.00Â°C)ê°€ ê°€ìž¥ ë†’ì•˜ìŠµë‹ˆë‹¤.'}, 'í™”ì„±ì˜ ì„¤ì •ì˜¨ë„ í™•ì¸í•´ì¤˜': {\"('WoThinking', 'ours', 'woExp', 'woScript')\": \"ì£„ì†¡í•©ë‹ˆë‹¤, ['í™”ì„±']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", \"('WoMetadata+Thinking', 'woQM')\": 'í™”ì„±ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cìž…ë‹ˆë‹¤.', 'woQM+Script': 'í™”ì„±ì˜ í˜„ìž¬ ì„¤ì •ì˜¨ë„ëŠ” ë°ì´í„° ì œê³µì´ ë¶€ì¡±í•˜ì—¬ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['í™”ì„±']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"}, 'ì˜†ë°˜ ìŠµë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woExp')\": \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ìŠµë„']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", \"('WoMetadata+Thinking', 'woQM+Script')\": 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'woQM': 'ì˜†ë°˜ ìŠµë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woScript': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ', 'GT_Response': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ìŠµë„']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"}, 'ì§€ë‚œ 3ì¼ ë™ì•ˆ ìš°ë¦¬ë°˜ ì‹¤ë‚´ ì˜¨ë„ í‰ê·  ê°’ ì•Œë ¤ì¤˜.': {'ours': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· (24.65Â°C)ì€ 24.65Â°C ìž…ë‹ˆë‹¤.', 'woExp': 'ì§€ë‚œ 3ì¼ ë™ì•ˆ ìš°ë¦¬ë°˜ ì‹¤ë‚´ ì˜¨ë„ í‰ê·  ê°’ì€ 24.65Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì§€ë‚œ 3ì¼ ë™ì•ˆ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 24.65Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woQM': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 25.15Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 25.00Â°C ìž…ë‹ˆë‹¤. \\n\\n(ì°¸ê³ : ì œê³µëœ ë°ì´í„°ëŠ” 2022-09-25ì¼ì˜ ë°ì´í„°ë§Œ í¬í•¨ë˜ì–´ ìžˆì–´, ì •í™•ížˆ 3ì¼ ë™ì•ˆì˜ í‰ê· ì„ ê³„ì‚°í•˜ê¸°ì—ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„° ë‚´ì—ì„œì˜ í‰ê· ì„ ì œê³µí•˜ì˜€ìŠµë‹ˆë‹¤.) \\n\\n**ìµœì¢… ë‹µë³€ë§Œ:** ì§€ë‚œ 3ì¼ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 25.00Â°C ìž…ë‹ˆë‹¤.', 'woScript': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 25.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'WoThinking': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 24.65Â°Cì˜€ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì§€ë‚œ 3ì¼ê°„ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ê°’ì€ 26.52Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œì— ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” ì–´ë• ì–´?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woExp', 'woQM', 'woQM+Script')\": 'ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woScript': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ ì•Œë ¤ì¤˜': {'ours': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ì€ 27.00Â°Cì´ê³ , ìµœì†Œê°’ì€ 20.50Â°Cìž…ë‹ˆë‹¤.', 'woExp': 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ì€ 27.00Â°Cì´ê³ , ìµœì†Œê°’ì€ 20.50Â°Cìž…ë‹ˆë‹¤. ìµœëŒ€ê°’ì€ 2022ë…„ 8ì›” 15ì¼, 16ì¼, 18ì¼ì— ê¸°ë¡ë˜ì—ˆìœ¼ë©°, ìµœì†Œê°’ì€ 2022ë…„ 8ì›” 19ì¼ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cë¡œ ê°€ìž¥ ë†’ì•˜ìŠµë‹ˆë‹¤. 20.50Â°Cë¡œ ê°€ìž¥ ë‚®ì•˜ìŠµë‹ˆë‹¤.', 'woQM': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cë¡œ ìµœëŒ€ì˜€ê³ , 20.50Â°Cë¡œ ìµœì†Œì˜€ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ì€ 25.50Â°Cì´ê³ , ìµœì†Œê°’ì€ 25.00Â°Cìž…ë‹ˆë‹¤.', 'woScript': 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ì€ 25.00Â°Cë¡œ íŠ¹ì • ê³µê°„ì„ ì§€ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìµœì†Œê°’ì€ 25.00Â°Cë¡œ ì—­ì‹œ íŠ¹ì • ê³µê°„ì„ ì§€ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. \\n\\n(ì°¸ê³ : ì œê³µëœ ë°ì´í„°ì—ì„œ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì€ ë™ì¼í•˜ë©°, ê³µê°„ ì •ë³´ê°€ êµ¬ì²´ì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.) \\n\\n**ê°„ê²°í™”ëœ ë‹µë³€:**\\nì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ì€ 25.00Â°C, ìµœì†Œê°’ì€ 25.00Â°Cìž…ë‹ˆë‹¤. ê³µê°„ ì •ë³´ëŠ” ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.', 'WoThinking': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cë¡œ ê°€ìž¥ ë†’ì•˜ìŠµë‹ˆë‹¤. 20.50Â°Cë¡œ ê°€ìž¥ ë‚®ì•˜ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì€ ê°ê° 30.50Â°Cì™€ 22.50Â°Cìž…ë‹ˆë‹¤.'}, 'ìš°ë¦¬ë°˜ê³¼ ì•žë°˜ ì¤‘ ê°€ìž¥ ë”ìš´ ë°©ì€?': {\"('WoThinking', 'ours', 'woScript')\": 'ì•žë°˜(28.50Â°C)ì´ ìš°ë¦¬ë°˜(27.00Â°C)ë³´ë‹¤ 1.50Â°C ë” ë†’ìŠµë‹ˆë‹¤.', 'woExp': 'ê°€ìž¥ ë”ìš´ ë°©ì€ ì•žë°˜ìœ¼ë¡œ ì‹¤ë‚´ì˜¨ë„ê°€ 28.50Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'woQM': 'ì•žë°˜(28.50Â°C)ì´ ìš°ë¦¬ë°˜(27.00Â°C)ë³´ë‹¤ 1.50Â°C ë†’ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì•žë°˜(28.50Â°C)ì´ ì•žë°˜ìœ¼ë¡œ ê°€ìž¥ ë”ìš´ ë°©ìž…ë‹ˆë‹¤.', 'GT_Response': 'ì•žë°˜ì´(ê°€) 27.00Â°Cë¡œ ê°€ìž¥ ë”ìš´ ë°©ì´ì—ìš”.'}, 'ì§€ê¸ˆ 4ì¸µ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('ours', 'woQM+Script')\": 'ì§€ê¸ˆ 4ì¸µì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.50Â°C ìž…ë‹ˆë‹¤.', \"('woExp', 'woScript')\": 'ì£„ì†¡í•©ë‹ˆë‹¤, []ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': '4ì¸µ í˜„ìž¬ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woQM': 'ì§€ê¸ˆ 4ì¸µì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'WoThinking': 'ì§€ê¸ˆ 4ì¸µì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.50Â°Cìž…ë‹ˆë‹¤.', 'GT_Response': 'ì§€ê¸ˆ 4ì¸µ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.50Â°Cìž…ë‹ˆë‹¤.'}, 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ê³¼ ì•žì§‘ì˜ í‰ê·  ì˜¨ë„ ì•Œë ¤ì¤˜': {'ours': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ê³¼ ì•žì§‘ì˜ í‰ê·  ì˜¨ë„ëŠ” 26.08Â°C ìž…ë‹ˆë‹¤.', 'woExp': 'ìš°ë¦¬ì§‘ê³¼ ì•žì§‘ì˜ ì´ë²ˆì£¼ í‰ê·  ì˜¨ë„ëŠ” 26.51Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘(25.98Â°C)ê³¼ ì•žì§‘(27.04Â°C)ì˜ í‰ê·  ì˜¨ë„ëŠ” 26.51Â°Cìž…ë‹ˆë‹¤.', 'woQM': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 26.08Â°Cì´ê³ , ì•žì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.04Â°Cìž…ë‹ˆë‹¤. ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ë³´ë‹¤ ì•žì§‘ì´ 1.00Â°C ë†’ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cì´ê³ , ì•žì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.', 'woScript': 'ì´ë²ˆì£¼ ë°ì´í„°ì—ì„œ ì •í™•í•œ í‰ê·  ì˜¨ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ì¶©ë¶„í•œ ë‚ ì§œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë” ë§Žì€ ë‚ ì§œë³„ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.', 'WoThinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘(27.04Â°C)ì€ ì•žì§‘(27.04Â°C)ë³´ë‹¤ 0.00Â°C ë†’ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.04Â°Cì´ê³ , ì•žì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.04Â°Cìž…ë‹ˆë‹¤.'}, 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ê³¼ ì˜†ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜': {'ours': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ê³¼ ì˜†ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'woExp': 'ìš°ë¦¬ì§‘ í‰ê·  ì‹¤ë‚´ì˜¨ë„(25.98Â°C)ì™€ ì˜†ì§‘ í‰ê·  ì‹¤ë‚´ì˜¨ë„(27.56Â°C)ì˜ ì°¨ì´ëŠ” -1.58Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘(25.98Â°C)ê³¼ ì˜†ì§‘(27.56Â°C)ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ëŠ” -1.58Â°Cìž…ë‹ˆë‹¤.', 'woQM': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(26.08Â°C)ëŠ” ì˜†ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(26.51Â°C)ë³´ë‹¤ -0.43Â°C ë‚®ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(28.00Â°C)ëŠ” ì˜†ì§‘ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„(27.50Â°C)ë³´ë‹¤ 0.50Â°C ë†’ìŠµë‹ˆë‹¤.', 'woScript': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ', 'WoThinking': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘(27.04Â°C)ì€ ì˜†ì§‘(27.04Â°C)ë³´ë‹¤ 0.00Â°C ë†’ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì´ë²ˆì£¼ ìš°ë¦¬ì§‘ í‰ê·  ì‹¤ë‚´ì˜¨ë„(27.04Â°C)ì™€ ì˜†ì§‘ í‰ê·  ì‹¤ë‚´ì˜¨ë„(27.56Â°C) ì°¨ì´ëŠ” 0.52Â°Cìž…ë‹ˆë‹¤.'}, 'ì˜†ì§‘ ìŠµë„ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woExp')\": \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ìŠµë„']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", 'WoMetadata+Thinking': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ', 'woQM': 'ì˜†ì§‘ì˜ ìŠµë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', \"('woQM+Script', 'woScript')\": 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'GT_Response': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['ìŠµë„']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\"}, 'ì§€ë‚œ 3ì¼ ë™ì•ˆ ìš°ë¦¬ì§‘ ì‹¤ë‚´ ì˜¨ë„ í‰ê·  ê°’ ì•Œë ¤ì¤˜.': {'ours': 'ì§€ë‚œ 3ì¼ ë™ì•ˆ ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 24.53Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woExp': 'ì§€ë‚œ 3ì¼ ë™ì•ˆ ìš°ë¦¬ì§‘ ì‹¤ë‚´ ì˜¨ë„ í‰ê·  ê°’ì€ 26.52Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 26.52Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woQM': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 26.30Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woQM+Script': \"ì§€ë‚œ 3ì¼ ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 25.00Â°C ìž…ë‹ˆë‹¤. \\n\\n(ì°¸ê³ : ì œê³µëœ ë°ì´í„°ëŠ” 2022ë…„ 9ì›” 26ì¼ í•˜ë£¨ ë™ì•ˆì˜ ì˜¨ë„ ë°ì´í„°ë§Œ í¬í•¨ë˜ì–´ ìžˆì–´, ì •í™•í•œ 'ì§€ë‚œ 3ì¼' í‰ê· ì„ ê³„ì‚°í•˜ê¸°ì—ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ í•˜ë£¨ í‰ê· ì„ ì œê³µí•˜ì˜€ìŠµë‹ˆë‹¤.) \\n\\n**ìµœì¢… ë‹µë³€ë§Œ:** ì§€ë‚œ 3ì¼ ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 25.00Â°C ìž…ë‹ˆë‹¤.\", 'woScript': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 29.50Â°Cì˜€ìŠµë‹ˆë‹¤.', 'WoThinking': 'ì§€ë‚œ 3ì¼ ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ í‰ê· ì€ 27.86Â°Cì˜€ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì§€ë‚œ 3ì¼ê°„ ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ í‰ê· ê°’ì€ 27.86Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œì— ì˜†ì§‘ì˜ ì„¤ì •ì˜¨ë„ëŠ” ì–´ë• ì–´?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours', 'woExp', 'woQM', 'woQM+Script')\": 'ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œ ì˜†ì§‘ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.', 'woScript': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œ ì˜†ì§‘ì˜ ì„¤ì •ì˜¨ë„ëŠ” 23.00Â°Cì˜€ìŠµë‹ˆë‹¤.'}, 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ ì•Œë ¤ì¤˜': {'ours': 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cë¡œ ê°€ìž¥ ë”ì› ê³ , 20.50Â°Cë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.', 'woExp': 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ì€ 30.50Â°C, ìµœì†Œê°’ì€ 22.50Â°Cìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ëŠ” 30.50Â°Cë¡œ ê°€ìž¥ ë”ì› ê³ , 22.50Â°Cë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.', 'woQM': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 31.50Â°Cë¡œ ìµœëŒ€ì˜€ê³ , 20.50Â°Cë¡œ ìµœì†Œì˜€ìŠµë‹ˆë‹¤.', 'woQM+Script': 'ì˜¬í•´ ì—¬ë¦„(2022-06-01 ~ 2022-08-31) ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ê°’ì€ {{v_ì˜¬í•´ì—¬ë¦„_ìš°ë¦¬ì§‘_ì‹¤ë‚´ì˜¨ë„_ìµœëŒ€}}Â°Cë¡œ ìµœëŒ€ê°’ì´ê³ , {{v_ì˜¬í•´ì—¬ë¦„_ìš°ë¦¬ì§‘_ì‹¤ë‚´ì˜¨ë„_ìµœì†Œ}}Â°Cë¡œ ìµœì†Œê°’ìž…ë‹ˆë‹¤.\\n\\nì¶œë ¥: ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’(28.00Â°C), ìµœì†Œê°’(23.00Â°C)ìž…ë‹ˆë‹¤.', 'woScript': 'ì˜¬í•´ ì—¬ë¦„(2022-06-01 ~ 2022-08-31) ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ì€ 28.00Â°C, ìµœì†Œê°’ì€ 26.00Â°Cìž…ë‹ˆë‹¤.', 'WoThinking': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ì§‘ ì‹¤ë‚´ì˜¨ë„ëŠ” 31.50Â°Cë¡œ ê°€ìž¥ ë”ì› ê³ , 24.00Â°Cë¡œ ê°€ìž¥ ì¶”ì› ìŠµë‹ˆë‹¤.', 'GT_Response': 'ì˜¬í•´ ì—¬ë¦„(6ì›” ~ 8ì›”) ìš°ë¦¬ì§‘ì˜ ì‹¤ë‚´ì˜¨ë„ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì€ ê°ê° 31.50Â°Cì™€ 24.00Â°Cìž…ë‹ˆë‹¤.'}, 'ìš°ë¦¬ì§‘ê³¼ ì•žì§‘ ì¤‘ ê°€ìž¥ ë”ìš´ ì§‘ì€?': {\"('WoMetadata+Thinking', 'WoThinking', 'ours')\": 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', \"('woExp', 'woQM')\": 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ', 'woQM+Script': 'ì•žì§‘(27.50Â°C)ì´ ì•žì§‘ìœ¼ë¡œ 27.50Â°Cë¡œ ê°€ìž¥ ë”ìš´ ì§‘ìž…ë‹ˆë‹¤.', 'woScript': 'ì•žì§‘ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ê°€ ì—†ì–´ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'GT_Response': 'í˜„ìž¬ ë‘ ì§‘ ì¤‘ ìš°ë¦¬ì§‘ì´(ê°€) 27.50Â°Cë¡œ ê°€ìž¥ ë”ìš´ ì§‘ì´ì—ìš”.'}, 'ì§€ê¸ˆ 8ì¸µ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜': {\"('ours', 'woExp')\": 'ì£„ì†¡í•©ë‹ˆë‹¤, []ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.', 'WoMetadata+Thinking': 'ì§€ê¸ˆ 8ì¸µ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.00Â°Cìž…ë‹ˆë‹¤.', 'woQM': 'ì§€ê¸ˆ 8ì¸µì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', \"('woQM+Script', 'woScript')\": 'ì§€ê¸ˆ 8ì¸µì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.50Â°C ìž…ë‹ˆë‹¤.', 'WoThinking': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['8ì¸µ']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", 'GT_Response': 'ì§€ê¸ˆ 8ì¸µ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 27.83Â°Cìž…ë‹ˆë‹¤.'}, 'í˜„ìž¬ 4ì¸µì—ì„œ ì˜¨ë„ 22â„ƒ ì´ìƒì¸ ë°©ë“¤ ì•Œë ¤ì¤˜': {\"('WoThinking', 'ours', 'woExp', 'woScript')\": 'ì£„ì†¡í•©ë‹ˆë‹¤, []ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.', \"('WoMetadata+Thinking', 'woQM', 'woQM+Script')\": 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'GT_Response': 'í˜„ìž¬ 4ì¸µ ì˜¨ë„ 22Â°C ì´ìƒì¸ ë°©ë“¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, '3ì£¼ì „ ì˜¨ë„ê°€ 22â„ƒ ì´ìƒì´ì—ˆë˜ ë‚ ì§œ ì•Œë ¤ì¤˜': {\"('WoMetadata+Thinking', 'ours', 'woExp', 'woQM')\": 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'woQM+Script': \"3ì£¼ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” ['2022-09-05', '2022-09-11']ë¡œ 22.00Â°Cì˜€ìŠµë‹ˆë‹¤.\", 'woScript': \"ì£„ì†¡í•©ë‹ˆë‹¤, ['Unknown']ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” ê³µê°„ì´ë‚˜ ëª¨ë‹¬ë¦¬í‹° ìž…ë‹ˆë‹¤.\", 'WoThinking': 'ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ', 'GT_Response': '3ì£¼ì „ 2022-09-05, 2022-09-06, 2022-09-07, 2022-09-08, 2022-09-09, 2022-09-10, 2022-09-11ì— 22Â°C ì´ìƒì´ì—ˆìŠµë‹ˆë‹¤.'}}\n"
     ]
    }
   ],
   "source": [
    "responses = {}\n",
    "\n",
    "dir_all = \"../experiments/revision_used\"\n",
    "for filename in Path(dir_all).iterdir():\n",
    "    name = filename.name\n",
    "    if not name.endswith(\"response.json\"):\n",
    "        continue\n",
    "    exp_tag = name.split(\"_\")[3]\n",
    "    # print(name, exp_tag)\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            input = item[\"Input\"]\n",
    "            if input not in responses:\n",
    "                responses[input] = {\n",
    "                    \"GT_Response\": item[\"GT_Response\"],\n",
    "                }\n",
    "            # if \"GT_Response\" in item:\n",
    "            #     print(item[\"GT_Response\"])\n",
    "            if \"PD_Response\" in item:\n",
    "                pd_response = item[\"PD_Response\"]\n",
    "            else:\n",
    "                pd_response = \"ì‹¤í–‰ì¤‘ ì—ëŸ¬ ë°œìƒ\"\n",
    "            \n",
    "            responses[input][f\"{exp_tag}\"] = pd_response\n",
    "\n",
    "# if the response is exactly equal, then merge them and make in to one, key is then tuple\n",
    "for input, response in responses.items():\n",
    "    if len(response) == 1:\n",
    "        continue\n",
    "    \n",
    "    # merge every matching pd_response (not only first one but every combination)\n",
    "    # create groups of responses with same values\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # group responses by their values (excluding GT_Response)\n",
    "    value_groups = defaultdict(list)\n",
    "    \n",
    "    for key, value in response.items():\n",
    "        if key != \"GT_Response\":\n",
    "            value_groups[value].append(key)\n",
    "    \n",
    "    # merge keys that have the same response values\n",
    "    merged_responses = {}\n",
    "    for value, keys in value_groups.items():\n",
    "        if len(keys) > 1:\n",
    "            # create tuple key for merged responses\n",
    "            merged_key = str(tuple(sorted(keys)))\n",
    "            merged_responses[merged_key] = value\n",
    "        else:\n",
    "            # keep single responses as is\n",
    "            merged_responses[keys[0]] = value\n",
    "    \n",
    "    # add back GT_Response\n",
    "    merged_responses[\"GT_Response\"] = response[\"GT_Response\"]\n",
    "    \n",
    "    # for key in merged_responses:\n",
    "    #     if isinstance(merged_responses[key], list):\n",
    "    #         merged_responses[key] = \" \".join(merged_responses[key])\n",
    "\n",
    "    # update responses dict\n",
    "    responses[input] = merged_responses\n",
    "\n",
    "# delete input = ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œì— ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” ì–´ë• ì–´?\n",
    "# try:\n",
    "#     del responses[\"ì˜¤ëŠ˜ ì˜¤í›„ 5ì‹œì— ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ëŠ” ì–´ë• ì–´?\"]\n",
    "# except KeyError:\n",
    "#     pass\n",
    "# try :\n",
    "#     del responses[\"ì§€ê¸ˆ 4ì¸µ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜\"]\n",
    "# except KeyError:\n",
    "#     pass\n",
    "\n",
    "# import pprint\n",
    "# pprint.pprint(responses)\n",
    "\n",
    "# save to json\n",
    "with open(\"responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, ensure_ascii=False, indent=4)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
