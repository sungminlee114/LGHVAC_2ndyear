{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:db.instance:Connected to the database PerSite_DB\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"TruePositive\"\n",
    "    false_positive = \"FalsePositive\"\n",
    "    false_negative = \"FalseNegative\"\n",
    "    semantic_true_positive = \"SemanticTruePositive\"\n",
    "    semantic_false_positive = \"SemanticFalsePositive\"\n",
    "    semantic_false_negative = \"SemanticFalseNegative\"\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result\n",
    "\n",
    "def run_query_and_get_report(input, tags, metadata, scenario, instruction_set):\n",
    "    input_report = {}\n",
    "    input_report[\"Input\"] = input\n",
    "    input_report[\"Tags\"] = tags\n",
    "    input_report[\"Scenario\"] = scenario\n",
    "    input_report[\"Result\"] = []\n",
    "    variables = {\n",
    "        \"Metadata\": metadata\n",
    "    }\n",
    "    print(input)\n",
    "    for instruction in instruction_set:\n",
    "        i_type = instruction[\"type\"]\n",
    "        if i_type == \"q\":\n",
    "            # query\n",
    "            args = instruction[\"args\"]\n",
    "            result_var_name = instruction[\"result_name\"]\n",
    "            # print(f\"Query: {args}, {result_var_name}\")\n",
    "            if \"temporal\" in args:\n",
    "                del args[\"table_name\"]\n",
    "                args[\"metadata\"] = metadata\n",
    "                result_df = DBManager.structured_query_data_t(args, get_rowids=True)\n",
    "            else:\n",
    "                result_df = DBManager.structured_query(args, get_rowids=True)\n",
    "            # print(f\"Result:\\n{result_df}\")\n",
    "            try:\n",
    "                if \"timestamp\" in result_df.columns:\n",
    "                    try:\n",
    "                        timestamp = result_df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    except Exception as e:\n",
    "                        print(args)\n",
    "                        print(result_df[\"timestamp\"])\n",
    "                result = result_df.to_dict(orient=\"index\")\n",
    "                cols = list(result_df.columns)\n",
    "                result = [[row[col] for col in cols] for row in result.values()]\n",
    "                input_report[\"Metadata\"] = metadata\n",
    "                input_report[\"Result\"].append({\n",
    "                    \"type\": \"q\",\n",
    "                    \"args\": args,\n",
    "                    # \"result_name\": result_var_name,\n",
    "                    \"result_shape\": result_df.shape,\n",
    "                    \"result_columns\": cols,\n",
    "                    \"result_indices\": list(result_df[\"id\"]),\n",
    "                    # \"result\": result\n",
    "                })\n",
    "\n",
    "                # drop rows where any value is -1\n",
    "                result_df = result_df[~result_df.isin([-1]).any(axis=1)]\n",
    "                variables[result_var_name] = result_df\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inside: {e}\")\n",
    "                logger.error(f\"Invoked with Query: {args}, {result_var_name}\")\n",
    "        elif i_type == \"o\":\n",
    "            script, returns = instruction[\"script\"], instruction[\"returns\"]\n",
    "            scripts = script.split(\";\")\n",
    "            scripts = [script.strip() for script in scripts]\n",
    "            scripts = [script for script in scripts if script != \"\"]\n",
    "            try:\n",
    "                variables.update(\n",
    "                    OperationExecutor.run_script(variables, scripts, returns)\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inside: {e}\")\n",
    "                logger.error(f\"Invoked with Script: {script}, Returns: {returns}\")\n",
    "                input_report[\"Result\"].append({\n",
    "                    \"type\": \"o\",\n",
    "                    \"script\": script,\n",
    "                    \"returns\": {k: None for k in returns}\n",
    "                })\n",
    "                \n",
    "                continue\n",
    "            variables_str = {}\n",
    "            k_to_track = []\n",
    "            k_to_track = [\"total_time_insec\"]\n",
    "            for k, v in variables.items():\n",
    "                if k in k_to_track:\n",
    "                    print(1, k, v, type(v))\n",
    "                # print(k, type(v))\n",
    "                type_ = None\n",
    "                while True:\n",
    "                    if type(v) in [pd.DataFrame]:\n",
    "                        # sort by timestamp\n",
    "                        \n",
    "                        if \"timestamp\" in v.columns:\n",
    "                            v = v.sort_values(by=\"timestamp\")\n",
    "                        \n",
    "                        v['timestamp'] = v['timestamp'].map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        v = v.to_dict(orient=\"index\")\n",
    "                        type_ = \"pd\"\n",
    "                        break\n",
    "                    \n",
    "                    # pd.Index\n",
    "                    elif type(v) in [pd.Index, np.ndarray, pd.Series]:\n",
    "                        if len(v) == 0:\n",
    "                            v = v.tolist()\n",
    "                            type_ = \"primitive\"\n",
    "                            continue\n",
    "                        # if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                        #     # v = [x.strftime(\"%Y-%m-%d %H:%M:%S\") for x in v]\n",
    "                        # elif type(v[0]) in [np.int64, np.float64, np.bool]:\n",
    "                        #     v = [x.item() for x in v]\n",
    "                        # break\n",
    "\n",
    "                        # if type(v) == np.ndarray:\n",
    "                        #     v = pd.Series(v)\n",
    "                        if type(v) in [pd.Series]:\n",
    "                            v.reset_index(drop=True, inplace=True)\n",
    "                        \n",
    "                        if k in k_to_track:\n",
    "                            print(2, k, v[0], type(v[0]))\n",
    "                            # print(2, k, v)\n",
    "\n",
    "                        v = pd.unique(v)\n",
    "                        v = pd.Series(v)\n",
    "                        if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                            v = v.map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        # v = v.to_dict()\n",
    "                        v = v.tolist()\n",
    "                        # remove -1 in the list\n",
    "                        v = [x for x in v if x not in [-1, np.nan]]\n",
    "                        if len(v) > 5:\n",
    "                            v = v[:5]\n",
    "                        # v = v.to_dict()\n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    \n",
    "                    elif type(v) in [np.int64, np.float64, np.bool]:\n",
    "                        v = v.item()\n",
    "                    elif type(v) in [np.datetime64]:\n",
    "                        v = pd.Timestamp(v)\n",
    "                        if k in k_to_track:\n",
    "                            print(3, k, v, type(v))\n",
    "                    elif type(v) in [pd.Timestamp, datetime.date, datetime.datetime]:\n",
    "                        v = v.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    elif type(v) in [int, float, bool, str, list, dict]:\n",
    "                        if type(v) in [int, float]:\n",
    "                            if v in [-1, np.nan]:\n",
    "                                v = None\n",
    "                        elif type(v) in [list, dict]:\n",
    "                            if len(v) == 0:\n",
    "                                v = None\n",
    "                            \n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Type not handled: {k}: {type(v), v}\")\n",
    "                        type_ = \"unknown\"\n",
    "                        break\n",
    "                variables_str[k] = (type_, v)\n",
    "                # print(k, v)\n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"o\",\n",
    "                \"script\": script,\n",
    "                \"returns\": {k: variables_str[k] for k in returns}\n",
    "            })\n",
    "\n",
    "            for k in variables_str:\n",
    "                if k in k_to_track:\n",
    "                    print(k, variables_str[k])\n",
    "                    \n",
    "        elif i_type == \"r\":\n",
    "            force = False\n",
    "\n",
    "            type_os = [r for r in input_report[\"Result\"] if r[\"type\"] == \"o\"]\n",
    "            returns = [r[\"returns\"] for r in type_os]\n",
    "            variables = {}\n",
    "\n",
    "            for r in returns:\n",
    "                variables.update(r)\n",
    "\n",
    "            values = variables.values()\n",
    "            values_has_no_value = any([v[1] is None for v in values])\n",
    "            if len(variables) == 0 or values_has_no_value:\n",
    "                force = True\n",
    "                if values_has_no_value:\n",
    "                    instruction[\"expectations\"] = [\"관련 데이터를 찾을 수 없습니다.\"]\n",
    "            \n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"r\",\n",
    "                \"expectations\": instruction[\"expectations\"],\n",
    "                \"force\": force\n",
    "            })\n",
    "        elif i_type == \"g\":\n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"g\",\n",
    "                \"args\": instruction[\"args\"]\n",
    "            })\n",
    "    return input_report\n",
    "\n",
    "def build_query_groundtruth(dateset_name):\n",
    "    def read(path):\n",
    "        data = read_json(path)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = directory.name\n",
    "            if \"v7\" in dateset_name:\n",
    "                data[i][\"Metadata\"] = metadata\n",
    "        return data\n",
    "\n",
    "    ds_ts = []\n",
    "    dt_tr = []\n",
    "    base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dateset_name}\")\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir() and \"scenario\" in directory.name:\n",
    "            if \"v7\" in dateset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            \n",
    "            ds_ts.extend(read(f\"{directory}/onlyq_ts.json\"))\n",
    "            dt_tr.extend(read(f\"{directory}/onlyq_tr.json\"))\n",
    "    \n",
    "    ds = ds_ts + dt_tr\n",
    "    \n",
    "    if \"v7\" in dateset_name:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "    else:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "        metadata = None\n",
    "    \n",
    "    with open(db_gt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[\")\n",
    "        with tqdm(total=len(ds)) as pbar:\n",
    "            for d in ds:\n",
    "                pbar.set_description(f\"Processing {d['Input']}\")\n",
    "                # print(\"--\")\n",
    "                \n",
    "                input = d[\"Input\"]\n",
    "                # if not \"time\" in input:\n",
    "                #     continue\n",
    "                # print(f\"Input: {input}\")\n",
    "                scenario = d[\"Scenario\"]\n",
    "                tags = d[\"Tags\"]\n",
    "                \n",
    "                metadata = d[\"Metadata\"]\n",
    "                response = d[\"Response\"]\n",
    "                # instruction_set = response[\"Instruction Set\"]\n",
    "                instruction_set = response[\"Instructions\"]\n",
    "                # print(f\"Instruction Set: {type(instruction_set)}, {len(instruction_set)}\")\n",
    "                instruction_set.append({\n",
    "                    \"type\": \"r\",\n",
    "                    \"expectations\": response[\"Expectations\"]\n",
    "                })\n",
    "                input_report = run_query_and_get_report(input, tags, metadata, scenario, instruction_set)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # print(input_report)\n",
    "                    # del input_report[\"Metadata\"]\n",
    "                    f.write(json.dumps(input_report, ensure_ascii=False) + \",\\n\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error outside: {e}\")\n",
    "                    logger.error(f\"Invoked with Input: {input}\")\n",
    "                    logger.error(f\"Input Report: {input_report}\")\n",
    "                    # exit()\n",
    "                    raise e\n",
    "                \n",
    "                # print(\"\\n\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "        # make it json array format\n",
    "        # remove last comma\n",
    "        f.seek(f.tell() - 2, 0)\n",
    "        f.write(\"]\")\n",
    "\n",
    "def eval_query(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            # if \"오늘 아침과 저녁\" not in input:\n",
    "            #     continue\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            gt_report = gt_report[0]\n",
    "            assert gt_report[\"Result\"] != []\n",
    "            # if gt_report[\"Result\"] == []:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            \n",
    "            gt_results = [d for d in gt_report[\"Result\"] if d[\"type\"] == \"q\"]\n",
    "            if len(gt_results) != 0:\n",
    "                gt_args = gt_results[0][\"args\"]\n",
    "\n",
    "                # Assume all cols and spatials are same across all queries\n",
    "                gt_semantic_cols = gt_args[\"columns\"]\n",
    "                gt_semantic_spatials = gt_args[\"spatials\"]\n",
    "                gt_semantics = gt_semantic_cols + gt_semantic_spatials\n",
    "                \n",
    "                gt_rows = []\n",
    "                for gt_result in gt_results:\n",
    "                    gt_rows.extend(gt_result[\"result_indices\"])\n",
    "                gt_rows = set(gt_rows)\n",
    "                gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "                gt_cols.remove(\"id\")\n",
    "                gt_cols.remove(\"idu\")\n",
    "                gt_total_combinations = len(gt_cols) * len(gt_rows)\n",
    "            else:\n",
    "                gt_total_combinations = 0\n",
    "                gt_semantics = []\n",
    "\n",
    "            # ---\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Scenario\"] = scenario\n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict) and (\"Instruction Set\" in cand_response[\"Candidate\"] or \"지시\" in cand_response[\"Candidate\"] or \"Instructions\" in cand_response[\"Candidate\"]):\n",
    "                if \"Instruction Set\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instruction Set\"]\n",
    "                elif \"지시\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"지시\"]\n",
    "                elif \"Instructions\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instructions\"]\n",
    "\n",
    "                evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                try:\n",
    "                    import re\n",
    "                    # get data between \"Instruction Set\": [ and the last]\n",
    "                    cand_instruction_set = re.search(r'(?<=\"Instructions\": \\[)(.*)(?=\\])', cand_response[\"Candidate\"], re.DOTALL).group(0)\n",
    "                    # find all {\"type\": ~ }, {\"type\": ~ }, {\"type\": ~ }\n",
    "                    cand_instruction_set = re.findall(r'({\"type\".*?})', cand_instruction_set)\n",
    "                    # print(list(cand_instruction_set))\n",
    "                    cand_instruction_set = [eval(d) for d in cand_instruction_set]\n",
    "                except Exception as e:\n",
    "                    evaluation_report[EM.json_structure] = False\n",
    "                    evaluation_report[EM.true_positive] = 0\n",
    "                    evaluation_report[EM.false_positive] = 0\n",
    "                    evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                    evaluation_report[EM.semantic_true_positive] = 0\n",
    "                    evaluation_report[EM.semantic_false_positive] = 0\n",
    "                    evaluation_report[EM.semantic_false_negative] = len(gt_semantics)\n",
    "\n",
    "                    print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                    print(e)\n",
    "                    evaluation_reports.append(evaluation_report)\n",
    "                    pbar.update(1)\n",
    "                    print(evaluation_report)\n",
    "                    continue\n",
    "            \n",
    "\n",
    "            cand_report = run_query_and_get_report(input, None, metadata, scenario, cand_instruction_set) \n",
    "            \n",
    "            cand_results = cand_report[\"Result\"]\n",
    "            cand_results = [d for d in cand_results if d[\"type\"] == \"q\"]\n",
    "\n",
    "            if len(cand_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                evaluation_report[EM.semantic_true_positive] = 0\n",
    "                evaluation_report[EM.semantic_false_positive] = 0\n",
    "                evaluation_report[EM.semantic_false_negative] = len(gt_semantics)\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                # print(evaluation_report)\n",
    "                            \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # assert len(cand_results) == 1\n",
    "\n",
    "            cand_args = [d[\"args\"] for d in cand_results if d[\"type\"] == \"q\"]\n",
    "            cand_semantic_cols = cand_args[0][\"columns\"]\n",
    "            cand_semantic_spatials = cand_args[0][\"spatials\"]\n",
    "            cand_semantics = cand_semantic_cols + cand_semantic_spatials\n",
    "\n",
    "            cand_rows = []\n",
    "            for cand_result in cand_results:\n",
    "                cand_rows.extend(cand_result[\"result_indices\"])\n",
    "\n",
    "            \n",
    "            cand_rows = set(cand_rows)\n",
    "            cand_cols = set(cand_results[0][\"result_columns\"])\n",
    "            cand_cols.remove(\"id\")\n",
    "            try:\n",
    "                cand_cols.remove(\"idu\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            if len(gt_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = len(cand_cols) * len(cand_rows)\n",
    "                evaluation_report[EM.false_negative] = 0\n",
    "\n",
    "                evaluation_report[EM.semantic_true_positive] = 0\n",
    "                evaluation_report[EM.semantic_false_positive] = len(cand_semantics)\n",
    "                evaluation_report[EM.semantic_false_negative] = 0\n",
    "\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "\n",
    "\n",
    "            # print(len(gt_flatten), len(cand_flatten))\n",
    "            \n",
    "            # gt_counter = Counter(gt_flatten)\n",
    "            # cand_counter = Counter(cand_flatten)\n",
    "\n",
    "            # true_positive = sum(min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_negative = sum(gt_counter[item] - min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_positive = sum(cand_counter[item] - min(cand_counter[item], gt_counter.get(item, 0)) for item in cand_counter)\n",
    "            \n",
    "            # # check if all gt results are in cand results\n",
    "            # true_positive, false_positive, false_negative = 0, 0, 0\n",
    "            # for gt_data in gt_flatten:\n",
    "            #     try:\n",
    "            #         cand_flatten.remove(gt_data)\n",
    "            #         true_positive += 1\n",
    "            #     except ValueError as e:\n",
    "            #         false_negative += 1\n",
    "            \n",
    "            # false_positive = len(cand_flatten)\n",
    "            \n",
    "            gt_semantics, cand_semantics = set(gt_semantics), set(cand_semantics)\n",
    "\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            evaluation_report[EM.semantic_true_positive] = len(gt_semantics & cand_semantics)\n",
    "            evaluation_report[EM.semantic_false_positive] = len(cand_semantics - gt_semantics)\n",
    "            evaluation_report[EM.semantic_false_negative] = len(gt_semantics - cand_semantics)\n",
    "\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # replace nan with 0\n",
    "    # eval_df.fillna(0, inplace=True)\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "\n",
    "    eval_df[\"Semantic_ExactMatch\"] = eval_df.apply(lambda x: x[EM.semantic_false_positive] == 0 and x[EM.semantic_false_negative] == 0, axis=1).astype(int)\n",
    "    final_result[\"Semantic_ExactMatch\"] = eval_df[\"Semantic_ExactMatch\"].mean()\n",
    "\n",
    "    eval_df[\"Semantic_Total\"] = eval_df[EM.semantic_true_positive] + eval_df[EM.semantic_false_positive] + eval_df[EM.semantic_false_negative]\n",
    "    eval_df[\"Semantic_TruePositive\"] = eval_df[EM.semantic_true_positive] / eval_df[\"Semantic_Total\"]\n",
    "    eval_df[\"Semantic_FalsePositive\"] = eval_df[EM.semantic_false_positive] / eval_df[\"Semantic_Total\"]\n",
    "    eval_df[\"Semantic_FalseNegative\"] = eval_df[EM.semantic_false_negative] / eval_df[\"Semantic_Total\"]\n",
    "\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.semantic_true_positive].sum(), eval_df[EM.semantic_false_positive].sum(), eval_df[EM.semantic_false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    final_result[\"Semantic_F1\"] = f1\n",
    "    final_result[\"Semantic_Recall\"] = recall\n",
    "\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_query_groundtruth(\"v5-250228-multimetadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WoAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 오늘 아침과 저녁의 온도차이는 얼마나 돼?:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 아침과 저녁의 온도차이는 얼마나 돼?\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '6 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '9 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '18 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '21 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지금 옆반 온도랑 우리반 온도 알려줘:   7%|▋         | 1/15 [00:00<00:00, 18.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금 옆반 온도랑 우리반 온도 알려줘\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB7') AND timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 현재 설정온도랑 실내온도 차이 알려줘.:  13%|█▎        | 2/15 [00:00<00:00, 33.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 설정온도랑 실내온도 차이 알려줘.\n",
      "SELECT \"settemp\", \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00' AND \"settemp\" IS NOT NULL AND \"settemp\" IS DISTINCT FROM 'NaN' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?:  20%|██        | 3/15 [00:00<00:00, 46.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?\n",
      "SELECT \"settemp\", \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('week', DATE '2022-09-30' - INTERVAL '1 week') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30') AND \"settemp\" IS NOT NULL AND \"settemp\" IS DISTINCT FROM 'NaN' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 설정온도가 실내온도보다 더 낮았던 날은?:  27%|██▋       | 4/15 [00:00<00:00, 19.82it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번주 설정온도가 실내온도보다 더 낮았던 날은?\n",
      "SELECT \"settemp\", \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('week', DATE '2022-09-30') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30' + INTERVAL '1 week') AND \"settemp\" IS NOT NULL AND \"settemp\" IS DISTINCT FROM 'NaN' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 어제 전원 껐어?:  33%|███▎      | 5/15 [00:00<00:00, 19.82it/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어제 전원 껐어?\n",
      "SELECT \"oper\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \"oper\" IS NOT NULL AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"oper\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB7') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \"oper\" IS NOT NULL AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"oper\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '02_I81') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \"oper\" IS NOT NULL AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:operation.execute:Error executing operation off_dates = daily_oper[daily_oper == False].index.strftime('%Y-%m-%d')\n",
      "ERROR:operation.execute:'Index' object has no attribute 'strftime'\n",
      "Processing 어제 전원 껐어?:  33%|███▎      | 5/15 [00:00<00:00, 14.09it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m cand_response_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr-v7_r256_a512_woall_16bit_adamw16bit_0322-checkpoint-60\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m cand_response_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcand_response_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_gt_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand_response_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(eval_df)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 376\u001b[0m, in \u001b[0;36meval_query\u001b[0;34m(db_gt_filename, cand_response_filename)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;28mprint\u001b[39m(evaluation_report)\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m cand_report \u001b[38;5;241m=\u001b[39m \u001b[43mrun_query_and_get_report\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand_instruction_set\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    378\u001b[0m cand_results \u001b[38;5;241m=\u001b[39m cand_report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    379\u001b[0m cand_results \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m cand_results \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[50], line 76\u001b[0m, in \u001b[0;36mrun_query_and_get_report\u001b[0;34m(input, tags, metadata, scenario, instruction_set)\u001b[0m\n\u001b[1;32m     73\u001b[0m script, returns \u001b[38;5;241m=\u001b[39m instruction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m], instruction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     variables\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mOperationExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError inside: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/src/operation/execute.py:27\u001b[0m, in \u001b[0;36mOperationExecutor.execute\u001b[0;34m(cls, args, python_script, returns)\u001b[0m\n\u001b[1;32m     25\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError executing operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(e)\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print([name for name in globals() if not name.startswith('_')])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# return variables named in the returns list\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28mglobals\u001b[39m()[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m returns}\n",
      "File \u001b[0;32m/workspace/src/operation/execute.py:23\u001b[0m, in \u001b[0;36mOperationExecutor.execute\u001b[0;34m(cls, args, python_script, returns)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m script \u001b[38;5;129;01min\u001b[39;00m scripts:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(script)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# if script == \"dates = daily_avg_temp[daily_avg_temp['settemp'] < daily_avg_temp['roomtemp']].index;\":\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#     print(daily_avg_temp[daily_avg_temp['settemp'] < daily_avg_temp['roomtemp']].index)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# print(script)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         exec(script, \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     25\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError executing operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r128_a256_woall-checkpoint-60\"\n",
    "cand_response_filename = \"r-v7_r256_a512_woall_16bit_adamw16bit_0322-checkpoint-60\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "# print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>FalsePositive</th>\n",
       "      <th>FalseNegative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Input, Scenario, FalsePositive, FalseNegative]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"r-v5_r256_a512_FI-checkpoint-43-batch\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r256_a512_ISP-checkpoint-104\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-v5_r256_a512_ours-checkpoint-20\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours-checkpoint-52-batch\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours_noexample-checkpoint-50-batch\"\n",
    "# cand_response_filename = \"r-v6_r128_a256_ours-checkpoint-52\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours-checkpoint-40\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours_shorten-checkpoint-30\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Why is our classroom so cold:   0%|          | 0/72 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is our classroom so cold\n",
      "['qr']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuild_query_groundtruth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv7-250309-reduceinputanddatefunctioncall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 262\u001b[0m, in \u001b[0;36mbuild_query_groundtruth\u001b[0;34m(dateset_name)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# print(f\"Instruction Set: {type(instruction_set)}, {len(instruction_set)}\")\u001b[39;00m\n\u001b[1;32m    258\u001b[0m instruction_set\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpectations\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpectations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    261\u001b[0m })\n\u001b[0;32m--> 262\u001b[0m input_report \u001b[38;5;241m=\u001b[39m \u001b[43mrun_query_and_get_report\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# print(input_report)\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# del input_report[\"Metadata\"]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(input_report, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 107\u001b[0m, in \u001b[0;36mrun_query_and_get_report\u001b[0;34m(input, tags, metadata, scenario, instruction_set)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    105\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m type_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/series.py:4700\u001b[0m, in \u001b[0;36mSeries.map\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap\u001b[39m(\n\u001b[1;32m   4621\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4622\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[1;32m   4623\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4624\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4626\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[1;32m   4627\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4698\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   4699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4700\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4702\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4703\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[14], line 107\u001b[0m, in \u001b[0;36mrun_query_and_get_report.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    105\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    108\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m type_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "build_query_groundtruth(\"v7-250309-reduceinputanddatefunctioncall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_query_gtgt(db_gt_filename, cand_response_filename):\n",
    "#     db_gts = read_json(db_gt_filename)\n",
    "#     cand_responses = read_json(cand_response_filename)\n",
    "#     # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "#     evaluation_reports = []\n",
    "\n",
    "#     with tqdm(total=len(cand_responses)) as pbar:\n",
    "#         for cand_report in cand_responses:\n",
    "#             pbar.set_description(f\"Processing {cand_report['Input']}\")\n",
    "#             input = cand_report[\"Input\"]\n",
    "#             scenario = cand_report[\"Scenario\"]\n",
    "\n",
    "\n",
    "#             # 관계 없는 질문들은 건너뛰자\n",
    "#             gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            \n",
    "            \n",
    "#             assert len(gt_report) <= 1\n",
    "#             if len(gt_report) == 0:\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "#             gt_report = gt_report[0]\n",
    "#             if gt_report[\"Result\"] == []:\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "            \n",
    "            \n",
    "#             # print(f\"Input: {input}\")\n",
    "            \n",
    "#             gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "#             cand_results = cand_results[0]\n",
    "\n",
    "#             gt_rows = []\n",
    "#             for gt_result in gt_results:\n",
    "#                 gt_rows.extend(gt_result[\"result_indices\"])\n",
    "\n",
    "#             gt_rows = set(gt_rows)\n",
    "#             gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "#             cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "#             gt_cols.remove(\"id\")\n",
    "#             cand_cols.remove(\"id\")\n",
    "#             # gt_cols.remove(\"idu\")\n",
    "#             cand_cols.remove(\"idu\")\n",
    "\n",
    "#             # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "#             true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "#             # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "#             false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "#             # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "#             false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "            \n",
    "#             evaluation_report = defaultdict(lambda: None)\n",
    "#             evaluation_report[EM.true_positive] = true_positive\n",
    "#             evaluation_report[EM.false_positive] = false_positive\n",
    "#             evaluation_report[EM.false_negative] = false_negative\n",
    "#             evaluation_report[\"Input\"] = input\n",
    "#             evaluation_reports.append(evaluation_report)\n",
    "#             # print(evaluation_report)\n",
    "            \n",
    "#             pbar.update(1)\n",
    "\n",
    "#     eval_df = pd.DataFrame(evaluation_reports)\n",
    "#     # print(eval_df)\n",
    "\n",
    "#     eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "#     # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "#     # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "#     # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "#     final_result = {}\n",
    "\n",
    "#     for col in [\"ExactMatch\"]:\n",
    "#         # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "#         final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "#     # normalize per query\n",
    "#     eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "#     eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "#     eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "#     eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "#     # # F1 score except nans.\n",
    "#     truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "#     precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "#     recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     # print(f\"F1: {f1}\")\n",
    "#     final_result[\"F1\"] = f1\n",
    "#     final_result[\"Recall\"] = recall\n",
    "#     for col in final_result:\n",
    "#         print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "#     return eval_df\n",
    "\n",
    "# db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "# cand_response_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "\n",
    "# eval_df = eval_query_gtgt(db_gt_filename, cand_response_filename)\n",
    "# print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 오늘 아침과 저녁의 온도차이는 얼마나 돼?:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "175\n",
      "355\n",
      "오늘 아침과 저녁의 온도차이는 얼마나 돼?\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '6 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '9 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB7') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '18 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '21 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지금 몇시야?:   7%|▋         | 1/15 [00:00<00:00, 14.28it/s]                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JsonStructureCorrectness: 1.00\n",
      "ExactMatch: 0.00\n",
      "F1: 0.51\n",
      "Recall: 0.51\n",
      "Semantic_ExactMatch: 1.00\n",
      "Semantic_F1: 1.00\n",
      "Semantic_Recall: 1.00\n",
      "                     Input   Scenario  JsonStructureCorrectness  TruePositive  \\\n",
      "0  오늘 아침과 저녁의 온도차이는 얼마나 돼?  scenario1                      True      0.339623   \n",
      "\n",
      "   FalsePositive  FalseNegative  SemanticTruePositive  SemanticFalsePositive  \\\n",
      "0       0.330189       0.330189                     2                      0   \n",
      "\n",
      "   SemanticFalseNegative  ExactMatch  Total  Semantic_ExactMatch  \\\n",
      "0                      0           0   1060                    1   \n",
      "\n",
      "   Semantic_Total  Semantic_TruePositive  Semantic_FalsePositive  \\\n",
      "0               2                    1.0                     0.0   \n",
      "\n",
      "   Semantic_FalseNegative  \n",
      "0                     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/r-v7_r256_a512_ours_16bit_adamw16bit_0322-checkpoint-56.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>TruePositive</th>\n",
       "      <th>FalsePositive</th>\n",
       "      <th>FalseNegative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오늘 아침과 저녁의 온도차이는 얼마나 돼?</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0.330189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>올해 여름 우리반 실내온도 최대값과 최소값 알려줘</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Input  TruePositive  FalsePositive  FalseNegative\n",
       "0       오늘 아침과 저녁의 온도차이는 얼마나 돼?      0.339623       0.330189       0.330189\n",
       "12  올해 여름 우리반 실내온도 최대값과 최소값 알려줘      0.333333       0.333333       0.333333"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
