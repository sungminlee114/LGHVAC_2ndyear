{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"TruePositive\"\n",
    "    false_positive = \"FalsePositive\"\n",
    "    false_negative = \"FalseNegative\"\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result\n",
    "\n",
    "def run_query_and_get_report(input, tags, metadata, scenario, instruction_set):\n",
    "    input_report = {}\n",
    "    input_report[\"Input\"] = input\n",
    "    input_report[\"Tags\"] = tags\n",
    "    input_report[\"Scenario\"] = scenario\n",
    "    input_report[\"Result\"] = []\n",
    "    variables = {\n",
    "        \"Metadata\": metadata\n",
    "    }\n",
    "    print(input)\n",
    "    for instruction in instruction_set:\n",
    "        i_type = instruction[\"type\"]\n",
    "        if i_type == \"q\":\n",
    "            # query\n",
    "            args = instruction[\"args\"]\n",
    "            result_var_name = instruction[\"result_name\"]\n",
    "            # print(f\"Query: {args}, {result_var_name}\")\n",
    "            if \"temporal\" in args:\n",
    "                del args[\"table_name\"]\n",
    "                args[\"metadata\"] = metadata\n",
    "                result_df = DBManager.structured_query_data_t(args, get_rowids=True)\n",
    "            else:\n",
    "                result_df = DBManager.structured_query(args, get_rowids=True)\n",
    "            # print(f\"Result:\\n{result_df}\")\n",
    "            try:\n",
    "                if \"timestamp\" in result_df.columns:\n",
    "                    try:\n",
    "                        timestamp = result_df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    except Exception as e:\n",
    "                        print(args)\n",
    "                        print(result_df[\"timestamp\"])\n",
    "                result = result_df.to_dict(orient=\"index\")\n",
    "                cols = list(result_df.columns)\n",
    "                result = [[row[col] for col in cols] for row in result.values()]\n",
    "                input_report[\"Metadata\"] = metadata\n",
    "                input_report[\"Result\"].append({\n",
    "                    \"type\": \"q\",\n",
    "                    # \"args\": args,\n",
    "                    # \"result_name\": result_var_name,\n",
    "                    \"result_shape\": result_df.shape,\n",
    "                    \"result_columns\": cols,\n",
    "                    \"result_indices\": list(result_df[\"id\"]),\n",
    "                    # \"result\": result\n",
    "                })\n",
    "\n",
    "                # drop rows where any value is -1\n",
    "                result_df = result_df[~result_df.isin([-1]).any(axis=1)]\n",
    "\n",
    "                variables[result_var_name] = result_df\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inside: {e}\")\n",
    "                logger.error(f\"Invoked with Query: {args}, {result_var_name}\")\n",
    "        elif i_type == \"o\":\n",
    "            script, returns = instruction[\"script\"], instruction[\"returns\"]\n",
    "            variables.update(\n",
    "                OperationExecutor.execute(variables, script, returns)\n",
    "            )\n",
    "            variables_str = {}\n",
    "            k_to_track = []\n",
    "            k_to_track = [\"total_time_insec\"]\n",
    "            for k, v in variables.items():\n",
    "                if k in k_to_track:\n",
    "                    print(1, k, v, type(v))\n",
    "                # print(k, type(v))\n",
    "                type_ = None\n",
    "                while True:\n",
    "                    if type(v) in [pd.DataFrame]:\n",
    "                        v['timestamp'] = v['timestamp'].map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        v = v.to_dict(orient=\"index\")\n",
    "                        type_ = \"pd\"\n",
    "                        break\n",
    "                    \n",
    "                    # pd.Index\n",
    "                    elif type(v) in [pd.Index, np.ndarray, pd.Series]:\n",
    "                        if len(v) == 0:\n",
    "                            v = v.tolist()\n",
    "                            type_ = \"primitive\"\n",
    "                            continue\n",
    "                        # if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                        #     # v = [x.strftime(\"%Y-%m-%d %H:%M:%S\") for x in v]\n",
    "                        # elif type(v[0]) in [np.int64, np.float64, np.bool]:\n",
    "                        #     v = [x.item() for x in v]\n",
    "                        # break\n",
    "\n",
    "                        # if type(v) == np.ndarray:\n",
    "                        #     v = pd.Series(v)\n",
    "                        if type(v) in [pd.Series]:\n",
    "                            v.reset_index(drop=True, inplace=True)\n",
    "                        \n",
    "                        if k in k_to_track:\n",
    "                            print(2, k, v[0], type(v[0]))\n",
    "                            # print(2, k, v)\n",
    "\n",
    "                        v = pd.unique(v)\n",
    "                        v = pd.Series(v)\n",
    "                        if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                            v = v.map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        # v = v.to_dict()\n",
    "                        v = v.tolist()\n",
    "                        # remove -1 in the list\n",
    "                        v = [x for x in v if x not in [-1, np.nan]]\n",
    "                        if len(v) > 5:\n",
    "                            v = v[:5]\n",
    "                        # v = v.to_dict()\n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    \n",
    "                    elif type(v) in [np.int64, np.float64, np.bool]:\n",
    "                        v = v.item()\n",
    "                    elif type(v) in [np.datetime64]:\n",
    "                        v = pd.Timestamp(v)\n",
    "                        if k in k_to_track:\n",
    "                            print(3, k, v, type(v))\n",
    "                    elif type(v) in [pd.Timestamp, datetime.date, datetime.datetime]:\n",
    "                        v = v.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    elif type(v) in [int, float, bool, str, list, dict]:\n",
    "                        if type(v) in [int, float]:\n",
    "                            if v in [-1, np.nan]:\n",
    "                                v = None\n",
    "                        elif type(v) in [list, dict]:\n",
    "                            if len(v) == 0:\n",
    "                                v = None\n",
    "                            \n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Type not handled: {k}: {type(v), v}\")\n",
    "                        type_ = \"unknown\"\n",
    "                        break\n",
    "                variables_str[k] = (type_, v)\n",
    "                # print(k, v)\n",
    "           \n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"o\",\n",
    "                \"script\": script,\n",
    "                \"returns\": {k: variables_str[k] for k in returns}\n",
    "            })\n",
    "            print(input_report[\"Result\"][-1][\"returns\"])\n",
    "            for k in variables_str:\n",
    "                if k in k_to_track:\n",
    "                    print(k, variables_str[k])\n",
    "        elif i_type == \"r\":\n",
    "            force = False\n",
    "\n",
    "            type_os = [r for r in input_report[\"Result\"] if r[\"type\"] == \"o\"]\n",
    "            returns = [r[\"returns\"] for r in type_os]\n",
    "            variables = {}\n",
    "            for r in returns:\n",
    "                variables.update(r)\n",
    "\n",
    "            values = variables.values()\n",
    "            values_has_no_value = any([v[1] is None for v in values])\n",
    "            if len(variables) == 0 or values_has_no_value:\n",
    "                force = True\n",
    "                if values_has_no_value:\n",
    "                    instruction[\"expectations\"] = [\"관련 데이터를 찾을 수 없습니다.\"]\n",
    "            \n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"r\",\n",
    "                \"expectations\": instruction[\"expectations\"],\n",
    "                \"force\": force\n",
    "            })\n",
    "\n",
    "    return input_report\n",
    "\n",
    "def build_query_groundtruth(dateset_name):\n",
    "    ds_ts = []\n",
    "    dt_tr = []\n",
    "    base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dateset_name}\")\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir() and \"scenario\" in directory.name:\n",
    "           \n",
    "            ts = read_json(f\"{directory}/onlyq_ts.json\")\n",
    "            if \"v7\" in dateset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            for i, d in enumerate(ts):\n",
    "                ts[i][\"Scenario\"] = directory.name\n",
    "                if \"v7\" in dateset_name:\n",
    "                    ts[i][\"Metadata\"] = metadata\n",
    "            ds_ts.extend(ts)\n",
    "            \n",
    "\n",
    "            tr = read_json(f\"{directory}/onlyq_tr.json\")\n",
    "            for i, d in enumerate(tr):\n",
    "                tr[i][\"Scenario\"] = directory.name\n",
    "                if \"v7\" in dateset_name:\n",
    "                    tr[i][\"Metadata\"] = metadata\n",
    "            dt_tr.extend(tr)\n",
    "    \n",
    "    ds = ds_ts + dt_tr\n",
    "    print(len(ds))\n",
    "    \n",
    "    if \"v7\" in dateset_name:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "    else:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "        metadata = None\n",
    "    \n",
    "    with open(db_gt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[\")\n",
    "        with tqdm(total=len(ds)) as pbar:\n",
    "            for d in ds:\n",
    "                pbar.set_description(f\"Processing {d['Input']}\")\n",
    "                # print(\"--\")\n",
    "                \n",
    "                input = d[\"Input\"]\n",
    "                # if not \"time\" in input:\n",
    "                #     continue\n",
    "                # print(f\"Input: {input}\")\n",
    "                scenario = d[\"Scenario\"]\n",
    "                tags = d[\"Tags\"]\n",
    "                \n",
    "                metadata = d[\"Metadata\"]\n",
    "                response = d[\"Response\"]\n",
    "                # instruction_set = response[\"Instruction Set\"]\n",
    "                instruction_set = response[\"Instructions\"]\n",
    "                #print(f\"Instruction Set: {type(instruction_set)}, {len(instruction_set)}\")\n",
    "                instruction_set.append({\n",
    "                    \"type\": \"r\",\n",
    "                    \"expectations\": response[\"Expectations\"]\n",
    "                })\n",
    "                input_report = run_query_and_get_report(input, tags, metadata, scenario, instruction_set)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    #print(input_report)\n",
    "                    # del input_report[\"Metadata\"]\n",
    "                    f.write(json.dumps(input_report, ensure_ascii=False) + \",\\n\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error outside: {e}\")\n",
    "                    logger.error(f\"Invoked with Input: {input}\")\n",
    "                    logger.error(f\"Input Report: {input_report}\")\n",
    "                    # exit()\n",
    "                    raise e\n",
    "                \n",
    "                # print(\"\\n\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "        # make it json array format\n",
    "        # remove last comma\n",
    "        f.seek(f.tell() - 2, 0)\n",
    "        f.write(\"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_query_groundtruth(\"v5-250228-multimetadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_query(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            gt_report = gt_report[0]\n",
    "            if gt_report[\"Result\"] == []:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Scenario\"] = scenario\n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict) and (\"Instruction Set\" in cand_response[\"Candidate\"] or \"지시\" in cand_response[\"Candidate\"] or \"Instructions\" in cand_response[\"Candidate\"]):\n",
    "                if \"Instruction Set\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instruction Set\"]\n",
    "                elif \"지시\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"지시\"]\n",
    "                elif \"Instructions\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instructions\"]\n",
    "\n",
    "                evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                try:\n",
    "                    import re\n",
    "                    # get data between \"Instruction Set\": [ and the last]\n",
    "                    cand_instruction_set = re.search(r'(?<=\"Instruction Set\": \\[)(.*)(?=\\])', cand_response[\"Candidate\"], re.DOTALL).group(0)\n",
    "                    # find all {\"type\": ~ }, {\"type\": ~ }, {\"type\": ~ }\n",
    "                    cand_instruction_set = re.findall(r'({\"type\".*?})', cand_instruction_set)\n",
    "                    # print(list(cand_instruction_set))\n",
    "                    cand_instruction_set = [eval(d) for d in cand_instruction_set]\n",
    "                except Exception as e:\n",
    "                    evaluation_report[EM.json_structure] = False\n",
    "                    print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                    print(e)\n",
    "                    evaluation_reports.append(evaluation_report)\n",
    "                    pbar.update(1)\n",
    "                    print(evaluation_report)\n",
    "                    continue\n",
    "                    \n",
    "            cand_report = run_query_and_get_report(input, metadata, scenario, cand_instruction_set) \n",
    "            \n",
    "            # print(f\"Input: {input}\")\n",
    "            \n",
    "            gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "\n",
    "            if len(cand_results) == 0:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                # evaluation_report[EM.true_positive] = 0\n",
    "                # evaluation_report[EM.false_positive] = false_positive\n",
    "                # evaluation_report[EM.false_negative] = false_negative\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                # print(evaluation_report)\n",
    "                            \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            cand_results = cand_results[0]\n",
    "\n",
    "            gt_rows = []\n",
    "            for gt_result in gt_results:\n",
    "                gt_rows.extend(gt_result[\"result_indices\"])\n",
    "            \n",
    "            gt_rows = set(gt_rows)\n",
    "            gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "            cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "            gt_cols.remove(\"id\")\n",
    "            cand_cols.remove(\"id\")\n",
    "            gt_cols.remove(\"idu\")\n",
    "            try:\n",
    "                cand_cols.remove(\"idu\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "\n",
    "\n",
    "            # print(len(gt_flatten), len(cand_flatten))\n",
    "            \n",
    "            # gt_counter = Counter(gt_flatten)\n",
    "            # cand_counter = Counter(cand_flatten)\n",
    "\n",
    "            # true_positive = sum(min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_negative = sum(gt_counter[item] - min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_positive = sum(cand_counter[item] - min(cand_counter[item], gt_counter.get(item, 0)) for item in cand_counter)\n",
    "            \n",
    "            # # check if all gt results are in cand results\n",
    "            # true_positive, false_positive, false_negative = 0, 0, 0\n",
    "            # for gt_data in gt_flatten:\n",
    "            #     try:\n",
    "            #         cand_flatten.remove(gt_data)\n",
    "            #         true_positive += 1\n",
    "            #     except ValueError as e:\n",
    "            #         false_negative += 1\n",
    "            \n",
    "            # false_positive = len(cand_flatten)\n",
    "            \n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            \n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WoAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난 3일 동안 우리반 실내 온도 평균 값 알려줘.: 100%|██████████| 21/21 [00:00<00:00, 2709.00it/s]                      \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot set a DataFrame without columns to the column ExactMatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_552935/46456084.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# cand_response_filename = \"r-v5_r32_a64_woall-checkpoint-70-batch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# cand_response_filename = \"r-v6_r64_a128_woall_shorten-checkpoint-53\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcand_response_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0meval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_gt_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_response_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print(eval_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_552935/2560626893.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(db_gt_filename, cand_response_filename)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0meval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_reports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# print(eval_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0meval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ExactMatch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfalse_positive\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfalse_negative\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;31m# eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4297\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4298\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4300\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4302\u001b[0m         elif (\n\u001b[1;32m   4303\u001b[0m             \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4304\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4460\u001b[0m                 \u001b[0;34m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4461\u001b[0m                 \u001b[0;34mf\"column {key}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4462\u001b[0m             )\n\u001b[1;32m   4463\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4464\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   4465\u001b[0m                 \u001b[0;34mf\"Cannot set a DataFrame without columns to the column {key}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4466\u001b[0m             )\n\u001b[1;32m   4467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set a DataFrame without columns to the column ExactMatch"
     ]
    }
   ],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r128_a256_woall-checkpoint-60\"\n",
    "cand_response_filename = \"r-v7_r8_a16_woall_4bit-checkpoint-97\"\n",
    "# cand_response_filename = \"r-v5_r32_a64_woall-checkpoint-70-batch\"\n",
    "# cand_response_filename = \"r-v6_r64_a128_woall_shorten-checkpoint-53\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "# print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"r-v5_r256_a512_FI-checkpoint-43-batch\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r256_a512_ISP-checkpoint-104\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# cand_response_filename = \"r-v5_r128_a256_ours-checkpoint-52-batch\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# cand_response_filename = \"r-v5_r128_a256_ours_noexample-checkpoint-50-batch\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# cand_response_filename = \"r-v6_r128_a256_ours-checkpoint-52\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# cand_response_filename = \"r-v6_r256_a512_ours-checkpoint-40\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# cand_response_filename = \"r-v6_r256_a512_ours_shorten-checkpoint-30\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m cand_response_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcand_response_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_gt_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand_response_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36meval_query\u001b[0;34m(db_gt_filename, cand_response_filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_query\u001b[39m(db_gt_filename, cand_response_filename):\n\u001b[0;32m----> 2\u001b[0m     db_gts \u001b[38;5;241m=\u001b[39m \u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_gt_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     cand_responses \u001b[38;5;241m=\u001b[39m read_json(cand_response_filename)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_json\u001b[39m(path):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 9\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-v5_r256_a512_ours-checkpoint-20\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours-checkpoint-52-batch\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours_noexample-checkpoint-50-batch\"\n",
    "# cand_response_filename = \"r-v6_r128_a256_ours-checkpoint-52\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours-checkpoint-40\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours_shorten-checkpoint-30\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'metadata' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuild_query_groundtruth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv7-250309-reduceinputanddatefunctioncall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 203\u001b[0m, in \u001b[0;36mbuild_query_groundtruth\u001b[0;34m(dateset_name)\u001b[0m\n\u001b[1;32m    201\u001b[0m             tr[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScenario\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m directory\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv7\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dateset_name:\n\u001b[0;32m--> 203\u001b[0m                 tr[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\n\u001b[1;32m    204\u001b[0m         dt_tr\u001b[38;5;241m.\u001b[39mextend(tr)\n\u001b[1;32m    206\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds_ts \u001b[38;5;241m+\u001b[39m dt_tr\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'metadata' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "build_query_groundtruth(\"v7-250309-reduceinputanddatefunctioncall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_query_gtgt(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_report in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_report['Input']}\")\n",
    "            input = cand_report[\"Input\"]\n",
    "            scenario = cand_report[\"Scenario\"]\n",
    "\n",
    "\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            \n",
    "            \n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            gt_report = gt_report[0]\n",
    "            if gt_report[\"Result\"] == []:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # print(f\"Input: {input}\")\n",
    "            \n",
    "            gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "            cand_results = cand_results[0]\n",
    "\n",
    "            gt_rows = []\n",
    "            for gt_result in gt_results:\n",
    "                gt_rows.extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_rows = set(gt_rows)\n",
    "            gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "            cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "            gt_cols.remove(\"id\")\n",
    "            cand_cols.remove(\"id\")\n",
    "            # gt_cols.remove(\"idu\")\n",
    "            cand_cols.remove(\"idu\")\n",
    "\n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "\n",
    "eval_df = eval_query_gtgt(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/r-v7_r8_a16_ours_8bit-checkpoint-194.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
