{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:db.instance:Connected to the database PerSite_DB\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"TruePositive\"\n",
    "    false_positive = \"FalsePositive\"\n",
    "    false_negative = \"FalseNegative\"\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result\n",
    "\n",
    "def run_query_and_get_report(input, tags, metadata, scenario, instruction_set):\n",
    "    input_report = {}\n",
    "    input_report[\"Input\"] = input\n",
    "    input_report[\"Tags\"] = tags\n",
    "    input_report[\"Scenario\"] = scenario\n",
    "    input_report[\"Result\"] = []\n",
    "    variables = {\n",
    "        \"Metadata\": metadata\n",
    "    }\n",
    "    print(input)\n",
    "    for instruction in instruction_set:\n",
    "        i_type = instruction[\"type\"]\n",
    "        if i_type == \"q\":\n",
    "            # query\n",
    "            args = instruction[\"args\"]\n",
    "            result_var_name = instruction[\"result_name\"]\n",
    "            # print(f\"Query: {args}, {result_var_name}\")\n",
    "            if \"temporal\" in args:\n",
    "                del args[\"table_name\"]\n",
    "                args[\"metadata\"] = metadata\n",
    "                result_df = DBManager.structured_query_data_t(args, get_rowids=True)\n",
    "            else:\n",
    "                result_df = DBManager.structured_query(args, get_rowids=True)\n",
    "            # print(f\"Result:\\n{result_df}\")\n",
    "            try:\n",
    "                if \"timestamp\" in result_df.columns:\n",
    "                    try:\n",
    "                        timestamp = result_df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    except Exception as e:\n",
    "                        print(args)\n",
    "                        print(result_df[\"timestamp\"])\n",
    "                result = result_df.to_dict(orient=\"index\")\n",
    "                cols = list(result_df.columns)\n",
    "                result = [[row[col] for col in cols] for row in result.values()]\n",
    "                input_report[\"Metadata\"] = metadata\n",
    "                input_report[\"Result\"].append({\n",
    "                    \"type\": \"q\",\n",
    "                    # \"args\": args,\n",
    "                    # \"result_name\": result_var_name,\n",
    "                    \"result_shape\": result_df.shape,\n",
    "                    \"result_columns\": cols,\n",
    "                    \"result_indices\": list(result_df[\"id\"]),\n",
    "                    # \"result\": result\n",
    "                })\n",
    "\n",
    "                # drop rows where any value is -1\n",
    "                result_df = result_df[~result_df.isin([-1]).any(axis=1)]\n",
    "\n",
    "                variables[result_var_name] = result_df\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inside: {e}\")\n",
    "                logger.error(f\"Invoked with Query: {args}, {result_var_name}\")\n",
    "        elif i_type == \"o\":\n",
    "            script, returns = instruction[\"script\"], instruction[\"returns\"]\n",
    "            variables.update(\n",
    "                OperationExecutor.execute(variables, script, returns)\n",
    "            )\n",
    "            variables_str = {}\n",
    "            k_to_track = []\n",
    "            k_to_track = [\"total_time_insec\"]\n",
    "            for k, v in variables.items():\n",
    "                if k in k_to_track:\n",
    "                    print(1, k, v, type(v))\n",
    "                # print(k, type(v))\n",
    "                type_ = None\n",
    "                while True:\n",
    "                    if type(v) in [pd.DataFrame]:\n",
    "                        v['timestamp'] = v['timestamp'].map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        v = v.to_dict(orient=\"index\")\n",
    "                        type_ = \"pd\"\n",
    "                        break\n",
    "                    \n",
    "                    # pd.Index\n",
    "                    elif type(v) in [pd.Index, np.ndarray, pd.Series]:\n",
    "                        if len(v) == 0:\n",
    "                            v = v.tolist()\n",
    "                            type_ = \"primitive\"\n",
    "                            continue\n",
    "                        # if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                        #     # v = [x.strftime(\"%Y-%m-%d %H:%M:%S\") for x in v]\n",
    "                        # elif type(v[0]) in [np.int64, np.float64, np.bool]:\n",
    "                        #     v = [x.item() for x in v]\n",
    "                        # break\n",
    "\n",
    "                        # if type(v) == np.ndarray:\n",
    "                        #     v = pd.Series(v)\n",
    "                        if type(v) in [pd.Series]:\n",
    "                            v.reset_index(drop=True, inplace=True)\n",
    "                        \n",
    "                        if k in k_to_track:\n",
    "                            print(2, k, v[0], type(v[0]))\n",
    "                            # print(2, k, v)\n",
    "\n",
    "                        v = pd.unique(v)\n",
    "                        v = pd.Series(v)\n",
    "                        if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                            v = v.map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        # v = v.to_dict()\n",
    "                        v = v.tolist()\n",
    "                        # remove -1 in the list\n",
    "                        v = [x for x in v if x not in [-1, np.nan]]\n",
    "                        if len(v) > 5:\n",
    "                            v = v[:5]\n",
    "                        # v = v.to_dict()\n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    \n",
    "                    elif type(v) in [np.int64, np.float64, np.bool]:\n",
    "                        v = v.item()\n",
    "                    elif type(v) in [np.datetime64]:\n",
    "                        v = pd.Timestamp(v)\n",
    "                        if k in k_to_track:\n",
    "                            print(3, k, v, type(v))\n",
    "                    elif type(v) in [pd.Timestamp, datetime.date, datetime.datetime]:\n",
    "                        v = v.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    elif type(v) in [int, float, bool, str, list, dict]:\n",
    "                        if type(v) in [int, float]:\n",
    "                            if v in [-1, np.nan]:\n",
    "                                v = None\n",
    "                        elif type(v) in [list, dict]:\n",
    "                            if len(v) == 0:\n",
    "                                v = None\n",
    "                            \n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Type not handled: {k}: {type(v), v}\")\n",
    "                        type_ = \"unknown\"\n",
    "                        break\n",
    "                variables_str[k] = (type_, v)\n",
    "                # print(k, v)\n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"o\",\n",
    "                \"script\": script,\n",
    "                \"returns\": {k: variables_str[k] for k in returns}\n",
    "            })\n",
    "\n",
    "            for k in variables_str:\n",
    "                if k in k_to_track:\n",
    "                    print(k, variables_str[k])\n",
    "        elif i_type == \"r\":\n",
    "            force = False\n",
    "\n",
    "            type_os = [r for r in input_report[\"Result\"] if r[\"type\"] == \"o\"]\n",
    "            returns = [r[\"returns\"] for r in type_os]\n",
    "            variables = {}\n",
    "            for r in returns:\n",
    "                variables.update(r)\n",
    "\n",
    "            values = variables.values()\n",
    "            values_has_no_value = any([v[1] is None for v in values])\n",
    "            if len(variables) == 0 or values_has_no_value:\n",
    "                force = True\n",
    "                if values_has_no_value:\n",
    "                    instruction[\"expectations\"] = [\"관련 데이터를 찾을 수 없습니다.\"]\n",
    "            \n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"r\",\n",
    "                \"expectations\": instruction[\"expectations\"],\n",
    "                \"force\": force\n",
    "            })\n",
    "        elif i_type == \"g\":\n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"g\",\n",
    "                \"args\": instruction[\"args\"]\n",
    "            })\n",
    "    return input_report\n",
    "\n",
    "def build_query_groundtruth(dateset_name):\n",
    "    ds_ts = []\n",
    "    dt_tr = []\n",
    "    base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dateset_name}\")\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir() and \"scenario\" in directory.name:\n",
    "            ts = read_json(f\"{directory}/onlyq_ts.json\")\n",
    "            if \"v7\" in dateset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            for i, d in enumerate(ts):\n",
    "                ts[i][\"Scenario\"] = directory.name\n",
    "                if \"v7\" in dateset_name:\n",
    "                    ts[i][\"Metadata\"] = metadata\n",
    "            ds_ts.extend(ts)\n",
    "            \n",
    "\n",
    "            tr = read_json(f\"{directory}/onlyq_tr.json\")\n",
    "            for i, d in enumerate(tr):\n",
    "                tr[i][\"Scenario\"] = directory.name\n",
    "                if \"v7\" in dateset_name:\n",
    "                    tr[i][\"Metadata\"] = metadata\n",
    "            dt_tr.extend(tr)\n",
    "    \n",
    "    ds = ds_ts + dt_tr\n",
    "    print(len(ds))\n",
    "    \n",
    "    if \"v7\" in dateset_name:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "    else:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "        metadata = None\n",
    "    \n",
    "    with open(db_gt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[\")\n",
    "        with tqdm(total=len(ds)) as pbar:\n",
    "            for d in ds:\n",
    "                pbar.set_description(f\"Processing {d['Input']}\")\n",
    "                # print(\"--\")\n",
    "                \n",
    "                input = d[\"Input\"]\n",
    "                # if not \"time\" in input:\n",
    "                #     continue\n",
    "                # print(f\"Input: {input}\")\n",
    "                scenario = d[\"Scenario\"]\n",
    "                tags = d[\"Tags\"]\n",
    "                \n",
    "                metadata = d[\"Metadata\"]\n",
    "                response = d[\"Response\"]\n",
    "                # instruction_set = response[\"Instruction Set\"]\n",
    "                instruction_set = response[\"Instructions\"]\n",
    "                # print(f\"Instruction Set: {type(instruction_set)}, {len(instruction_set)}\")\n",
    "                instruction_set.append({\n",
    "                    \"type\": \"r\",\n",
    "                    \"expectations\": response[\"Expectations\"]\n",
    "                })\n",
    "                input_report = run_query_and_get_report(input, tags, metadata, scenario, instruction_set)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # print(input_report)\n",
    "                    # del input_report[\"Metadata\"]\n",
    "                    f.write(json.dumps(input_report, ensure_ascii=False) + \",\\n\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error outside: {e}\")\n",
    "                    logger.error(f\"Invoked with Input: {input}\")\n",
    "                    logger.error(f\"Input Report: {input_report}\")\n",
    "                    # exit()\n",
    "                    raise e\n",
    "                \n",
    "                # print(\"\\n\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "        # make it json array format\n",
    "        # remove last comma\n",
    "        f.seek(f.tell() - 2, 0)\n",
    "        f.write(\"]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_query_groundtruth(\"v5-250228-multimetadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_query(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            gt_report = gt_report[0]\n",
    "            if gt_report[\"Result\"] == []:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Scenario\"] = scenario\n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict) and (\"Instruction Set\" in cand_response[\"Candidate\"] or \"지시\" in cand_response[\"Candidate\"] or \"Instructions\" in cand_response[\"Candidate\"]):\n",
    "                if \"Instruction Set\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instruction Set\"]\n",
    "                elif \"지시\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"지시\"]\n",
    "                elif \"Instructions\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instructions\"]\n",
    "\n",
    "                evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                try:\n",
    "                    import re\n",
    "                    # get data between \"Instruction Set\": [ and the last]\n",
    "                    cand_instruction_set = re.search(r'(?<=\"Instruction Set\": \\[)(.*)(?=\\])', cand_response[\"Candidate\"], re.DOTALL).group(0)\n",
    "                    # find all {\"type\": ~ }, {\"type\": ~ }, {\"type\": ~ }\n",
    "                    cand_instruction_set = re.findall(r'({\"type\".*?})', cand_instruction_set)\n",
    "                    # print(list(cand_instruction_set))\n",
    "                    cand_instruction_set = [eval(d) for d in cand_instruction_set]\n",
    "                except Exception as e:\n",
    "                    evaluation_report[EM.json_structure] = False\n",
    "                    print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                    print(e)\n",
    "                    evaluation_reports.append(evaluation_report)\n",
    "                    pbar.update(1)\n",
    "                    print(evaluation_report)\n",
    "                    continue\n",
    "                    \n",
    "            cand_report = run_query_and_get_report(input, metadata, scenario, cand_instruction_set) \n",
    "            \n",
    "            # print(f\"Input: {input}\")\n",
    "            \n",
    "            gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "\n",
    "            if len(cand_results) == 0:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                # evaluation_report[EM.true_positive] = 0\n",
    "                # evaluation_report[EM.false_positive] = false_positive\n",
    "                # evaluation_report[EM.false_negative] = false_negative\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                # print(evaluation_report)\n",
    "                            \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            cand_results = cand_results[0]\n",
    "\n",
    "            gt_rows = []\n",
    "            for gt_result in gt_results:\n",
    "                gt_rows.extend(gt_result[\"result_indices\"])\n",
    "            \n",
    "            gt_rows = set(gt_rows)\n",
    "            gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "            cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "            gt_cols.remove(\"id\")\n",
    "            cand_cols.remove(\"id\")\n",
    "            gt_cols.remove(\"idu\")\n",
    "            try:\n",
    "                cand_cols.remove(\"idu\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "\n",
    "\n",
    "            # print(len(gt_flatten), len(cand_flatten))\n",
    "            \n",
    "            # gt_counter = Counter(gt_flatten)\n",
    "            # cand_counter = Counter(cand_flatten)\n",
    "\n",
    "            # true_positive = sum(min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_negative = sum(gt_counter[item] - min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_positive = sum(cand_counter[item] - min(cand_counter[item], gt_counter.get(item, 0)) for item in cand_counter)\n",
    "            \n",
    "            # # check if all gt results are in cand results\n",
    "            # true_positive, false_positive, false_negative = 0, 0, 0\n",
    "            # for gt_data in gt_flatten:\n",
    "            #     try:\n",
    "            #         cand_flatten.remove(gt_data)\n",
    "            #         true_positive += 1\n",
    "            #     except ValueError as e:\n",
    "            #         false_negative += 1\n",
    "            \n",
    "            # false_positive = len(cand_flatten)\n",
    "            \n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            \n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WoAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r128_a256_woall-checkpoint-60\"\n",
    "cand_response_filename = \"r-v7_r8_a16_woall_4bit-checkpoint-97\"\n",
    "# cand_response_filename = \"r-v5_r32_a64_woall-checkpoint-70-batch\"\n",
    "# cand_response_filename = \"r-v6_r64_a128_woall_shorten-checkpoint-53\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "# print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"r-v5_r256_a512_FI-checkpoint-43-batch\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r256_a512_ISP-checkpoint-104\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-v5_r256_a512_ours-checkpoint-20\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours-checkpoint-52-batch\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours_noexample-checkpoint-50-batch\"\n",
    "# cand_response_filename = \"r-v6_r128_a256_ours-checkpoint-52\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours-checkpoint-40\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours_shorten-checkpoint-30\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 우리반과 옆반 온도 변화 추이 비교해줘:   4%|▍         | 3/72 [00:00<00:02, 32.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is our classroom so cold\n",
      "오늘 아침과 저녁의 온도차이는 얼마나 돼?\n",
      "지금 옆반 온도랑 우리반 온도 알려줘\n",
      "이번주 우리반과 옆반 온도 변화 추이 비교해줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 설정온도가 실내온도보다 더 낮았던 날은?:   8%|▊         | 6/72 [00:00<00:06,  9.74it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 설정온도랑 실내온도 차이 알려줘.\n",
      "지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?\n",
      "이번주 설정온도가 실내온도보다 더 낮았던 날은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 최근에 설정온도가 가장 높았던 날 알려줘:  11%|█         | 8/72 [00:00<00:06, 10.22it/s]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어제 전원 껐어?\n",
      "최근에 설정온도가 가장 높았던 날 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 올해 여름 우리반 실내온도 최대값과 최소값 알려줘:  19%|█▉        | 14/72 [00:01<00:05, 10.30it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "화성의 설정온도 확인해줘\n",
      "올해 경상수지는?\n",
      "지난 3일 동안 우리반 실내 온도 평균 값 알려줘.\n",
      "3층 평균 실내온도는?\n",
      "오늘 오후 5시에 옆반의 설정온도는 어땠어?\n",
      "올해 여름 우리반 실내온도 최대값과 최소값 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 작년 겨울 우리반 평균온도 알려줘:  28%|██▊       | 20/72 [00:01<00:03, 13.44it/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음주 우리반 실내온도 어떨거 같아?\n",
      "지금 우리반 너무 더워\n",
      "지금 몇시야?\n",
      "어제 우리반과 옆반의 설정온도 차이 알려줘\n",
      "오늘 우리반과 옆반의 평균 온도차이 알려줘\n",
      "작년 겨울 우리반 평균온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 올해 여름 앞반 평균온도 알려줘:  29%|██▉       | 21/72 [00:02<00:03, 13.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올해 여름 앞반 평균온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 올해 봄 옆반 제일 추웠던 날 알려줘:  31%|███       | 22/72 [00:03<00:10,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올해 봄 옆반 제일 추웠던 날 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4월 앞반 평균온도 알려줘:  32%|███▏      | 23/72 [00:04<00:10,  4.61it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4월 앞반 평균온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번달 중 우리반 온도가 가장 덜 더운날이 언제야?:  33%|███▎      | 24/72 [00:04<00:15,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번달 중 우리반 온도가 가장 덜 더운날이 언제야?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2주전 우리반과 옆반 합쳐서 설정온도가 가장 낮은날이 언제야?:  35%|███▍      | 25/72 [00:05<00:16,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2주전 우리반과 옆반 합쳐서 설정온도가 가장 낮은날이 언제야?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 8일전 설정온도는?:  49%|████▊     | 35/72 [00:05<00:09,  3.83it/s]                          3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번달 중 뒷반 온도가 가장 더운날이 언제야?\n",
      "지난주 설정온도 변화 추이 보여줘\n",
      "우리반의 가장 최근 설정 온도 알려줘\n",
      "옆반의 가장 최근 온도랑 설정온도 알려줘\n",
      "지금 옆반 너무 추워\n",
      "우리반 왜이리 덥노\n",
      "What time is it now?\n",
      "What is the current temperature in our class?\n",
      "내일 실내온도 어떨거 같아?\n",
      "8일전 설정온도는?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 우리반 이번달 제일 추웠던 날은 언제냐?:  58%|█████▊    | 42/72 [00:05<00:02, 13.34it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10년 전 오늘 우리반 온도는?\n",
      "{'columns': ['roomtemp'], 'temporal': \"[DATE_TRUNC('day', DATE 'CURRENT_DATE' - INTERVAL '10 year'), DATE_TRUNC('day', DATE 'CURRENT_DATE' - INTERVAL '10 year' + INTERVAL '1 day'))\", 'spatials': ['01_IB5'], 'metadata': {'site_name': 'YongDongIllHighSchool', 'user_name': '홍길동', 'user_role': 'customer', 'idu_name': '01_IB5', 'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도'], 'oper': ['전원']}, 'current_datetime': '2022-09-30 12:00:00'}}\n",
      "Series([], Name: timestamp, dtype: float64)\n",
      "롯데캐슬의 현재 온도 알려줘\n",
      "다음 대통령 선거는 언제인가요?\n",
      "1층 평균 실내온도 알려줘\n",
      "에어컨 꺼진 방들 알려줘\n",
      "지난주 설정온도가 실내온도보다 더 높았던 날은?\n",
      "우리반 이번달 제일 추웠던 날은 언제냐?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 우리반과 옆반 이번주 설정온도 차이 보여줘:  60%|█████▉    | 43/72 [00:06<00:02, 13.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리반과 옆반 이번주 설정온도 차이 보여줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 작년 옆반 가장 더웠던 달은?:  67%|██████▋   | 48/72 [00:06<00:02,  8.75it/s]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어제 우리반 에어컨 작동 시간 알려줘\n",
      "우리반, 옆반, 앞반 중 가장 추운 방은?\n",
      "농담 좀 해줘\n",
      "에어켠 제일 세게 킨 방 3개 알려줘\n",
      "작년 옆반 가장 더웠던 달은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난달에 설정온도랑 현재온도 차이가 제일 많이 났던 때는 언제야?:  68%|██████▊   | 49/72 [00:10<00:08,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난달에 설정온도랑 현재온도 차이가 제일 많이 났던 때는 언제야?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난 한달간 설정온도 평균을 알려줘.:  69%|██████▉   | 50/72 [00:11<00:07,  2.87it/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난 한달간 설정온도 평균을 알려줘.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 우리반 평균 온도 알려줘:  82%|████████▏ | 59/72 [00:12<00:03,  4.14it/s]                                                            2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the temperature difference between our class and the adjacent class right now?\n",
      "지금 에너지 사용량 알려줘\n",
      "앞반 전원 켜져있어?\n",
      "오늘 우리반 에어컨 전원이 처음 켜진 시각 알려줘.\n",
      "이번주 경제 소식 알려줘.\n",
      "지난주 우리반은 대체로 몇 도로 설정되어 있었나요?\n",
      "옆반 어제 아침과 저녁의 설정온도 차이는 얼마나 돼?\n",
      "지금 옆반 에어컨 상태 알려줘\n",
      "이번주 우리반 평균 온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번달 우리반 설정온도 최대값과 최소값 알려줘:  83%|████████▎ | 60/72 [00:12<00:02,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번달 우리반 설정온도 최대값과 최소값 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing How hot was our class over the last month?:  89%|████████▉ | 64/72 [00:12<00:01,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리반과 옆반중 더 추운곳은 어디야?\n",
      "에어컨 켜져있어?\n",
      "내일 우리반과 앞반 온도 차이 알려줘\n",
      "How hot was our class over the last month?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 3주전 우리반 평균 실내온도 알려줘:  93%|█████████▎| 67/72 [00:13<00:00,  5.40it/s]               .82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리 반 에어컨이 가장 오래 작동했던 날은 언제야?\n",
      "3주전 우리반 평균 실내온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 오늘 오후 2시에 옆반의 설정온도는 어땠어?: 100%|██████████| 72/72 [00:13<00:00,  5.25it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난달 오늘 오후 2시에 옆반의 설정온도는 어땠어?\n",
      "오늘 오후 4시부터 6시까지 실내온도 평균 알려줘\n",
      "지난주 수요일 실내온도 최고값 알려줘\n",
      "오늘 오전 11시에 옆반의 실내온도는 어땠어?\n",
      "오늘 오후 2시에 옆반의 설정온도는 어땠어?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "build_query_groundtruth(\"v7-250309-reduceinputanddatefunctioncall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_query_gtgt(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_report in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_report['Input']}\")\n",
    "            input = cand_report[\"Input\"]\n",
    "            scenario = cand_report[\"Scenario\"]\n",
    "\n",
    "\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            \n",
    "            \n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            gt_report = gt_report[0]\n",
    "            if gt_report[\"Result\"] == []:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # print(f\"Input: {input}\")\n",
    "            \n",
    "            gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "            cand_results = cand_results[0]\n",
    "\n",
    "            gt_rows = []\n",
    "            for gt_result in gt_results:\n",
    "                gt_rows.extend(gt_result[\"result_indices\"])\n",
    "\n",
    "            gt_rows = set(gt_rows)\n",
    "            gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "            cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "            gt_cols.remove(\"id\")\n",
    "            cand_cols.remove(\"id\")\n",
    "            # gt_cols.remove(\"idu\")\n",
    "            cand_cols.remove(\"idu\")\n",
    "\n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "\n",
    "eval_df = eval_query_gtgt(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/r-v7_r8_a16_ours_8bit-checkpoint-194.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
