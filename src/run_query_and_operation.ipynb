{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from db.manager import DBManager\n",
    "from operation.execute import OperationExecutor\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    json_structure = \"JsonStructureCorrectness\"\n",
    "    true_positive = \"TruePositive\"\n",
    "    false_positive = \"FalsePositive\"\n",
    "    false_negative = \"FalseNegative\"\n",
    "    semantic_true_positive = \"SemanticTruePositive\"\n",
    "    semantic_false_positive = \"SemanticFalsePositive\"\n",
    "    semantic_false_negative = \"SemanticFalseNegative\"\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result\n",
    "\n",
    "def run_query_and_get_report(input, tags, metadata, scenario, instruction_set):\n",
    "    input_report = {}\n",
    "    input_report[\"Input\"] = input\n",
    "    input_report[\"Tags\"] = tags\n",
    "    input_report[\"Scenario\"] = scenario\n",
    "    input_report[\"Result\"] = []\n",
    "    variables = {\n",
    "        \"Metadata\": metadata\n",
    "    }\n",
    "    print(input)\n",
    "    for instruction in instruction_set:\n",
    "        i_type = instruction[\"type\"]\n",
    "        if i_type == \"q\":\n",
    "            # query\n",
    "            args = instruction[\"args\"]\n",
    "            result_var_name = instruction[\"result_name\"]\n",
    "            # print(f\"Query: {args}, {result_var_name}\")\n",
    "            if \"temporal\" in args:\n",
    "                del args[\"table_name\"]\n",
    "                args[\"metadata\"] = metadata\n",
    "                result_df = DBManager.structured_query_data_t(args, get_rowids=True)\n",
    "            else:\n",
    "                result_df = DBManager.structured_query(args, get_rowids=True)\n",
    "            # print(f\"Result:\\n{result_df}\")\n",
    "            try:\n",
    "                if \"timestamp\" in result_df.columns:\n",
    "                    try:\n",
    "                        timestamp = result_df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    except Exception as e:\n",
    "                        print(args)\n",
    "                        print(result_df[\"timestamp\"])\n",
    "                result = result_df.to_dict(orient=\"index\")\n",
    "                cols = list(result_df.columns)\n",
    "                result = [[row[col] for col in cols] for row in result.values()]\n",
    "                input_report[\"Metadata\"] = metadata\n",
    "                input_report[\"Result\"].append({\n",
    "                    \"type\": \"q\",\n",
    "                    \"args\": args,\n",
    "                    # \"result_name\": result_var_name,\n",
    "                    \"result_shape\": result_df.shape,\n",
    "                    \"result_columns\": cols,\n",
    "                    \"result_indices\": list(result_df[\"id\"]),\n",
    "                    # \"result\": result\n",
    "                })\n",
    "\n",
    "                # drop rows where any value is -1\n",
    "                result_df = result_df[~result_df.isin([-1]).any(axis=1)]\n",
    "\n",
    "                variables[result_var_name] = result_df\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inside: {e}\")\n",
    "                logger.error(f\"Invoked with Query: {args}, {result_var_name}\")\n",
    "        elif i_type == \"o\":\n",
    "            script, returns = instruction[\"script\"], instruction[\"returns\"]\n",
    "            try:\n",
    "                variables.update(\n",
    "                    OperationExecutor.execute(variables, script, returns)\n",
    "                )\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error inside: {e}\")\n",
    "                logger.error(f\"Invoked with Script: {script}, Returns: {returns}\")\n",
    "                input_report[\"Result\"].append({\n",
    "                    \"type\": \"o\",\n",
    "                    \"script\": script,\n",
    "                    \"returns\": {k: None for k in returns}\n",
    "                })\n",
    "                \n",
    "                continue\n",
    "            variables_str = {}\n",
    "            k_to_track = []\n",
    "            k_to_track = [\"total_time_insec\"]\n",
    "            for k, v in variables.items():\n",
    "                if k in k_to_track:\n",
    "                    print(1, k, v, type(v))\n",
    "                # print(k, type(v))\n",
    "                type_ = None\n",
    "                while True:\n",
    "                    if type(v) in [pd.DataFrame]:\n",
    "                        # sort by timestamp\n",
    "                        if \"timestamp\" in v.columns:\n",
    "                            v = v.sort_values(by=\"timestamp\")\n",
    "                        \n",
    "                        v['timestamp'] = v['timestamp'].map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        v = v.to_dict(orient=\"index\")\n",
    "                        type_ = \"pd\"\n",
    "                        break\n",
    "                    \n",
    "                    # pd.Index\n",
    "                    elif type(v) in [pd.Index, np.ndarray, pd.Series]:\n",
    "                        if len(v) == 0:\n",
    "                            v = v.tolist()\n",
    "                            type_ = \"primitive\"\n",
    "                            continue\n",
    "                        # if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                        #     # v = [x.strftime(\"%Y-%m-%d %H:%M:%S\") for x in v]\n",
    "                        # elif type(v[0]) in [np.int64, np.float64, np.bool]:\n",
    "                        #     v = [x.item() for x in v]\n",
    "                        # break\n",
    "\n",
    "                        # if type(v) == np.ndarray:\n",
    "                        #     v = pd.Series(v)\n",
    "                        if type(v) in [pd.Series]:\n",
    "                            v.reset_index(drop=True, inplace=True)\n",
    "                        \n",
    "                        if k in k_to_track:\n",
    "                            print(2, k, v[0], type(v[0]))\n",
    "                            # print(2, k, v)\n",
    "\n",
    "                        v = pd.unique(v)\n",
    "                        v = pd.Series(v)\n",
    "                        if type(v[0]) in [pd.Timestamp, datetime.date, datetime.datetime, np.datetime64]:\n",
    "                            v = v.map(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                        # v = v.to_dict()\n",
    "                        v = v.tolist()\n",
    "                        # remove -1 in the list\n",
    "                        v = [x for x in v if x not in [-1, np.nan]]\n",
    "                        if len(v) > 5:\n",
    "                            v = v[:5]\n",
    "                        # v = v.to_dict()\n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    \n",
    "                    elif type(v) in [np.int64, np.float64, np.bool]:\n",
    "                        v = v.item()\n",
    "                    elif type(v) in [np.datetime64]:\n",
    "                        v = pd.Timestamp(v)\n",
    "                        if k in k_to_track:\n",
    "                            print(3, k, v, type(v))\n",
    "                    elif type(v) in [pd.Timestamp, datetime.date, datetime.datetime]:\n",
    "                        v = v.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    elif type(v) in [int, float, bool, str, list, dict]:\n",
    "                        if type(v) in [int, float]:\n",
    "                            if v in [-1, np.nan]:\n",
    "                                v = None\n",
    "                        elif type(v) in [list, dict]:\n",
    "                            if len(v) == 0:\n",
    "                                v = None\n",
    "                            \n",
    "                        type_ = \"primitive\"\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Type not handled: {k}: {type(v), v}\")\n",
    "                        type_ = \"unknown\"\n",
    "                        break\n",
    "                variables_str[k] = (type_, v)\n",
    "                # print(k, v)\n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"o\",\n",
    "                \"script\": script,\n",
    "                \"returns\": {k: variables_str[k] for k in returns}\n",
    "            })\n",
    "\n",
    "            for k in variables_str:\n",
    "                if k in k_to_track:\n",
    "                    print(k, variables_str[k])\n",
    "                    \n",
    "        elif i_type == \"r\":\n",
    "            force = False\n",
    "\n",
    "            type_os = [r for r in input_report[\"Result\"] if r[\"type\"] == \"o\"]\n",
    "            returns = [r[\"returns\"] for r in type_os]\n",
    "            variables = {}\n",
    "\n",
    "            for r in returns:\n",
    "                variables.update(r)\n",
    "\n",
    "            values = variables.values()\n",
    "            values_has_no_value = any([v[1] is None for v in values])\n",
    "            if len(variables) == 0 or values_has_no_value:\n",
    "                force = True\n",
    "                if values_has_no_value:\n",
    "                    instruction[\"expectations\"] = [\"관련 데이터를 찾을 수 없습니다.\"]\n",
    "            \n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"r\",\n",
    "                \"expectations\": instruction[\"expectations\"],\n",
    "                \"force\": force\n",
    "            })\n",
    "        elif i_type == \"g\":\n",
    "            input_report[\"Result\"].append({\n",
    "                \"type\": \"g\",\n",
    "                \"args\": instruction[\"args\"]\n",
    "            })\n",
    "    return input_report\n",
    "\n",
    "def build_query_groundtruth(dateset_name):\n",
    "    def read(path):\n",
    "        data = read_json(path)\n",
    "        for i, d in enumerate(data):\n",
    "            data[i][\"Scenario\"] = directory.name\n",
    "            if \"v7\" in dateset_name:\n",
    "                data[i][\"Metadata\"] = metadata\n",
    "        return data\n",
    "\n",
    "    ds_ts = []\n",
    "    dt_tr = []\n",
    "    base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dateset_name}\")\n",
    "    \n",
    "    for directory in base_dataset_dir.iterdir():\n",
    "        if directory.is_dir() and \"scenario\" in directory.name:\n",
    "            if \"v7\" in dateset_name:\n",
    "                metadata = read_json(f\"{directory}/metadata.json\")\n",
    "            \n",
    "            ds_ts.extend(read(f\"{directory}/onlyq_ts.json\"))\n",
    "            dt_tr.extend(read(f\"{directory}/onlyq_tr.json\"))\n",
    "    \n",
    "    ds = ds_ts + dt_tr\n",
    "    \n",
    "    if \"v7\" in dateset_name:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "    else:\n",
    "        db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "        metadata = None\n",
    "    \n",
    "    with open(db_gt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"[\")\n",
    "        with tqdm(total=len(ds)) as pbar:\n",
    "            for d in ds:\n",
    "                pbar.set_description(f\"Processing {d['Input']}\")\n",
    "                # print(\"--\")\n",
    "                \n",
    "                input = d[\"Input\"]\n",
    "                # if not \"time\" in input:\n",
    "                #     continue\n",
    "                # print(f\"Input: {input}\")\n",
    "                scenario = d[\"Scenario\"]\n",
    "                tags = d[\"Tags\"]\n",
    "                \n",
    "                metadata = d[\"Metadata\"]\n",
    "                response = d[\"Response\"]\n",
    "                # instruction_set = response[\"Instruction Set\"]\n",
    "                instruction_set = response[\"Instructions\"]\n",
    "                # print(f\"Instruction Set: {type(instruction_set)}, {len(instruction_set)}\")\n",
    "                instruction_set.append({\n",
    "                    \"type\": \"r\",\n",
    "                    \"expectations\": response[\"Expectations\"]\n",
    "                })\n",
    "                input_report = run_query_and_get_report(input, tags, metadata, scenario, instruction_set)\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    # print(input_report)\n",
    "                    # del input_report[\"Metadata\"]\n",
    "                    f.write(json.dumps(input_report, ensure_ascii=False) + \",\\n\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error outside: {e}\")\n",
    "                    logger.error(f\"Invoked with Input: {input}\")\n",
    "                    logger.error(f\"Input Report: {input_report}\")\n",
    "                    # exit()\n",
    "                    raise e\n",
    "                \n",
    "                # print(\"\\n\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "        # make it json array format\n",
    "        # remove last comma\n",
    "        f.seek(f.tell() - 2, 0)\n",
    "        f.write(\"]\")\n",
    "\n",
    "def eval_query(db_gt_filename, cand_response_filename):\n",
    "    db_gts = read_json(db_gt_filename)\n",
    "    cand_responses = read_json(cand_response_filename)\n",
    "    # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "    evaluation_reports = []\n",
    "\n",
    "    with tqdm(total=len(cand_responses)) as pbar:\n",
    "        for cand_response in cand_responses:\n",
    "            pbar.set_description(f\"Processing {cand_response['Input']}\")\n",
    "            input = cand_response[\"Input\"]\n",
    "            scenario = cand_response[\"Scenario\"]\n",
    "\n",
    "            # if \"오늘 아침과 저녁\" not in input:\n",
    "            #     continue\n",
    "\n",
    "            if \"Metadata\" in cand_response:\n",
    "                metadata = cand_response[\"Metadata\"]\n",
    "            else:\n",
    "                # metadata = metadata_\n",
    "                metadata = None\n",
    "            # 관계 없는 질문들은 건너뛰자\n",
    "            gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            assert len(gt_report) <= 1\n",
    "            if len(gt_report) == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            gt_report = gt_report[0]\n",
    "            assert gt_report[\"Result\"] != []\n",
    "            # if gt_report[\"Result\"] == []:\n",
    "            #     pbar.update(1)\n",
    "            #     continue\n",
    "            \n",
    "            gt_results = [d for d in gt_report[\"Result\"] if d[\"type\"] == \"q\"]\n",
    "            if len(gt_results) != 0:\n",
    "                gt_args = gt_results[0][\"args\"]\n",
    "\n",
    "                # Assume all cols and spatials are same across all queries\n",
    "                gt_semantic_cols = gt_args[\"columns\"]\n",
    "                gt_semantic_spatials = gt_args[\"spatials\"]\n",
    "                gt_semantics = gt_semantic_cols + gt_semantic_spatials\n",
    "                \n",
    "                gt_rows = []\n",
    "                for gt_result in gt_results:\n",
    "                    gt_rows.extend(gt_result[\"result_indices\"])\n",
    "                gt_rows = set(gt_rows)\n",
    "                gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "                gt_cols.remove(\"id\")\n",
    "                gt_cols.remove(\"idu\")\n",
    "                gt_total_combinations = len(gt_cols) * len(gt_rows)\n",
    "            else:\n",
    "                gt_total_combinations = 0\n",
    "                gt_semantics = []\n",
    "\n",
    "            # ---\n",
    "            \n",
    "            evaluation_report = defaultdict(lambda: None)\n",
    "            evaluation_report[\"Input\"] = input\n",
    "            evaluation_report[\"Scenario\"] = scenario\n",
    "            \n",
    "            if isinstance(cand_response[\"Candidate\"], dict) and (\"Instruction Set\" in cand_response[\"Candidate\"] or \"지시\" in cand_response[\"Candidate\"] or \"Instructions\" in cand_response[\"Candidate\"]):\n",
    "                if \"Instruction Set\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instruction Set\"]\n",
    "                elif \"지시\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"지시\"]\n",
    "                elif \"Instructions\" in cand_response[\"Candidate\"]:\n",
    "                    cand_instruction_set = cand_response[\"Candidate\"][\"Instructions\"]\n",
    "\n",
    "                evaluation_report[EM.json_structure] = True\n",
    "            else:\n",
    "                evaluation_report[EM.json_structure] = False\n",
    "                try:\n",
    "                    import re\n",
    "                    # get data between \"Instruction Set\": [ and the last]\n",
    "                    cand_instruction_set = re.search(r'(?<=\"Instructions\": \\[)(.*)(?=\\])', cand_response[\"Candidate\"], re.DOTALL).group(0)\n",
    "                    # find all {\"type\": ~ }, {\"type\": ~ }, {\"type\": ~ }\n",
    "                    cand_instruction_set = re.findall(r'({\"type\".*?})', cand_instruction_set)\n",
    "                    # print(list(cand_instruction_set))\n",
    "                    cand_instruction_set = [eval(d) for d in cand_instruction_set]\n",
    "                except Exception as e:\n",
    "                    evaluation_report[EM.json_structure] = False\n",
    "                    evaluation_report[EM.true_positive] = 0\n",
    "                    evaluation_report[EM.false_positive] = 0\n",
    "                    evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                    evaluation_report[EM.semantic_true_positive] = 0\n",
    "                    evaluation_report[EM.semantic_false_positive] = 0\n",
    "                    evaluation_report[EM.semantic_false_negative] = len(gt_semantics)\n",
    "\n",
    "                    print(\"Failed to parse input: \", input, cand_response[\"Candidate\"])\n",
    "                    print(e)\n",
    "                    evaluation_reports.append(evaluation_report)\n",
    "                    pbar.update(1)\n",
    "                    print(evaluation_report)\n",
    "                    continue\n",
    "            \n",
    "\n",
    "            cand_report = run_query_and_get_report(input, None, metadata, scenario, cand_instruction_set) \n",
    "            \n",
    "            cand_results = cand_report[\"Result\"]\n",
    "            cand_results = [d for d in cand_results if d[\"type\"] == \"q\"]\n",
    "\n",
    "            if len(cand_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = 0\n",
    "                evaluation_report[EM.false_negative] = gt_total_combinations\n",
    "                evaluation_report[EM.semantic_true_positive] = 0\n",
    "                evaluation_report[EM.semantic_false_positive] = 0\n",
    "                evaluation_report[EM.semantic_false_negative] = len(gt_semantics)\n",
    "                            \n",
    "                evaluation_reports.append(evaluation_report)\n",
    "                # print(evaluation_report)\n",
    "                            \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # assert len(cand_results) == 1\n",
    "\n",
    "            cand_args = [d[\"args\"] for d in cand_results if d[\"type\"] == \"q\"]\n",
    "            cand_semantic_cols = cand_args[0][\"columns\"]\n",
    "            cand_semantic_spatials = cand_args[0][\"spatials\"]\n",
    "            cand_semantics = cand_semantic_cols + cand_semantic_spatials\n",
    "\n",
    "            cand_rows = []\n",
    "            for cand_result in cand_results:\n",
    "                cand_rows.extend(cand_result[\"result_indices\"])\n",
    "\n",
    "            \n",
    "            cand_rows = set(cand_rows)\n",
    "            cand_cols = set(cand_results[0][\"result_columns\"])\n",
    "            cand_cols.remove(\"id\")\n",
    "            try:\n",
    "                cand_cols.remove(\"idu\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "            if len(gt_results) == 0:\n",
    "                evaluation_report[EM.true_positive] = 0\n",
    "                evaluation_report[EM.false_positive] = len(cand_cols) * len(cand_rows)\n",
    "                evaluation_report[EM.false_negative] = 0\n",
    "\n",
    "                evaluation_report[EM.semantic_true_positive] = 0\n",
    "                evaluation_report[EM.semantic_false_positive] = len(cand_semantics)\n",
    "                evaluation_report[EM.semantic_false_negative] = 0\n",
    "\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "            true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "            # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "            false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "            # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "            false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "\n",
    "\n",
    "            # print(len(gt_flatten), len(cand_flatten))\n",
    "            \n",
    "            # gt_counter = Counter(gt_flatten)\n",
    "            # cand_counter = Counter(cand_flatten)\n",
    "\n",
    "            # true_positive = sum(min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_negative = sum(gt_counter[item] - min(gt_counter[item], cand_counter.get(item, 0)) for item in gt_counter)\n",
    "            # false_positive = sum(cand_counter[item] - min(cand_counter[item], gt_counter.get(item, 0)) for item in cand_counter)\n",
    "            \n",
    "            # # check if all gt results are in cand results\n",
    "            # true_positive, false_positive, false_negative = 0, 0, 0\n",
    "            # for gt_data in gt_flatten:\n",
    "            #     try:\n",
    "            #         cand_flatten.remove(gt_data)\n",
    "            #         true_positive += 1\n",
    "            #     except ValueError as e:\n",
    "            #         false_negative += 1\n",
    "            \n",
    "            # false_positive = len(cand_flatten)\n",
    "            \n",
    "            gt_semantics, cand_semantics = set(gt_semantics), set(cand_semantics)\n",
    "\n",
    "            evaluation_report[EM.true_positive] = true_positive\n",
    "            evaluation_report[EM.false_positive] = false_positive\n",
    "            evaluation_report[EM.false_negative] = false_negative\n",
    "            evaluation_report[EM.semantic_true_positive] = len(gt_semantics & cand_semantics)\n",
    "            evaluation_report[EM.semantic_false_positive] = len(cand_semantics - gt_semantics)\n",
    "            evaluation_report[EM.semantic_false_negative] = len(gt_semantics - cand_semantics)\n",
    "\n",
    "            evaluation_reports.append(evaluation_report)\n",
    "            # print(evaluation_report)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    eval_df = pd.DataFrame(evaluation_reports)\n",
    "    # print(eval_df)\n",
    "\n",
    "    eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "    # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "    # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "    # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "    final_result = {}\n",
    "\n",
    "    for col in [\"JsonStructureCorrectness\", \"ExactMatch\"]:\n",
    "        # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "        final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "    # normalize per query\n",
    "    eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "    eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "    eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "    # # replace nan with 0\n",
    "    # eval_df.fillna(0, inplace=True)\n",
    "\n",
    "    # # F1 score except nans.\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    # print(f\"F1: {f1}\")\n",
    "    final_result[\"F1\"] = f1\n",
    "    final_result[\"Recall\"] = recall\n",
    "\n",
    "    eval_df[\"Semantic_ExactMatch\"] = eval_df.apply(lambda x: x[EM.semantic_false_positive] == 0 and x[EM.semantic_false_negative] == 0, axis=1).astype(int)\n",
    "    final_result[\"Semantic_ExactMatch\"] = eval_df[\"Semantic_ExactMatch\"].mean()\n",
    "\n",
    "    eval_df[\"Semantic_Total\"] = eval_df[EM.semantic_true_positive] + eval_df[EM.semantic_false_positive] + eval_df[EM.semantic_false_negative]\n",
    "    eval_df[\"Semantic_TruePositive\"] = eval_df[EM.semantic_true_positive] / eval_df[\"Semantic_Total\"]\n",
    "    eval_df[\"Semantic_FalsePositive\"] = eval_df[EM.semantic_false_positive] / eval_df[\"Semantic_Total\"]\n",
    "    eval_df[\"Semantic_FalseNegative\"] = eval_df[EM.semantic_false_negative] / eval_df[\"Semantic_Total\"]\n",
    "\n",
    "    truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.semantic_true_positive].sum(), eval_df[EM.semantic_false_positive].sum(), eval_df[EM.semantic_false_negative].sum()\n",
    "    precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "    recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    final_result[\"Semantic_F1\"] = f1\n",
    "    final_result[\"Semantic_Recall\"] = recall\n",
    "\n",
    "    for col in final_result:\n",
    "        print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "    return eval_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_query_groundtruth(\"v5-250228-multimetadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WoAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 오늘 아침과 저녁의 온도차이는 얼마나 돼?:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 아침과 저녁의 온도차이는 얼마나 돼?\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '6 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '9 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '18 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '21 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지금 옆반 온도랑 우리반 온도 알려줘:   7%|▋         | 1/15 [00:00<00:00, 18.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금 옆반 온도랑 우리반 온도 알려줘\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB7') AND timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 현재 설정온도랑 실내온도 차이 알려줘.:  13%|█▎        | 2/15 [00:00<00:00, 33.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 설정온도랑 실내온도 차이 알려줘.\n",
      "SELECT \"settemp\", \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= TIMESTAMP '2022-09-30 12:00:00' - INTERVAL '5 minutes' AND timestamp <= TIMESTAMP '2022-09-30 12:00:00' AND \"settemp\" IS NOT NULL AND \"settemp\" IS DISTINCT FROM 'NaN' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?:  20%|██        | 3/15 [00:00<00:00, 46.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?\n",
      "SELECT \"settemp\", \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('week', DATE '2022-09-30' - INTERVAL '1 week') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30') AND \"settemp\" IS NOT NULL AND \"settemp\" IS DISTINCT FROM 'NaN' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 설정온도가 실내온도보다 더 낮았던 날은?:  27%|██▋       | 4/15 [00:00<00:00, 19.82it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번주 설정온도가 실내온도보다 더 낮았던 날은?\n",
      "SELECT \"settemp\", \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('week', DATE '2022-09-30') AND timestamp < DATE_TRUNC('week', DATE '2022-09-30' + INTERVAL '1 week') AND \"settemp\" IS NOT NULL AND \"settemp\" IS DISTINCT FROM 'NaN' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 어제 전원 껐어?:  33%|███▎      | 5/15 [00:00<00:00, 19.82it/s]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어제 전원 껐어?\n",
      "SELECT \"oper\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \"oper\" IS NOT NULL AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"oper\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB7') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \"oper\" IS NOT NULL AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"oper\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '02_I81') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30' - INTERVAL '1 day') AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') AND \"oper\" IS NOT NULL AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:operation.execute:Error executing operation off_dates = daily_oper[daily_oper == False].index.strftime('%Y-%m-%d')\n",
      "ERROR:operation.execute:'Index' object has no attribute 'strftime'\n",
      "Processing 어제 전원 껐어?:  33%|███▎      | 5/15 [00:00<00:00, 14.09it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m cand_response_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr-v7_r256_a512_woall_16bit_adamw16bit_0322-checkpoint-60\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m cand_response_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcand_response_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m \u001b[43meval_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_gt_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand_response_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(eval_df)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 376\u001b[0m, in \u001b[0;36meval_query\u001b[0;34m(db_gt_filename, cand_response_filename)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;28mprint\u001b[39m(evaluation_report)\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m cand_report \u001b[38;5;241m=\u001b[39m \u001b[43mrun_query_and_get_report\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcand_instruction_set\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    378\u001b[0m cand_results \u001b[38;5;241m=\u001b[39m cand_report[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    379\u001b[0m cand_results \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m cand_results \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[50], line 76\u001b[0m, in \u001b[0;36mrun_query_and_get_report\u001b[0;34m(input, tags, metadata, scenario, instruction_set)\u001b[0m\n\u001b[1;32m     73\u001b[0m script, returns \u001b[38;5;241m=\u001b[39m instruction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m], instruction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     variables\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m---> 76\u001b[0m         \u001b[43mOperationExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError inside: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/src/operation/execute.py:27\u001b[0m, in \u001b[0;36mOperationExecutor.execute\u001b[0;34m(cls, args, python_script, returns)\u001b[0m\n\u001b[1;32m     25\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError executing operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(e)\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print([name for name in globals() if not name.startswith('_')])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# return variables named in the returns list\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28mglobals\u001b[39m()[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m returns}\n",
      "File \u001b[0;32m/workspace/src/operation/execute.py:23\u001b[0m, in \u001b[0;36mOperationExecutor.execute\u001b[0;34m(cls, args, python_script, returns)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m script \u001b[38;5;129;01min\u001b[39;00m scripts:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(script)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# if script == \"dates = daily_avg_temp[daily_avg_temp['settemp'] < daily_avg_temp['roomtemp']].index;\":\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#     print(daily_avg_temp[daily_avg_temp['settemp'] < daily_avg_temp['roomtemp']].index)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# print(script)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         exec(script, \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     25\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError executing operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Index' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r128_a256_woall-checkpoint-60\"\n",
    "cand_response_filename = \"r-v7_r256_a512_woall_16bit_adamw16bit_0322-checkpoint-60\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "# print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>FalsePositive</th>\n",
       "      <th>FalseNegative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Input, Scenario, FalsePositive, FalseNegative]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"r-v5_r256_a512_FI-checkpoint-43-batch\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-r256_a512_ISP-checkpoint-104\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "cand_response_filename = \"response-sh2orc-Llama-3.1-Korean-8B-Instruct-v5_r256_a512_ours-checkpoint-20\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours-checkpoint-52-batch\"\n",
    "# cand_response_filename = \"r-v5_r128_a256_ours_noexample-checkpoint-50-batch\"\n",
    "# cand_response_filename = \"r-v6_r128_a256_ours-checkpoint-52\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours-checkpoint-40\"\n",
    "# cand_response_filename = \"r-v6_r256_a512_ours_shorten-checkpoint-30\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/{cand_response_filename}.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"Scenario\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 우리반과 옆반 온도 변화 추이 비교해줘:   4%|▍         | 3/72 [00:00<00:01, 36.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is our classroom so cold\n",
      "오늘 아침과 저녁의 온도차이는 얼마나 돼?\n",
      "지금 옆반 온도랑 우리반 온도 알려줘\n",
      "이번주 우리반과 옆반 온도 변화 추이 비교해줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?:   8%|▊         | 6/72 [00:00<00:08,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 설정온도랑 실내온도 차이 알려줘.\n",
      "지난주에 설정온도와 실내온도 차이가 가장 많이 났던 날은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 최근에 설정온도가 가장 높았던 날 알려줘:  11%|█         | 8/72 [00:00<00:07,  8.37it/s]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번주 설정온도가 실내온도보다 더 낮았던 날은?\n",
      "어제 전원 껐어?\n",
      "최근에 설정온도가 가장 높았던 날 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 올해 여름 우리반 실내온도 최대값과 최소값 알려줘:  19%|█▉        | 14/72 [00:01<00:06,  8.47it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "화성의 설정온도 확인해줘\n",
      "올해 경상수지는?\n",
      "지난 3일 동안 우리반 실내 온도 평균 값 알려줘.\n",
      "3층 평균 실내온도는?\n",
      "오늘 오후 5시에 옆반의 설정온도는 어땠어?\n",
      "올해 여름 우리반 실내온도 최대값과 최소값 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 작년 겨울 우리반 평균온도 알려줘:  28%|██▊       | 20/72 [00:01<00:03, 13.18it/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음주 우리반 실내온도 어떨거 같아?\n",
      "지금 우리반 너무 더워\n",
      "지금 몇시야?\n",
      "어제 우리반과 옆반의 설정온도 차이 알려줘\n",
      "오늘 우리반과 옆반의 평균 온도차이 알려줘\n",
      "작년 겨울 우리반 평균온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 올해 여름 앞반 평균온도 알려줘:  29%|██▉       | 21/72 [00:02<00:03, 13.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올해 여름 앞반 평균온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 올해 봄 옆반 제일 추웠던 날 알려줘:  31%|███       | 22/72 [00:03<00:10,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올해 봄 옆반 제일 추웠던 날 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4월 앞반 평균온도 알려줘:  32%|███▏      | 23/72 [00:04<00:10,  4.72it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4월 앞반 평균온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번달 중 우리반 온도가 가장 덜 더운날이 언제야?:  33%|███▎      | 24/72 [00:04<00:15,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번달 중 우리반 온도가 가장 덜 더운날이 언제야?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2주전 우리반과 옆반 합쳐서 설정온도가 가장 낮은날이 언제야?:  35%|███▍      | 25/72 [00:05<00:16,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2주전 우리반과 옆반 합쳐서 설정온도가 가장 낮은날이 언제야?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난주 설정온도 변화 추이 보여줘:  38%|███▊      | 27/72 [00:05<00:15,  2.94it/s]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번달 중 뒷반 온도가 가장 더운날이 언제야?\n",
      "지난주 설정온도 변화 추이 보여줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난주 설정온도가 실내온도보다 더 높았던 날은?:  57%|█████▋    | 41/72 [00:05<00:08,  3.74it/s]          3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리반의 가장 최근 설정 온도 알려줘\n",
      "옆반의 가장 최근 온도랑 설정온도 알려줘\n",
      "지금 옆반 너무 추워\n",
      "우리반 왜이리 덥노\n",
      "What time is it now?\n",
      "What is the current temperature in our class?\n",
      "내일 실내온도 어떨거 같아?\n",
      "8일전 설정온도는?\n",
      "10년 전 오늘 우리반 온도는?\n",
      "{'columns': ['roomtemp'], 'temporal': \"[DATE_TRUNC('day', DATE 'CURRENT_DATE' - INTERVAL '10 year'), DATE_TRUNC('day', DATE 'CURRENT_DATE' - INTERVAL '10 year' + INTERVAL '1 day'))\", 'spatials': ['01_IB5'], 'metadata': {'site_name': 'YongDongIllHighSchool', 'user_name': '홍길동', 'user_role': 'customer', 'idu_name': '01_IB5', 'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도'], 'oper': ['전원']}, 'current_datetime': '2022-09-30 12:00:00'}}\n",
      "Series([], Name: timestamp, dtype: float64)\n",
      "롯데캐슬의 현재 온도 알려줘\n",
      "다음 대통령 선거는 언제인가요?\n",
      "1층 평균 실내온도 알려줘\n",
      "에어컨 꺼진 방들 알려줘\n",
      "지난주 설정온도가 실내온도보다 더 높았던 날은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 우리반 이번달 제일 추웠던 날은 언제냐?:  58%|█████▊    | 42/72 [00:06<00:02, 12.65it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리반 이번달 제일 추웠던 날은 언제냐?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 우리반과 옆반 이번주 설정온도 차이 보여줘:  60%|█████▉    | 43/72 [00:06<00:02, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리반과 옆반 이번주 설정온도 차이 보여줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 작년 옆반 가장 더웠던 달은?:  67%|██████▋   | 48/72 [00:07<00:02,  8.61it/s]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어제 우리반 에어컨 작동 시간 알려줘\n",
      "우리반, 옆반, 앞반 중 가장 추운 방은?\n",
      "농담 좀 해줘\n",
      "에어켠 제일 세게 킨 방 3개 알려줘\n",
      "작년 옆반 가장 더웠던 달은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난달에 설정온도랑 현재온도 차이가 제일 많이 났던 때는 언제야?:  68%|██████▊   | 49/72 [00:11<00:08,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난달에 설정온도랑 현재온도 차이가 제일 많이 났던 때는 언제야?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지난 한달간 설정온도 평균을 알려줘.:  69%|██████▉   | 50/72 [00:11<00:07,  2.76it/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난 한달간 설정온도 평균을 알려줘.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번주 우리반 평균 온도 알려줘:  82%|████████▏ | 59/72 [00:12<00:03,  4.05it/s]                                                            2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the temperature difference between our class and the adjacent class right now?\n",
      "지금 에너지 사용량 알려줘\n",
      "앞반 전원 켜져있어?\n",
      "오늘 우리반 에어컨 전원이 처음 켜진 시각 알려줘.\n",
      "이번주 경제 소식 알려줘.\n",
      "지난주 우리반은 대체로 몇 도로 설정되어 있었나요?\n",
      "옆반 어제 아침과 저녁의 설정온도 차이는 얼마나 돼?\n",
      "지금 옆반 에어컨 상태 알려줘\n",
      "이번주 우리반 평균 온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 이번달 우리반 설정온도 최대값과 최소값 알려줘:  83%|████████▎ | 60/72 [00:12<00:02,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이번달 우리반 설정온도 최대값과 최소값 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing How hot was our class over the last month?:  89%|████████▉ | 64/72 [00:13<00:01,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리반과 옆반중 더 추운곳은 어디야?\n",
      "에어컨 켜져있어?\n",
      "내일 우리반과 앞반 온도 차이 알려줘\n",
      "How hot was our class over the last month?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 3주전 우리반 평균 실내온도 알려줘:  92%|█████████▏| 66/72 [00:13<00:01,  4.84it/s]               .84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리 반 에어컨이 가장 오래 작동했던 날은 언제야?\n",
      "3주전 우리반 평균 실내온도 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 오늘 오후 2시에 옆반의 설정온도는 어땠어?: 100%|██████████| 72/72 [00:14<00:00,  5.13it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난달 오늘 오후 2시에 옆반의 설정온도는 어땠어?\n",
      "오늘 오후 4시부터 6시까지 실내온도 평균 알려줘\n",
      "지난주 수요일 실내온도 최고값 알려줘\n",
      "오늘 오전 11시에 옆반의 실내온도는 어땠어?\n",
      "오늘 오후 2시에 옆반의 설정온도는 어땠어?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "build_query_groundtruth(\"v7-250309-reduceinputanddatefunctioncall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_query_gtgt(db_gt_filename, cand_response_filename):\n",
    "#     db_gts = read_json(db_gt_filename)\n",
    "#     cand_responses = read_json(cand_response_filename)\n",
    "#     # metadata_ = read_json(f\"{BASE_DIR}/finetuning/dataset/v7-250309-reduceinputanddatefunctioncall/scenario1/metadata.json\")\n",
    "#     evaluation_reports = []\n",
    "\n",
    "#     with tqdm(total=len(cand_responses)) as pbar:\n",
    "#         for cand_report in cand_responses:\n",
    "#             pbar.set_description(f\"Processing {cand_report['Input']}\")\n",
    "#             input = cand_report[\"Input\"]\n",
    "#             scenario = cand_report[\"Scenario\"]\n",
    "\n",
    "\n",
    "#             # 관계 없는 질문들은 건너뛰자\n",
    "#             gt_report = [d for d in db_gts if d[\"Input\"] == input and d[\"Scenario\"] == scenario]\n",
    "            \n",
    "            \n",
    "#             assert len(gt_report) <= 1\n",
    "#             if len(gt_report) == 0:\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "#             gt_report = gt_report[0]\n",
    "#             if gt_report[\"Result\"] == []:\n",
    "#                 pbar.update(1)\n",
    "#                 continue\n",
    "            \n",
    "            \n",
    "#             # print(f\"Input: {input}\")\n",
    "            \n",
    "#             gt_results, cand_results = gt_report[\"Result\"], cand_report[\"Result\"]\n",
    "#             cand_results = cand_results[0]\n",
    "\n",
    "#             gt_rows = []\n",
    "#             for gt_result in gt_results:\n",
    "#                 gt_rows.extend(gt_result[\"result_indices\"])\n",
    "\n",
    "#             gt_rows = set(gt_rows)\n",
    "#             gt_cols = set(gt_results[0][\"result_columns\"])\n",
    "#             cand_cols, cand_rows = set(cand_results[\"result_columns\"]), set(cand_results[\"result_indices\"])\n",
    "\n",
    "#             gt_cols.remove(\"id\")\n",
    "#             cand_cols.remove(\"id\")\n",
    "#             # gt_cols.remove(\"idu\")\n",
    "#             cand_cols.remove(\"idu\")\n",
    "\n",
    "#             # True Positive: 공통된 컬럼과 로우의 모든 조합\n",
    "#             true_positive = len(gt_cols & cand_cols) * len(gt_rows & cand_rows)\n",
    "\n",
    "#             # Ground Truth의 총 조합에서 TP를 뺀 값이 FN\n",
    "#             false_negative = (len(gt_cols) * len(gt_rows)) - true_positive\n",
    "\n",
    "#             # Candidate의 총 조합에서 TP를 뺀 값이 FP\n",
    "#             false_positive = (len(cand_cols) * len(cand_rows)) - true_positive\n",
    "            \n",
    "#             evaluation_report = defaultdict(lambda: None)\n",
    "#             evaluation_report[EM.true_positive] = true_positive\n",
    "#             evaluation_report[EM.false_positive] = false_positive\n",
    "#             evaluation_report[EM.false_negative] = false_negative\n",
    "#             evaluation_report[\"Input\"] = input\n",
    "#             evaluation_reports.append(evaluation_report)\n",
    "#             # print(evaluation_report)\n",
    "            \n",
    "#             pbar.update(1)\n",
    "\n",
    "#     eval_df = pd.DataFrame(evaluation_reports)\n",
    "#     # print(eval_df)\n",
    "\n",
    "#     eval_df['ExactMatch'] = eval_df.apply(lambda x: x[EM.false_positive] == 0 and x[EM.false_negative] == 0, axis=1).astype(int)\n",
    "#     # eval_df['TruePositive'] = eval_df['TruePositive'].astype(int)\n",
    "#     # eval_df['FalsePositive'] = eval_df['FalsePositive'].astype(int)\n",
    "#     # eval_df['FalseNegative'] = eval_df['FalseNegative'].astype(int)\n",
    "\n",
    "#     final_result = {}\n",
    "\n",
    "#     for col in [\"ExactMatch\"]:\n",
    "#         # print(f\"{col}: {eval_df[col].mean()}\")\n",
    "#         final_result[col] = eval_df[col].mean()\n",
    "    \n",
    "#     # normalize per query\n",
    "#     eval_df[\"Total\"] = eval_df[EM.true_positive] + eval_df[EM.false_positive] + eval_df[EM.false_negative]\n",
    "#     eval_df[\"TruePositive\"] = eval_df[EM.true_positive] / eval_df[\"Total\"]\n",
    "#     eval_df[\"FalsePositive\"] = eval_df[EM.false_positive] / eval_df[\"Total\"]\n",
    "#     eval_df[\"FalseNegative\"] = eval_df[EM.false_negative] / eval_df[\"Total\"]\n",
    "\n",
    "#     # # F1 score except nans.\n",
    "#     truepos_sum, falsepos_sum, falseneg_sum = eval_df[EM.true_positive].sum(), eval_df[EM.false_positive].sum(), eval_df[EM.false_negative].sum()\n",
    "#     precision = truepos_sum / (truepos_sum + falsepos_sum)\n",
    "#     recall = truepos_sum / (truepos_sum + falseneg_sum)\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     # print(f\"F1: {f1}\")\n",
    "#     final_result[\"F1\"] = f1\n",
    "#     final_result[\"Recall\"] = recall\n",
    "#     for col in final_result:\n",
    "#         print(f\"{col}: {final_result[col]:.2f}\")\n",
    "    \n",
    "#     return eval_df\n",
    "\n",
    "# db_gt_filename = f\"{BASE_DIR}/experiments/db_gt.json\"\n",
    "# cand_response_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "\n",
    "# eval_df = eval_query_gtgt(db_gt_filename, cand_response_filename)\n",
    "# print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 오늘 아침과 저녁의 온도차이는 얼마나 돼?:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "175\n",
      "355\n",
      "오늘 아침과 저녁의 온도차이는 얼마나 돼?\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB5') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '6 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '9 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n",
      "SELECT \"roomtemp\", \"id\", \"timestamp\" FROM \"data_t\" WHERE idu_id IN (SELECT id FROM idu_t WHERE name = '01_IB7') AND timestamp >= DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '18 hours' AND timestamp < DATE_TRUNC('day', DATE '2022-09-30') + INTERVAL '21 hours' AND \"roomtemp\" IS NOT NULL AND \"roomtemp\" IS DISTINCT FROM 'NaN' AND \"id\" IS NOT NULL ORDER BY timestamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 지금 몇시야?:   7%|▋         | 1/15 [00:00<00:00, 14.28it/s]                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JsonStructureCorrectness: 1.00\n",
      "ExactMatch: 0.00\n",
      "F1: 0.51\n",
      "Recall: 0.51\n",
      "Semantic_ExactMatch: 1.00\n",
      "Semantic_F1: 1.00\n",
      "Semantic_Recall: 1.00\n",
      "                     Input   Scenario  JsonStructureCorrectness  TruePositive  \\\n",
      "0  오늘 아침과 저녁의 온도차이는 얼마나 돼?  scenario1                      True      0.339623   \n",
      "\n",
      "   FalsePositive  FalseNegative  SemanticTruePositive  SemanticFalsePositive  \\\n",
      "0       0.330189       0.330189                     2                      0   \n",
      "\n",
      "   SemanticFalseNegative  ExactMatch  Total  Semantic_ExactMatch  \\\n",
      "0                      0           0   1060                    1   \n",
      "\n",
      "   Semantic_Total  Semantic_TruePositive  Semantic_FalsePositive  \\\n",
      "0               2                    1.0                     0.0   \n",
      "\n",
      "   Semantic_FalseNegative  \n",
      "0                     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db_gt_filename = f\"{BASE_DIR}/experiments/db_gt_v7.json\"\n",
    "cand_response_filename = f\"{BASE_DIR}/experiments/r-v7_r256_a512_ours_16bit_adamw16bit_0322-checkpoint-56.json\"\n",
    "\n",
    "eval_df = eval_query(db_gt_filename, cand_response_filename)\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>TruePositive</th>\n",
       "      <th>FalsePositive</th>\n",
       "      <th>FalseNegative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오늘 아침과 저녁의 온도차이는 얼마나 돼?</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0.330189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>올해 여름 우리반 실내온도 최대값과 최소값 알려줘</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Input  TruePositive  FalsePositive  FalseNegative\n",
       "0       오늘 아침과 저녁의 온도차이는 얼마나 돼?      0.339623       0.330189       0.330189\n",
       "12  올해 여름 우리반 실내온도 최대값과 최소값 알려줘      0.333333       0.333333       0.333333"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df[\"ExactMatch\"] == 0][[\"Input\", \"TruePositive\", \"FalsePositive\", \"FalseNegative\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
