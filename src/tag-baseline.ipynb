{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "from unsloth import FastLanguageModel, unsloth_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.db.manager import DBManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'defog/llama-3-sqlcoder-8b'\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2afb7174b04686b688466d76907884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defog/llama-3-sqlcoder-8b does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n",
      "Pad Token id: 128255 and Pad Token: <|reserved_special_token_250|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = torch_dtype,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     # load_in_8bit=True,\n",
    "    #     # llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    # ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # trust_remote_code=True\n",
    "    # local_files_only=True\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Ï£ºÏñ¥ÏßÑ ÌÖåÏù¥Î∏î Ïä§ÌÇ§ÎßàÎ•º Ï∞∏Í≥†Ìï¥ ÏßàÎ¨∏Ïóê ÎãµÌïòÍ∏∞ ÏúÑÌïú ÏøºÎ¶¨Î•º ÏûëÏÑ±Ìï¥Ï§ò\n",
    "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§: timescale db (postgresql Í∏∞Î∞ò)\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.idu_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
    "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
    "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
    ")\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.data_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
    "    idu_id integer,\n",
    "    roomtemp double precision,\n",
    "    settemp double precision,\n",
    "    timestamp timestamp without time zone NOT NULL\n",
    ")\n",
    "\n",
    "Ï°∞ÌöåÌï†Îïå NaNÏù∏ rowÎäî Ï†úÏô∏\n",
    "\n",
    "\n",
    "ÏßÄÍ∏àÏùÄ 2022-09-29 21:30:00 ÏûÖÎãàÎã§.\n",
    "\"\"\" + \"\"\"\n",
    "Ïã§ÎÇ¥Ïò®ÎèÑ: roomtemp\n",
    "ÏÑ§Ï†ïÏò®ÎèÑ: settemp\n",
    "Ïö∞Î¶¨Î∞ò: 02_I81 (idu.name)\n",
    "ÏòÜÎ∞ò: 01_IB5 (idu.name)\n",
    "ÏïûÎ∞ò: 01_IB7 (idu.name)\n",
    "4Ï∏µ: 02_I81, 01_IB5, 01_IB7 (idu.name)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text: str):\n",
    "    \"\"\"Extract content from model output.\"\"\"\n",
    "    if \"start_header_id\" in text:\n",
    "        pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def generate(query):\n",
    "    \"\"\"\n",
    "    suppress all outputs, warnings, errors from this function\n",
    "    Î™®Îì† Ï∂úÎ†•, Í≤ΩÍ≥†, ÏóêÎü¨Î•º ÏñµÏ†úÌïòÎäî Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import contextlib\n",
    "    import warnings\n",
    "    import io\n",
    "\n",
    "    # Suppress stdout, stderr, and warnings\n",
    "    with contextlib.redirect_stdout(io.StringIO()), \\\n",
    "         contextlib.redirect_stderr(io.StringIO()), \\\n",
    "         warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            chat = [\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Input:{query};\"},\n",
    "            ]\n",
    "\n",
    "            chat = tokenizer.apply_chat_template(\n",
    "                chat,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                # return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            chat = re.sub(\n",
    "                r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', \n",
    "                '', \n",
    "                chat\n",
    "            )\n",
    "\n",
    "            # Tokenize the chat input using the tokenizer\n",
    "            # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÏÇ¨Ïö©ÌïòÏó¨ chat ÏûÖÎ†•ÏùÑ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï¶àÌï©ÎãàÎã§.\n",
    "            input_ids = tokenizer(\n",
    "                chat, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids.to(model.device)\n",
    "\n",
    "            outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_new_tokens=1000,\n",
    "                    temperature=0.001,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                )\n",
    "            decoded = tokenizer.batch_decode(\n",
    "                outputs, \n",
    "                skip_special_tokens=False\n",
    "            )\n",
    "\n",
    "            return extract_content(decoded[0])\n",
    "        except Exception:\n",
    "            # suppress all errors, return None\n",
    "            # Î™®Îì† ÏóêÎü¨Î•º ÏñµÏ†úÌïòÍ≥† NoneÏùÑ Î∞òÌôòÌï©ÎãàÎã§.\n",
    "            return None\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = \"../\"\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result\n",
    "\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dataset_name}\")\n",
    "\n",
    "\n",
    "inputs = []\n",
    "for scenario_dir in [d for d in base_dataset_dir.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]:\n",
    "    path = scenario_dir / \"onlyq_ts.json\"\n",
    "    if \"scenario3\" in str(path):\n",
    "        continue\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    metadata = json.load(open(scenario_dir / \"metadata.json\", \"r\"))\n",
    "    result = []\n",
    "    # for d in data:\n",
    "    inputs.extend([{\n",
    "        \"Input\": i[\"Input\"],\n",
    "        \"Scenario\": scenario_dir.name,\n",
    "        \"Metadata\": metadata,\n",
    "    } for i in data])\n",
    "\n",
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Input': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞òÏùò ÌèâÍ∑† Ïò®ÎèÑ ÏïåÎ†§Ï§ò', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT AVG(d.roomtemp) AS avg_roomtemp, AVG(d.settemp) AS avg_settemp FROM public.data_t d JOIN public.idu_t i ON d.idu_id = i.id WHERE i.name IN ('02_I81', '01_IB7') AND d.timestamp >= (TIMESTAMP '2022-09-29 21:30:00' - interval '1 week');\"}, {'Input': 'ÌòÑÏû¨ ÏÑ§Ï†ïÏò®ÎèÑÎûë Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò.', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT d.idu_id, d.roomtemp - d.settemp AS temperature_difference FROM public.data_t d WHERE d.timestamp >= '2022-09-29 21:30:00' AND d.settemp IS NOT NULL AND d.roomtemp IS NOT NULL;\"}, {'Input': 'ÏßÄÎÇúÎã¨Ïóê ÏÑ§Ï†ïÏò®ÎèÑÏôÄ Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥Í∞Ä Í∞ÄÏû• ÎßéÏù¥ ÎÇ¨Îçò ÎÇ†ÏùÄ?', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT d.timestamp FROM public.data_t d WHERE d.idu_id IN (SELECT id FROM public.idu_t WHERE name IN ('02_I81', '01_IB5', '01_IB7')) AND d.timestamp >= (TIMESTAMP '2022-09-29 21:30:00' - interval '1 month') ORDER BY (d.settemp - d.roomtemp) DESC LIMIT 1;\"}, {'Input': 'Ïù¥Î≤àÏ£º Ïö∞Î¶¨Î∞òÍ≥º ÏòÜÎ∞òÏùò ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ Ï∞®Ïù¥ ÏïåÎ†§Ï§ò', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT AVG(d.roomtemp) AS avg_roomtemp, (SELECT AVG(d2.roomtemp) FROM public.data_t d2 WHERE d2.idu_id IN (SELECT id FROM public.idu_t WHERE name = '01_IB5')) AS avg_roomtemp_neighbor FROM public.data_t d WHERE d.idu_id IN (SELECT id FROM public.idu_t WHERE name = '02_I81') AND d.timestamp >= (TIMESTAMP '2022-09-29 21:30:00' - interval '1 week') AND d.timestamp < TIMESTAMP '2022-09-29 21:30:00';\"}, {'Input': '2Ï£ºÏ†Ñ Í∞ÄÏû• ÎçîÏõ†Îçò ÎÇ† ÏïåÎ†§Ï§ò', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT d.timestamp, d.roomtemp FROM public.data_t d WHERE d.idu_id IN (SELECT id FROM public.idu_t WHERE name IN ('02_I81', '01_IB5', '01_IB7')) AND d.roomtemp IS NOT NULL AND d.timestamp >= (TIMESTAMP '2022-09-29 21:30:00' - interval '2 weeks') ORDER BY d.roomtemp DESC LIMIT 1;\"}, {'Input': 'ÌôîÏÑ±Ïùò ÏÑ§Ï†ïÏò®ÎèÑ ÌôïÏù∏Ìï¥Ï§ò', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT d.settemp FROM public.data_t d WHERE d.idu_id IN (SELECT id FROM public.idu_t WHERE name = 'ÌôîÏÑ±');\"}, {'Input': 'ÏòÜÎ∞ò ÏäµÎèÑ ÏïåÎ†§Ï§ò', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT d.settemp FROM public.data_t d JOIN public.idu_t i ON d.idu_id = i.id WHERE i.name = '01_IB5' AND d.timestamp >= '2022-09-29 21:30:00';\"}, {'Input': 'ÏßÄÎÇú 3Ïùº ÎèôÏïà Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥ Ïò®ÎèÑ ÌèâÍ∑† Í∞í ÏïåÎ†§Ï§ò.', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT AVG(d.roomtemp) AS average_roomtemp FROM public.data_t d JOIN public.idu_t i ON d.idu_id = i.id WHERE i.name = '02_I81' AND d.timestamp > (TIMESTAMP '2022-09-29 21:30:00' - interval '3 days');\"}, {'Input': 'Ïò§Îäò Ïò§ÌõÑ 5ÏãúÏóê ÏòÜÎ∞òÏùò ÏÑ§Ï†ïÏò®ÎèÑÎäî Ïñ¥Îï†Ïñ¥?', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT d.settemp FROM public.data_t d JOIN public.idu_t i ON d.idu_id = i.id WHERE i.name = '01_IB5' AND d.timestamp >= '2022-09-29 17:00:00' AND d.timestamp < '2022-09-30 00:00:00';\"}, {'Input': 'Ïò¨Ìï¥ Ïó¨Î¶Ñ Ïö∞Î¶¨Î∞ò Ïã§ÎÇ¥Ïò®ÎèÑ ÏµúÎåÄÍ∞íÍ≥º ÏµúÏÜåÍ∞í ÏïåÎ†§Ï§ò', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT MAX(d.roomtemp) AS max_roomtemp, MIN(d.roomtemp) AS min_roomtemp FROM public.data_t d JOIN public.idu_t i ON d.idu_id = i.id WHERE i.name = '02_I81' AND EXTRACT(YEAR FROM d.timestamp) = EXTRACT(YEAR FROM TIMESTAMP '2022-09-29 21:30:00') AND EXTRACT(SEASON FROM d.timestamp) ='summer';\"}, {'Input': 'Ïö∞Î¶¨Î∞òÍ≥º ÏïûÎ∞ò Ï§ë Í∞ÄÏû• ÎçîÏö¥ Î∞©ÏùÄ?', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"WITH room_temps AS (SELECT d.idu_id, d.roomtemp FROM public.data_t d WHERE d.timestamp >= '2022-09-29 21:30:00'::TIMESTAMP) SELECT idu.name, MAX(room_temps.roomtemp) AS max_room_temp FROM room_temps JOIN public.idu_t idu ON room_temps.idu_id = idu.id WHERE idu.name IN ('02_I81', '01_IB7') GROUP BY idu.name ORDER BY max_room_temp DESC NULLS LAST LIMIT 1;\"}, {'Input': 'ÏßÄÍ∏à 4Ï∏µ ÌèâÍ∑† Ïã§ÎÇ¥Ïò®ÎèÑ ÏïåÎ†§Ï§ò', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['ÏòÜÎ∞ò', '4Ï∏µ'], '01_IB7': ['ÏïûÎ∞ò', '4Ï∏µ'], '02_I81': ['Ïö∞Î¶¨Î∞ò', '4Ï∏µ']}, 'modality_mapping': {'roomtemp': ['Ïã§ÎÇ¥Ïò®ÎèÑ'], 'settemp': ['ÏÑ§Ï†ïÏò®ÎèÑ']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': \"SELECT AVG(d.roomtemp) AS average_roomtemp FROM public.data_t d WHERE d.timestamp >= '2022-09-29 21:30:00' AND d.idu_id IN (SELECT id FROM public.idu_t WHERE name IN ('02_I81', '01_IB5', '01_IB7'));\"}]\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "for input in inputs:\n",
    "    input, scenario, metadata = input[\"Input\"], input[\"Scenario\"], input[\"Metadata\"]\n",
    "    \n",
    "    result = generate(input)\n",
    "    if result == None:\n",
    "        reports.append({\n",
    "            \"Input\": input,\n",
    "            \"Scenario\": scenario,\n",
    "            \"Metadata\": metadata,\n",
    "            \"Candidate\": \"\",\n",
    "        })\n",
    "        continue\n",
    "    result = result.replace(\"CURRENT_DATE\", \"TIMESTAMP '2022-09-29 21:30:00'\")\n",
    "    # print(result)\n",
    "\n",
    "    reports.append({\n",
    "        \"Input\": input,\n",
    "        \"Scenario\": scenario,\n",
    "        \"Metadata\": metadata,\n",
    "        \"Candidate\": result,\n",
    "    })\n",
    "    # try:\n",
    "    #     results = DBManager.execute_sql(result)\n",
    "    #     has_nan = False\n",
    "    #     for r in results:\n",
    "    #         # r: tuple\n",
    "    #         # see if nan in r\n",
    "    #         if any(pd.isna(v) for v in r):\n",
    "    #             has_nan = True\n",
    "    #             break\n",
    "    #     if has_nan:\n",
    "    #         report[\"nan\"] += 1\n",
    "    #     else:\n",
    "    #         print(f\"Query: {query}\\nResult: {result}\\n\")\n",
    "    #         print(\"results\", results)\n",
    "    #         report[\"success\"] += 1\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error: {e}\")\n",
    "    #     report[\"error\"] += 1\n",
    "print(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save at ../experiments/tag-baseline.json\n",
    "with open(\"../experiments/r-v7_r211_a422_TAG_tr27_0629-step-0.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reports, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
