{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "from unsloth import FastLanguageModel, unsloth_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add the parent directory of src to the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.db.manager import DBManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'defog/llama-3-sqlcoder-8b'\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.7: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA H100 NVL. Num GPUs = 1. Max memory: 93.016 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fbd41ae0f3423eb041f9cbc8001a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling: Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defog/llama-3-sqlcoder-8b does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n",
      "Pad Token id: 128255 and Pad Token: <|reserved_special_token_250|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = torch_dtype,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     # load_in_8bit=True,\n",
    "    #     # llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    # ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # trust_remote_code=True\n",
    "    # local_files_only=True\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"주어진 테이블 스키마를 참고해 질문에 답하기 위한 쿼리를 작성해줘\n",
    "데이터베이스: timescale db (postgresql 기반)\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.idu_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
    "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
    "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
    ")\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.data_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
    "    idu_id integer,\n",
    "    roomtemp double precision,\n",
    "    settemp double precision,\n",
    "    timestamp timestamp without time zone NOT NULL\n",
    ")\n",
    "\n",
    "조회할때 NaN인 row는 제외\n",
    "\n",
    "\n",
    "지금은 2022-09-29 21:30:00 입니다.\n",
    "\"\"\" + \"\"\"\n",
    "실내온도: roomtemp\n",
    "설정온도: settemp\n",
    "우리반: 02_I81 (idu.name)\n",
    "옆반: 01_IB5 (idu.name)\n",
    "앞반: 01_IB7 (idu.name)\n",
    "4층: 02_I81, 01_IB5, 01_IB7 (idu.name)\n",
    "8층: 02_I81, 01_IB5, 01_IB7 (idu.name)\n",
    "옆집: 01_IB5 (idu.name)\n",
    "우리집 : 02_I81 (idu.name)\n",
    "앞집: 01_IB7 (idu.name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_content(text: str):\n",
    "    \"\"\"Extract content from model output.\"\"\"\n",
    "    if \"start_header_id\" in text:\n",
    "        pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "\n",
    "def generate(query):\n",
    "    \"\"\"\n",
    "    suppress all outputs, warnings, errors from this function\n",
    "    모든 출력, 경고, 에러를 억제하는 함수입니다.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import contextlib\n",
    "    import warnings\n",
    "    import io\n",
    "\n",
    "    # Suppress stdout, stderr, and warnings\n",
    "    with contextlib.redirect_stdout(io.StringIO()), \\\n",
    "         contextlib.redirect_stderr(io.StringIO()), \\\n",
    "         warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            chat = [\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Input:{query};\"},\n",
    "            ]\n",
    "\n",
    "            chat = tokenizer.apply_chat_template(\n",
    "                chat,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                # return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            chat = re.sub(\n",
    "                r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', \n",
    "                '', \n",
    "                chat\n",
    "            )\n",
    "\n",
    "            # Tokenize the chat input using the tokenizer\n",
    "            # 토크나이저를 사용하여 chat 입력을 토크나이즈합니다.\n",
    "            input_ids = tokenizer(\n",
    "                chat, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids.to(model.device)\n",
    "\n",
    "            outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_new_tokens=1000,\n",
    "                    temperature=0.001,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                )\n",
    "            decoded = tokenizer.batch_decode(\n",
    "                outputs, \n",
    "                skip_special_tokens=False\n",
    "            )\n",
    "\n",
    "            return extract_content(decoded[0])\n",
    "        except Exception:\n",
    "            # suppress all errors, return None\n",
    "            # 모든 에러를 억제하고 None을 반환합니다.\n",
    "            return None\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = \"../\"\n",
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.loads(f.read())\n",
    "    \n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in result]\n",
    "    return result\n",
    "\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "base_dataset_dir = Path(f\"{BASE_DIR}/finetuning/dataset/{dataset_name}\")\n",
    "\n",
    "\n",
    "inputs = []\n",
    "for scenario_dir in [d for d in base_dataset_dir.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]:\n",
    "    path = scenario_dir / \"onlyq_ts.json\"\n",
    "    # if \"scenario3\" in str(path):\n",
    "    #     continue\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    metadata = json.load(open(scenario_dir / \"metadata.json\", \"r\"))\n",
    "    result = []\n",
    "    # for d in data:\n",
    "    inputs.extend([{\n",
    "        \"Input\": i[\"Input\"],\n",
    "        \"Scenario\": scenario_dir.name,\n",
    "        \"Metadata\": metadata,\n",
    "    } for i in data])\n",
    "\n",
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Input': '이번주 우리반과 앞반의 평균 온도 알려줘', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '현재 설정온도랑 실내온도 차이 알려줘.', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '지난달에 설정온도와 실내온도 차이가 가장 많이 났던 날은?', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '이번주 우리반과 옆반의 평균 실내온도 차이 알려줘', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '2주전 가장 더웠던 날 알려줘', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '화성의 설정온도 확인해줘', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '옆반 습도 알려줘', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '지난 3일 동안 우리반 실내 온도 평균 값 알려줘.', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '오늘 오후 5시에 옆반의 설정온도는 어땠어?', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '올해 여름 우리반 실내온도 최대값과 최소값 알려줘', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '우리반과 앞반 중 가장 더운 방은?', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}, {'Input': '지금 4층 평균 실내온도 알려줘', 'Scenario': 'scenario2', 'Metadata': {'idu_mapping': {'01_IB5': ['옆반', '4층'], '01_IB7': ['앞반', '4층'], '02_I81': ['우리반', '4층']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도']}, 'current_datetime': '2022-09-29 21:30:00'}, 'Candidate': ''}]\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "for input in inputs:\n",
    "    input, scenario, metadata = input[\"Input\"], input[\"Scenario\"], input[\"Metadata\"]\n",
    "    \n",
    "    result = generate(input)\n",
    "    if result == None:\n",
    "        reports.append({\n",
    "            \"Input\": input,\n",
    "            \"Scenario\": scenario,\n",
    "            \"Metadata\": metadata,\n",
    "            \"Candidate\": \"\",\n",
    "        })\n",
    "        continue\n",
    "    result = result.replace(\"CURRENT_DATE\", \"TIMESTAMP '2022-09-29 21:30:00'\")\n",
    "    # print(result)\n",
    "\n",
    "    reports.append({\n",
    "        \"Input\": input,\n",
    "        \"Scenario\": scenario,\n",
    "        \"Metadata\": metadata,\n",
    "        \"Candidate\": result,\n",
    "    })\n",
    "    # try:\n",
    "    #     results = DBManager.execute_sql(result)\n",
    "    #     has_nan = False\n",
    "    #     for r in results:\n",
    "    #         # r: tuple\n",
    "    #         # see if nan in r\n",
    "    #         if any(pd.isna(v) for v in r):\n",
    "    #             has_nan = True\n",
    "    #             break\n",
    "    #     if has_nan:\n",
    "    #         report[\"nan\"] += 1\n",
    "    #     else:\n",
    "    #         print(f\"Query: {query}\\nResult: {result}\\n\")\n",
    "    #         print(\"results\", results)\n",
    "    #         report[\"success\"] += 1\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error: {e}\")\n",
    "    #     report[\"error\"] += 1\n",
    "print(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save at ../experiments/tag-baseline.json\n",
    "with open(\"../experiments/r-v7_r211_a422_TAG_tr27_revision-step-0.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reports, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
