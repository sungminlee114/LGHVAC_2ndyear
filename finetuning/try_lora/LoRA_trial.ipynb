{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Finetune I2I LLM w/ LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using eager for attention computation.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 3000     # Unsloth auto supports RoPE Scaling internally!\n",
    "dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "device = \"cuda:1\"\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# current_metadata = {\n",
    "#     \"site_name\": \"YongDongIllHighSchool\",\n",
    "#     \"user_name\": \"í™ê¸¸ë™\",\n",
    "#     \"user_role\": \"customer\", # customer, admin\n",
    "#     \"idu_name\": \"01_IB5\",\n",
    "#     \"idu_mapping\": {\n",
    "#         \"01_IB5\": [\"ìš°ë¦¬ë°˜\"],\n",
    "#         \"01_IB7\": [\"ì˜†ë°˜\"],\n",
    "#     },  \n",
    "#     \"modality_mapping\": {\n",
    "#         \"roomtemp\": [\"ì‹¤ë‚´ì˜¨ë„\"],\n",
    "#         \"settemp\": [\"ì„¤ì •ì˜¨ë„\"],\n",
    "#         \"oper\": [\"ì „ì›\"],\n",
    "#     },\n",
    "#     \"current_datetime\": \"2022-09-30 12:00:00\",\n",
    "# }\n",
    "\n",
    "current_metadata = json.load(open(\"metadata.json\", \"r\"))\n",
    "\n",
    "# <Input Semantic Parsing ì¡°ê±´>\n",
    "# Temporal: ì‹œê°„ í˜¹ì€ ì‹œê°„ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´. ì‚¬ì „ì •ë³´ ì°¸ê³ . ëŒ€í‘œì  representationê³¼ Timestamp í˜•ì‹ì˜ íŠœí”Œë¡œ í‘œí˜„ë¨. ì•„ë¬´ëŸ° ì •ë³´ê°€ ì—†ì„ ê²½ìš° 'ì§€ê¸ˆ'ìœ¼ë¡œ í‘œí˜„ë˜ì§€ë§Œ, ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ìì œí•´ì•¼ í•¨.\n",
    "# Spatial: ê³µê°„ í˜¹ì€ ê³µê°„ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´. ì‚¬ì „ì •ë³´ ì°¸ê³ . ëŒ€í‘œì  representationìœ¼ë¡œ í‘œí˜„ë¨. ì•„ë¬´ëŸ° ì •ë³´ê°€ ì—†ì„ ê²½ìš° 'ìš°ë¦¬ë°˜'ìœ¼ë¡œ í‘œí˜„ë˜ì§€ë§Œ, ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ìì œí•´ì•¼ í•¨.\n",
    "# Modality: ì •ë³´ì˜ í˜•íƒœ í˜¹ì€ ì •ë³´ì˜ ì†ì„±. ì‚¬ì „ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì œì¼ ìœ ì‚¬í•œ ë‹¨ì–´ë¡œ í‘œí˜„í•´ì•¼ í•˜ì§€ë§Œ, ì‚¬ì „ì •ë³´ì— ì—†ëŠ” ê²½ìš° ì¶”ì •í•´ì•¼ í•¨. \n",
    "# Operation: ì •ë³´ì— í–‰í•´ì§€ëŠ” ì¡°ì‘.\n",
    "\n",
    "\n",
    "# <ì ˆëŒ€ë¡œ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ>\n",
    "# ì´ë¯¸ ëª…ì‹œì ìœ¼ë¡œ ì‹œê°„/ê³µê°„ ì •ë³´ê°€ inputì— ìˆëŠ”ë°, ì—†ë‹¤ê³  ì˜ëª» íŒë‹¨í•˜ë©´ ì•ˆë¨.\n",
    "\n",
    "common_prompt = \"\"\"\n",
    "ë‚˜ëŠ” í›Œë£¡í•œ HVAC ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì—¬ì„œ, ê·¸ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë‚´ ì„ë¬´ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì¼ ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ ì ˆì°¨ë¥¼ ë”°ë¼ì•¼ í•œë‹¤.\n",
    "<ì¶œë ¥ ë‚´ìš©>\n",
    "1. 'Formalized Input': ì‚¬ìš©ìì˜ ì¶”ìƒì  ì§ˆë¬¸ì„ êµ¬ì²´í™” ë° ì •ê·œí™”í•œ ê²°ê³¼. ë‹¤ì–‘í•œ í˜•íƒœì˜ ì§ˆë¬¸ë“¤ì„ ê°€ì¥ í•µì‹¬ì ì´ê³  ê·¼ë³¸ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•œ ê²°ê³¼.\n",
    "2. 'Input Semantic Parsing': Input Semantic Parsing ê²°ê³¼. dict í˜•íƒœë¡œ êµ¬ì„±ë˜ë©°, Temporal, Spatial, Modality, Operationì„ ê°€ì§.\n",
    "3. 'Strategy': ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì „ëµì„ ë‹µë³€ ì „ ê³ ë¯¼. Objective: ì§ˆë¬¸ì˜ ê·¼ë³¸ì  ì˜ë„ ë° ë‹µë³€ì˜ ëª©ì . Expected Output: ë‹µë³€ì˜ ì˜ˆìƒ ê²°ê³¼. Step: ë‹µë³€ì„ ìœ„í•œ êµ¬ì²´ìœ¼ë¡œ ìª¼ê°œì§„ ë‹¨ê³„.\n",
    "4. 'Instruction Set': ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‚˜ì˜ ì‹¤ì œ í–‰ë™.\n",
    "\n",
    "<DDL statement>\n",
    "CREATE TABLE IF NOT EXISTS data_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
    "    idu_id integer,\n",
    "    roomtemp double precision,\n",
    "    settemp double precision,\n",
    "    oper boolean,\n",
    "    \"timestamp\" timestamp without time zone NOT NULL\n",
    ")\n",
    "    \n",
    "CREATE TABLE IF NOT EXISTS idu_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
    "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
    "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
    "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# common_prompt=\"\"\n",
    "\n",
    "# Current metadata ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹¼ê¸´í•´ì•¼í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.689 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc15366aafa430aab0da84c1fb663cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bllossom/llama-3.2-Korean-Bllossom-3B does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n",
      "Model dir: /model/Bllossom-llama-3.2-Korean-Bllossom-3B\n",
      "Pad Token id: 128004 and Pad Token: <|finetune_right_pad_id|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenizer initialization\n",
    "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,  \n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"/model/{model_id}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # local_files_only=True\n",
    ")\n",
    "\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    pretrained_model.save_pretrained(model_dir)\n",
    "print(f\"Model dir: {model_dir}\")\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n",
      "Pad Token id: 128004 and Pad Token: <|finetune_right_pad_id|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Initialize the tokenizer with the chat template and mapping\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "# pretrained_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "# print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "tokenizer_ = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\", \n",
    "    # mapping = {\n",
    "    #     \"role\" : \"from\",\n",
    "    #     \"content\" : \"value\",\n",
    "    #     \"user\" : \"human\", \n",
    "    #     \"assistant\" : \"gpt\",\n",
    "    #     }, # ShareGPT style # templateì—ì„œ roleì„ fromìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤ë˜ì§€..\n",
    "    map_eos_token = False,        # Maps <|im_end|> to <|eot_id|> instead\n",
    ") # https://docs.unsloth.ai/basics/chat-templates\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d772c3f9ad9486b88eec4b8de972f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer length:  1542\n",
      "Answer length:  958\n",
      "Answer length:  1103\n",
      "Answer length:  1462\n",
      "Answer length:  814\n",
      "Answer length:  809\n",
      "Answer length:  1193\n",
      "Answer length:  1252\n",
      "Answer length:  1569\n",
      "Answer length:  899\n",
      "Answer length:  1319\n",
      "Answer length:  411\n",
      "Answer length:  812\n",
      "Answer length:  416\n",
      "Answer length:  880\n",
      "Answer length:  455\n",
      "Answer length:  291\n",
      "Answer length:  468\n",
      "Answer length:  782\n",
      "Answer length:  1203\n",
      "Answer length:  1666\n",
      "Answer length:  1619\n",
      "Answer length:  281\n",
      "Answer length:  1302\n",
      "Answer length:  1302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ef28c4d4e04495915212859b920d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer length:  809\n",
      "Answer length:  930\n",
      "Answer length:  1047\n",
      "Answer length:  1462\n",
      "Answer length:  814\n",
      "Answer length:  809\n",
      "Answer length:  1193\n",
      "Answer length:  1252\n",
      "Answer length:  2103\n",
      "Answer length:  1329\n",
      "Answer length:  1061\n",
      "Answer length:  440\n",
      "Answer length:  905\n",
      "Answer length:  447\n",
      "Answer length:  348\n",
      "Answer length:  469\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "ë‚˜ëŠ” í›Œë£¡í•œ HVAC ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì—¬ì„œ, ê·¸ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë‚´ ì„ë¬´ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì¼ ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ ì ˆì°¨ë¥¼ ë”°ë¼ì•¼ í•œë‹¤.\n",
      "<ì¶œë ¥ ë‚´ìš©>\n",
      "1. 'Formalized Input': ì‚¬ìš©ìì˜ ì¶”ìƒì  ì§ˆë¬¸ì„ êµ¬ì²´í™” ë° ì •ê·œí™”í•œ ê²°ê³¼. ë‹¤ì–‘í•œ í˜•íƒœì˜ ì§ˆë¬¸ë“¤ì„ ê°€ì¥ í•µì‹¬ì ì´ê³  ê·¼ë³¸ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•œ ê²°ê³¼.\n",
      "2. 'Input Semantic Parsing': Input Semantic Parsing ê²°ê³¼. dict í˜•íƒœë¡œ êµ¬ì„±ë˜ë©°, Temporal, Spatial, Modality, Operationì„ ê°€ì§.\n",
      "3. 'Strategy': ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì „ëµì„ ë‹µë³€ ì „ ê³ ë¯¼. Objective: ì§ˆë¬¸ì˜ ê·¼ë³¸ì  ì˜ë„ ë° ë‹µë³€ì˜ ëª©ì . Expected Output: ë‹µë³€ì˜ ì˜ˆìƒ ê²°ê³¼. Step: ë‹µë³€ì„ ìœ„í•œ êµ¬ì²´ìœ¼ë¡œ ìª¼ê°œì§„ ë‹¨ê³„.\n",
      "4. 'Instruction Set': ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‚˜ì˜ ì‹¤ì œ í–‰ë™.\n",
      "\n",
      "<DDL statement>\n",
      "CREATE TABLE IF NOT EXISTS data_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
      "    idu_id integer,\n",
      "    roomtemp double precision,\n",
      "    settemp double precision,\n",
      "    oper boolean,\n",
      "    \"timestamp\" timestamp without time zone NOT NULL\n",
      ")\n",
      "    \n",
      "CREATE TABLE IF NOT EXISTS idu_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
      "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
      "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
      "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
      ")<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "{'site_name': 'YongDongIllHighSchool', 'user_name': 'í™ê¸¸ë™', 'user_role': 'customer', 'idu_name': '01_IB5', 'idu_mapping': {'01_IB5': ['ìš°ë¦¬ë°˜'], '01_IB7': ['ì˜†ë°˜'], '02_IB1': ['ì•ë°˜']}, 'modality_mapping': {'roomtemp': ['ì‹¤ë‚´ì˜¨ë„'], 'settemp': ['ì„¤ì •ì˜¨ë„'], 'oper': ['ì „ì›']}, 'ë§íˆ¬': 'ì¡´ëŒ“ë§', 'ì–¸ì–´': 'í•œêµ­ì–´', 'current_datetime': '2022-09-30 12:00:00'};ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì˜¨ë„ì°¨ì´ ì•Œë ¤ì¤˜<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{'Formalized Input': 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.', 'Input Semantic Parsing': {'Temporal': {'ì˜¤ëŠ˜': '2022-09-30 00:00:00 ~ 2022-09-30 23:59:59'}, 'Spatial': {'ìš°ë¦¬ë°˜': '01_IB5', 'ì˜†ë°˜': '01_IB7'}, 'Modality': {'ì‹¤ë‚´ì˜¨ë„': 'roomtemp'}, 'Operation': ['í‰ê· ì˜ ì°¨ì´']}, 'Strategy': {'Objective': 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ ê°ê°ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ í›„, ë‘ ê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨.', 'Expected Outputs': ['ìš°ë¦¬ë°˜(19.3â„ƒ)ì´ ì˜†ë°˜(17.0â„ƒ)ë³´ë‹¤ 2.3â„ƒ ë†’ìŠµë‹ˆë‹¤.'], 'Step': ['ìš°ë¦¬ë°˜ì˜ ì˜¤ëŠ˜ ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ê²°ê³¼ qr_roomtemp_ours ì— ì €ì¥', 'ì˜†ë°˜ì˜ ì˜¤ëŠ˜ ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ê²°ê³¼ qr_roomtemp_beside ì— ì €ì¥', 'ê° ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê·  ê³„ì‚°', 'ê° ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ì™€ ì°¨ì´ ë°˜í™˜']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-30 00:00:00' AND '2022-09-30 23:59:59'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')\"}}, 'result': {'name': 'qr_roomtemp_ours'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-30 00:00:00' AND '2022-09-30 23:59:59'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'o', 'python_script_to_process': \"avg_roomtemp_ours = qr_roomtemp_ours['roomtemp'].mean(); avg_roomtemp_beside = qr_roomtemp_beside['roomtemp'].mean(); temp_diff = avg_roomtemp_ours - avg_roomtemp_beside\", 'return': ['avg_roomtemp_ours', 'avg_roomtemp_beside', 'temp_diff']}, {'type': 'r', 'templates': ['{room_with_higher_temp}({avg_roomtemp_higher}â„ƒ)ì´ {room_with_lower_temp}({avg_roomtemp_lower}â„ƒ) ë³´ë‹¤ {temp_diff}â„ƒ ë§Œí¼ ë†’ìŠµë‹ˆë‹¤.']}]}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset = load_dataset(\"csv\", data_files=\"./training_dataset.tsv\",delimiter=\"\\t\")\n",
    "import re\n",
    "\n",
    "\n",
    "dataset_tr = json.load(open(\"training_dataset_v2_directsql_tr.json\", \"r\"))\n",
    "for i, data in enumerate(dataset_tr):\n",
    "    dataset_tr[i][\"Response\"] = str(dataset_tr[i][\"Response\"])\n",
    "dataset_tr = Dataset.from_list(dataset_tr)\n",
    "\n",
    "dataset_ts = json.load(open(\"training_dataset_v2_directsql_ts.json\", \"r\"))\n",
    "for i, data in enumerate(dataset_ts):\n",
    "    dataset_ts[i][\"Response\"] = str(dataset_ts[i][\"Response\"])\n",
    "dataset_ts = Dataset.from_list(dataset_ts)\n",
    "\n",
    "# dataset = load_dataset(\"json\", data_files=\"training_dataset_v2_directsql.json\")\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for input, response in zip(examples['Input'], examples['Response']):\n",
    "        prompt = {\n",
    "            \"content\": common_prompt,\n",
    "            \"role\": \"system\"\n",
    "        }\n",
    "        user_input = {\n",
    "            \"content\": f\"{current_metadata};{input}\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        convos.append([prompt, user_input, answer])\n",
    "        print(\"Answer length: \", len(response))\n",
    "        # convos.append([prompt, user_input, answer])\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    # remove \\n\\nCutting Knowledge Date: BLAH BLAH \\nToday Date: BLAH BLAH\\n\\n using regex\n",
    "    texts = [re.sub(r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', '', text) for text in texts]\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset_tr = dataset_tr.map(formatting_prompts_func, batched=True)\n",
    "dataset_ts = dataset_ts.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(dataset_tr[0][\"text\"])\n",
    "# print(f\"seq length: {len(tokenizer.encode(dataset_tr[0]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    pretrained_model,\n",
    "    r=1110,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=1000,\n",
    "    lora_dropout=0.0,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None   # No LoftQ, for standard fine-tuning\n",
    ")\n",
    "del pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(len(dataset_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Accumulation Steps: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d7586548ac48c190a9dcb0b72e1e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984bf5bd6a6049b6b2c42b7687ad6990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "per_device_train_batch_size, epochs = 4, 40 # 8\n",
    "gradient_accumulation_steps = int(np.ceil(len(dataset_tr) / per_device_train_batch_size))\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "args = TrainingArguments(\n",
    "    # num_train_epochs = 1,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,  # Controls the batch size per device\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # Accumulates gradients to simulate a larger batch\n",
    "    max_steps=gradient_accumulation_steps * epochs,\n",
    "    # ë¦¬ì†ŒìŠ¤ ì œì•½ë•Œë¬¸ì— batch sizeë¥¼ íƒ€í˜‘í•´ì•¼í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒ -> micro batch sizeë¥¼ ì¤„ì´ê³ ,\n",
    " \t# accumulated stepì„ ëŠ˜ë ¤, ì ì ˆí•œ sizeë¡œ gradientë¥¼ êµ¬í•´ weight update\n",
    "    # https://www.youtube.com/watch?v=ptlmj9Y9iwE\n",
    "    warmup_steps = gradient_accumulation_steps,\n",
    "    learning_rate = 1e-4,             # Sets the learning rate for optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "    lr_scheduler_type = \"cosine\",  # Sets the learning rate scheduler\n",
    "    seed = 3407,                        \n",
    "    output_dir = f\"{model_dir}/chkpts\",  # Output directory for checkpoints and predictions     \n",
    "    report_to = \"none\",              # Enables Weights & Biases (W&B) logging\n",
    "    logging_steps = gradient_accumulation_steps,                # Sets frequency of logging to W&B\n",
    "    logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "    evaluation_strategy=\"steps\",  # enable evaluation during training\n",
    "    eval_steps=gradient_accumulation_steps,\n",
    "    eval_accumulation_steps=1, # ë‚®ì„ìˆ˜ë¡ evalì‹œ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ì¤„ì–´ë“¦\n",
    "    save_steps=gradient_accumulation_steps,\n",
    "    save_strategy = \"steps\",               \n",
    "    load_best_model_at_end = True,    # Loads the best model at the end\n",
    "    save_only_model = False           # Saves entire model, not only weights\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_tr,\n",
    "    eval_dataset = dataset_ts,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,        # Can make training 5x faster for short sequences.\n",
    "    args = args,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 25 | Num Epochs = 280\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 7\n",
      "\\        /    Total batch size = 28 | Total steps = 280\n",
      " \"-____-\"     Number of trainable parameters = 1,686,773,760\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)  \n",
    "print(trainer_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model/Bllossom-llama-3.2-Korean-Bllossom-3B/chkpts/checkpoint-42\n",
      "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.689 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729f69e3804e4afb86e16346d50d5441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Token id: 128004 and Pad Token: <|finetune_right_pad_id|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n",
      "Padding side: right\n"
     ]
    }
   ],
   "source": [
    "# del base_model\n",
    "# del peft_model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Specify the checkpoint directory\n",
    "checkpoint_dir = f\"{model_dir}/chkpts/checkpoint-{42}\"\n",
    "print(checkpoint_dir)\n",
    "# Load the tokenizer (ensure it's the same tokenizer used for training)\n",
    "\n",
    "# Load the base model\n",
    "peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    device_map=\"cuda:0\",\n",
    "    # cache_dir=\"/model\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    cache_dir=f\"/model/{model_id}/cache\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "print(f\"Padding side: {tokenizer.padding_side}\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# \n",
    "\n",
    "\n",
    "# Make sure the tokenizer is ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Local saving\n",
    "    peft_model.save_pretrained(\"lora_i2i\") \n",
    "    tokenizer.save_pretrained(\"lora_i2i\")\n",
    "\n",
    "    # For merging the LoRA adapters with the base model and save the model to 16-bit precision for optimized performance with vLLM, use:\n",
    "    # # Merge to 16bit\n",
    "    peft_model.save_pretrained_merged(\"i2i_merged_16bit\", tokenizer, save_method = \"merged_16bit\",)\n",
    "    peft_model.save_pretrained_merged(\"i2i_merged_4bit\", tokenizer, save_method = \"merged_4bit_forced\",)\n",
    "    # model.push_to_hub_merged(\"<hf_username/model_name>\", tokenizer, save_method = \"merged_16bit\", token = hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=1110, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1110, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=1110, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1110, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=1110, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1110, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=1110, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1110, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=1110, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1110, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=1110, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1110, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=1110, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=1110, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "if False:\n",
    "    # del peft_model\n",
    "    torch.cuda.empty_cache()\n",
    "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"i2i_merged_16bit\",        # Trained model either locally or from huggingface\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = False,\n",
    "        attn_implementation=attn_implementation,\n",
    "        # device_map=[device],\n",
    "        local_files_only=True\n",
    "    )\n",
    "FastLanguageModel.for_inference(peft_model)  # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Instruction(['q', 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ê°€ ìµœê³ ì¸ ë‚ ì§œ ì•Œë ¤ì¤˜.', 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ê°€ ìµœê³ ì¸ ë‚ ì§œ', 'ê°’', Semantic(Temporal=[('ì´ë²ˆë‹¬', '2022-09-01 00:00:00 ~ 2022-09-30 23:59:59')], Spatial=['ìš°ë¦¬ë°˜'], Modality=['ì‹¤ë‚´ì˜¨ë„'], Operation=['ìµœê³ ì¸'], Target=['ë‚ ì§œ'])]), Instruction(['r', \"ì˜ˆ) 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ ì˜¨ë„ê°€ ê°€ì¥ ë”ì› ë˜ ë‚ ì€ 2022ë…„ 9ì›” 15ì¼ì…ë‹ˆë‹¤.'\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "\n",
    "def extract_content(text):\n",
    "    # Define the regex pattern to extract the content\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def run(query):\n",
    "    start_time = time.time()\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": f\"{common_prompt}\"},\n",
    "        # {\"role\": \"metadata\", \"content\": f\"{current_metadata}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{current_metadata};{query}\"},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = peft_model.generate(\n",
    "        input_ids = inputs,\n",
    "        max_new_tokens = max_seq_length,\n",
    "        use_cache = True,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    # print(response)\n",
    "    # print(\"Extracting..\")\n",
    "    parsed_response = extract_content(response)\n",
    "    pprint.pprint(f\"Query: {query}, Time: {time.time() - start_time}\")\n",
    "    print(parsed_response)\n",
    "    return parsed_response\n",
    "    \n",
    "    # text_streamer = transformers.TextStreamer(\n",
    "    #     tokenizer, \n",
    "    # skip_prompt = True\n",
    "    # )\n",
    "    # _ = peft_model.generate(\n",
    "    #     input_ids = inputs, \n",
    "    #     streamer = text_streamer, \n",
    "    #     max_new_tokens = max_seq_length, \n",
    "    #     use_cache = True,\n",
    "    #     pad_token_id = tokenizer.eos_token_id,\n",
    "    #     eos_token_id = tokenizer.eos_token_id,\n",
    "    # )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Query: ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì˜¨ë„ì°¨ì´ ì•Œë ¤ì¤˜, Time: 17.82789659500122'\n",
      "{'Formalized Input': 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.', 'Input Semantic Parsing': {'Temporal': {'ì˜¤ëŠ˜': '2022-09-30 00:00:00 ~ 2022-09-30 23:59:59'}, 'Spatial': {'ìš°ë¦¬ë°˜': '01_IB5', 'ì˜†ë°˜': '01_IB7'}, 'Modality': {'ì‹¤ë‚´ì˜¨ë„': 'roomtemp'}, 'Operation': ['í‰ê· ì˜ ì°¨ì´']}, 'Strategy': {'Objective': 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ ê°ê°ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ í›„, ë‘ ê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨.', 'Expected Outputs': ['ìš°ë¦¬ë°˜(19.3â„ƒ)ì´ ì˜†ë°˜(17.0â„ƒ)ë³´ë‹¤ 2.3â„ƒ ë†’ìŠµë‹ˆë‹¤.'], 'Step': ['ìš°ë¦¬ë°˜ì˜ ì˜¤ëŠ˜ ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ê²°ê³¼ qr_roomtemp_ours ì— ì €ì¥', 'ì˜†ë°˜ì˜ ì˜¤ëŠ˜ ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ê²°ê³¼ qr_roomtemp_beside ì— ì €ì¥', 'ê° ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê·  ê³„ì‚°', 'ê° ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ì™€ ì°¨ì´ ë°˜í™˜']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-30 00:00:00' AND '2022-09-30 23:59:59'\"],'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}},'result': {'name': 'qr_roomtemp_ours'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-30 00:00:00' AND '2022-09-30 23:59:59'\"],'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB7')\"}},'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'o', 'python_script_to_process': \"avg_roomtemp_ours = qr_roomtemp_ours['roomtemp'].mean(); avg_roomtemp_beside = qr_roomtemp_beside['roomtemp'].mean(); temp_diff = avg_roomtemp_ours - avg_roomtemp_beside\",'return': ['avg_roomtemp_ours', 'avg_roomtemp_beside', 'temp_diff']}, {'type': 'r', 'templates': ['{room_with_higher_temp}({avg_roomtemp_higher}â„ƒ)ì´ {room_with_lower_temp}({avg_roomtemp_lower}â„ƒ) ë³´ë‹¤ {temp_diff}â„ƒ ë§Œí¼ ë†’ìŠµë‹ˆë‹¤.']}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\'Formalized Input\\': \\'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.\\', \\'Input Semantic Parsing\\': {\\'Temporal\\': {\\'ì˜¤ëŠ˜\\': \\'2022-09-30 00:00:00 ~ 2022-09-30 23:59:59\\'}, \\'Spatial\\': {\\'ìš°ë¦¬ë°˜\\': \\'01_IB5\\', \\'ì˜†ë°˜\\': \\'01_IB7\\'}, \\'Modality\\': {\\'ì‹¤ë‚´ì˜¨ë„\\': \\'roomtemp\\'}, \\'Operation\\': [\\'í‰ê· ì˜ ì°¨ì´\\']}, \\'Strategy\\': {\\'Objective\\': \\'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ ê°ê°ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ í›„, ë‘ ê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨.\\', \\'Expected Outputs\\': [\\'ìš°ë¦¬ë°˜(19.3â„ƒ)ì´ ì˜†ë°˜(17.0â„ƒ)ë³´ë‹¤ 2.3â„ƒ ë†’ìŠµë‹ˆë‹¤.\\'], \\'Step\\': [\\'ìš°ë¦¬ë°˜ì˜ ì˜¤ëŠ˜ ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ê²°ê³¼ qr_roomtemp_ours ì— ì €ì¥\\', \\'ì˜†ë°˜ì˜ ì˜¤ëŠ˜ ì‹¤ë‚´ì˜¨ë„ ì¿¼ë¦¬ê²°ê³¼ qr_roomtemp_beside ì— ì €ì¥\\', \\'ê° ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ í‰ê·  ê³„ì‚°\\', \\'ê° ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ì™€ ì°¨ì´ ë°˜í™˜\\']}, \\'Instruction Set\\': [{\\'type\\': \\'q\\', \\'run\\': {\\'args\\': {\\'table_name\\': \\'data_t\\', \\'columns\\': [\\'roomtemp\\'], \\'conditions\\': [\"timestamp BETWEEN \\'2022-09-30 00:00:00\\' AND \\'2022-09-30 23:59:59\\'\"],\\'subquery\\': \"idu_id = (SELECT id FROM idu_t WHERE name = \\'01_IB5\\')\"}},\\'result\\': {\\'name\\': \\'qr_roomtemp_ours\\'}}, {\\'type\\': \\'q\\', \\'run\\': {\\'args\\': {\\'table_name\\': \\'data_t\\', \\'columns\\': [\\'roomtemp\\'], \\'conditions\\': [\"timestamp BETWEEN \\'2022-09-30 00:00:00\\' AND \\'2022-09-30 23:59:59\\'\"],\\'subquery\\': \"idu_id = (SELECT id FROM idu_t WHERE name = \\'01_IB7\\')\"}},\\'result\\': {\\'name\\': \\'qr_roomtemp_beside\\'}}, {\\'type\\': \\'o\\', \\'python_script_to_process\\': \"avg_roomtemp_ours = qr_roomtemp_ours[\\'roomtemp\\'].mean(); avg_roomtemp_beside = qr_roomtemp_beside[\\'roomtemp\\'].mean(); temp_diff = avg_roomtemp_ours - avg_roomtemp_beside\",\\'return\\': [\\'avg_roomtemp_ours\\', \\'avg_roomtemp_beside\\', \\'temp_diff\\']}, {\\'type\\': \\'r\\', \\'templates\\': [\\'{room_with_higher_temp}({avg_roomtemp_higher}â„ƒ)ì´ {room_with_lower_temp}({avg_roomtemp_lower}â„ƒ) ë³´ë‹¤ {temp_diff}â„ƒ ë§Œí¼ ë†’ìŠµë‹ˆë‹¤.\\']}]}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì˜¨ë„ì°¨ì´ ì•Œë ¤ì¤˜\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Query: ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜, Time: 10.161221981048584'\n",
      "{'Formalized Input': 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ë¥¼ ì•Œë ¤ì¤˜.', 'Input Semantic Parsing': {'Temporal': {'ì˜¤ëŠ˜': '2022-09-30 00:00:00 ~ 2022-09-30 23:59:59'}, 'Spatial': {'ìš°ë¦¬ë°˜': '01_IB5'}, 'Modality': {'ì‹¤ë‚´ì˜¨ë„': 'roomtemp'}, 'Operation': ['í‰ê· ']}, 'Strategy': {'Objective': 'ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì¿¼ë¦¬ í›„, í‰ê· ê°’ ê³„ì‚°.', 'Expected Outputs': ['ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” 24â„ƒ ì…ë‹ˆë‹¤.'], 'Step': ['ìš°ë¦¬ë°˜ì˜ ì˜¤ëŠ˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„° ì¿¼ë¦¬ê²°ê³¼ qr_roomtemp_todayì— ì €ì¥', 'qr_roomtemp_todayì—ì„œ roomtempì˜ í‰ê· ê°’ ë°˜í™˜']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-30 00:00:00' AND '2022-09-30 23:59:59'\"],'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}},'result': {'name': 'qr_roomtemp_today'}}, {'type': 'o', 'python_script_to_process': \"avg_roomtemp = qr_roomtemp_today['roomtemp'].mean()\",'return': ['avg_roomtemp']}, {'type': 'r', 'templates': ['ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ëŠ” {avg_roomtemp}â„ƒ ì…ë‹ˆë‹¤.']}]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m run(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì˜¤ëŠ˜ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mì–´ì œ ì˜†ë°˜ ì˜¨ë„ í‰ê·  ì•Œë ¤ì¤˜\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m run(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mí˜„ì¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     13\u001b[0m chat \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# {\"role\": \"metadata\", \"content\": f\"{current_metadata}\"},\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_metadata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     20\u001b[0m     chat,\n\u001b[1;32m     21\u001b[0m     tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m     add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpeft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(response)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(\"Extracting..\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/unsloth/models/llama.py:1508\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[0;32m-> 1508\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/peft/peft_model.py:1838\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1837\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1838\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1840\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/unsloth/models/llama.py:972\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    956\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    969\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 972\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    980\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/unsloth/models/llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[0;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m    923\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    924\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 925\u001b[0m hidden_states, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaAttention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m    935\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/unsloth/models/llama.py:230\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[0;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaged_attention_K[seq_len] \u001b[38;5;241m=\u001b[39m Kn\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaged_attention_V[seq_len] \u001b[38;5;241m=\u001b[39m Vn\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m Kn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaged_attention_K\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mkv_seq_len\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m Vn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaged_attention_V[:kv_seq_len]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Handle sliding windows\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "run(\"ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜\")\n",
    "\n",
    "run(\"ì–´ì œ ì˜†ë°˜ ì˜¨ë„ í‰ê·  ì•Œë ¤ì¤˜\")\n",
    "run(\"í˜„ì¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "ë‚˜ëŠ” í›Œë£¡í•œ HVAC ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì—¬ì„œ, ê·¸ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë‚´ ì„ë¬´ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì¼ ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ ì ˆì°¨ë¥¼ ë”°ë¼ì•¼ í•œë‹¤.\n",
      "<ì¶œë ¥ ë‚´ìš©>\n",
      "1. 'Formalized Input': ì‚¬ìš©ìì˜ ì¶”ìƒì  ì§ˆë¬¸ì„ êµ¬ì²´í™” ë° ì •ê·œí™”í•œ ê²°ê³¼. ë‹¤ì–‘í•œ í˜•íƒœì˜ ì§ˆë¬¸ë“¤ì„ ê°€ì¥ í•µì‹¬ì ì´ê³  ê·¼ë³¸ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•œ ê²°ê³¼.\n",
      "2. 'Input Semantic Parsing': Input Semantic Parsing ê²°ê³¼. dict í˜•íƒœë¡œ êµ¬ì„±ë˜ë©°, Temporal, Spatial, Modality, Operationì„ ê°€ì§.\n",
      "3. 'Strategy': ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì „ëµì„ ë‹µë³€ ì „ ê³ ë¯¼. Objective: ì§ˆë¬¸ì˜ ê·¼ë³¸ì  ì˜ë„ ë° ë‹µë³€ì˜ ëª©ì . Expected Output: ë‹µë³€ì˜ ì˜ˆìƒ ê²°ê³¼. Step: ë‹µë³€ì„ ìœ„í•œ êµ¬ì²´ìœ¼ë¡œ ìª¼ê°œì§„ ë‹¨ê³„.\n",
      "4. 'Instruction Set': ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‚˜ì˜ ì‹¤ì œ í–‰ë™.\n",
      "\n",
      "<DDL statement>\n",
      "CREATE TABLE IF NOT EXISTS data_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
      "    idu_id integer,\n",
      "    roomtemp double precision,\n",
      "    settemp double precision,\n",
      "    oper boolean,\n",
      "    \"timestamp\" timestamp without time zone NOT NULL\n",
      ")\n",
      "    \n",
      "CREATE TABLE IF NOT EXISTS idu_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
      "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
      "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
      "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
      ")<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "{'site_name': 'YongDongIllHighSchool', 'user_name': 'í™ê¸¸ë™', 'user_role': 'customer', 'idu_name': '01_IB5', 'idu_mapping': {'01_IB5': ['ìš°ë¦¬ë°˜'], '01_IB7': ['ì˜†ë°˜'], '02_IB1': ['ì•ë°˜']},'modality_mapping': {'roomtemp': ['ì‹¤ë‚´ì˜¨ë„'],'settemp': ['ì„¤ì •ì˜¨ë„'], 'oper': ['ì „ì›']}, 'ë§íˆ¬': 'ì¡´ëŒ“ë§', 'ì–¸ì–´': 'í•œêµ­ì–´', 'current_datetime': '2022-09-30 12:00:00'};ë¡¯ë°ìºìŠ¬ì˜ í˜„ì¬ ì˜¨ë„ ì•Œë ¤ì¤˜<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{'Formalized Input': 'ë¡¯ë°ìºìŠ¬ì˜ í˜„ì¬ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì•Œë ¤ì¤˜.', 'Input Semantic Parsing': {'Temporal': {'í˜„ì¬': '2022-09-30 12:00:00'}, 'Spatial': {'ë¡¯ë°ìºìŠ¬': 'Unknown'}, 'Modality': {'ì‹¤ë‚´ì˜¨ë„': 'roomtemp'}, 'Operation': ['ì§ˆë¬¸ ì‘ë‹µ']}, 'Strategy': {'Objective': 'Unknown spatial entityì˜ ì •ì˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³´ê³ , í•´ë‹¹ ì •ë³´ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•¨', 'Expected Outputs': ['ë¡¯ë°ìºìŠ¬ì´ ì–´ë–¤ ê³³ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”.'], 'Step': ['Unknown spatial entityì˜ ì •ì˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë´„.', 'ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë³€ìˆ˜ì— ì €ì¥']}, 'Instruction Set': [{'type': 'r', 'templates': ['ë¡¯ë°ìºìŠ¬ì´ ì–´ë–¤ ê³³ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”.']}]}<|eot_id|>\n",
      "Extracting..\n",
      "'Query: ë¡¯ë°ìºìŠ¬ì˜ í˜„ì¬ ì˜¨ë„ ì•Œë ¤ì¤˜, Time: 5.339292526245117'\n",
      "{'Formalized Input': 'ë¡¯ë°ìºìŠ¬ì˜ í˜„ì¬ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì•Œë ¤ì¤˜.', 'Input Semantic Parsing': {'Temporal': {'í˜„ì¬': '2022-09-30 12:00:00'}, 'Spatial': {'ë¡¯ë°ìºìŠ¬': 'Unknown'}, 'Modality': {'ì‹¤ë‚´ì˜¨ë„': 'roomtemp'}, 'Operation': ['ì§ˆë¬¸ ì‘ë‹µ']}, 'Strategy': {'Objective': 'Unknown spatial entityì˜ ì •ì˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³´ê³ , í•´ë‹¹ ì •ë³´ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•¨', 'Expected Outputs': ['ë¡¯ë°ìºìŠ¬ì´ ì–´ë–¤ ê³³ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”.'], 'Step': ['Unknown spatial entityì˜ ì •ì˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë´„.', 'ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë³€ìˆ˜ì— ì €ì¥']}, 'Instruction Set': [{'type': 'r', 'templates': ['ë¡¯ë°ìºìŠ¬ì´ ì–´ë–¤ ê³³ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”.']}]}\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Feb 2025\n",
      "\n",
      "ë‚˜ëŠ” í›Œë£¡í•œ HVAC ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì—¬ì„œ, ê·¸ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë‚´ ì„ë¬´ì´ë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ë“¤ì¼ ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ ì ˆì°¨ë¥¼ ë”°ë¼ì•¼ í•œë‹¤.\n",
      "<ì¶œë ¥ ë‚´ìš©>\n",
      "1. 'Formalized Input': ì‚¬ìš©ìì˜ ì¶”ìƒì  ì§ˆë¬¸ì„ êµ¬ì²´í™” ë° ì •ê·œí™”í•œ ê²°ê³¼. ë‹¤ì–‘í•œ í˜•íƒœì˜ ì§ˆë¬¸ë“¤ì„ ê°€ì¥ í•µì‹¬ì ì´ê³  ê·¼ë³¸ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•œ ê²°ê³¼.\n",
      "2. 'Input Semantic Parsing': Input Semantic Parsing ê²°ê³¼. dict í˜•íƒœë¡œ êµ¬ì„±ë˜ë©°, Temporal, Spatial, Modality, Operationì„ ê°€ì§.\n",
      "3. 'Strategy': ì§ˆë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì „ëµì„ ë‹µë³€ ì „ ê³ ë¯¼. Objective: ì§ˆë¬¸ì˜ ê·¼ë³¸ì  ì˜ë„ ë° ë‹µë³€ì˜ ëª©ì . Expected Output: ë‹µë³€ì˜ ì˜ˆìƒ ê²°ê³¼. Step: ë‹µë³€ì„ ìœ„í•œ êµ¬ì²´ìœ¼ë¡œ ìª¼ê°œì§„ ë‹¨ê³„.\n",
      "4. 'Instruction Set': ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‚˜ì˜ ì‹¤ì œ í–‰ë™.\n",
      "\n",
      "<DDL statement>\n",
      "CREATE TABLE IF NOT EXISTS data_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
      "    idu_id integer,\n",
      "    roomtemp double precision,\n",
      "    settemp double precision,\n",
      "    oper boolean,\n",
      "    \"timestamp\" timestamp without time zone NOT NULL\n",
      ")\n",
      "    \n",
      "CREATE TABLE IF NOT EXISTS idu_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
      "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
      "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
      "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
      ")<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "{'site_name': 'YongDongIllHighSchool', 'user_name': 'í™ê¸¸ë™', 'user_role': 'customer', 'idu_name': '01_IB5', 'idu_mapping': {'01_IB5': ['ìš°ë¦¬ë°˜'], '01_IB7': ['ì˜†ë°˜'], '02_IB1': ['ì•ë°˜']},'modality_mapping': {'roomtemp': ['ì‹¤ë‚´ì˜¨ë„'],'settemp': ['ì„¤ì •ì˜¨ë„'], 'oper': ['ì „ì›']}, 'ë§íˆ¬': 'ì¡´ëŒ“ë§', 'ì–¸ì–´': 'í•œêµ­ì–´', 'current_datetime': '2022-09-30 12:00:00'};10ë…„ì „ ìš°ë¦¬ë°˜ ì˜¨ë„ ì•Œë ¤ì¤˜<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{'Formalized Input': '10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì•Œë ¤ì¤˜.', 'Input Semantic Parsing': {'Temporal': {'10ë…„ ì „': '2012-09-30 00:00:00 ~ 2012-09-30 23:59:59'}, 'Spatial': {'ìš°ë¦¬ë°˜': '01_IB5'}, 'Modality': {'ì‹¤ë‚´ì˜¨ë„': 'roomtemp'}, 'Operation': ['ì§ˆë¬¸ ì‘ë‹µ']}, 'Strategy': {'Objective': '10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì¡°íšŒí•¨.', 'Expected Outputs': ['10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 24â„ƒì˜€ìŠµë‹ˆë‹¤.'], 'Step': ['ìš°ë¦¬ë°˜ì˜ 10ë…„ ì „ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•´ qr_roomtempì— ì €ì¥', 'qr_roomtempì—ì„œ ì‹¤ë‚´ì˜¨ë„ ë°˜í™˜']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2012-09-30 00:00:00' AND '2012-09-30 23:59:59'\"],'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}},'result': {'name': 'qr_roomtemp'}}, {'type': 'o', 'python_script_to_process': \"roomtemp = qr_roomtemp['roomtemp'].values[0]\",'return': ['roomtemp']}, {'type': 'r', 'templates': ['10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” {roomtemp}â„ƒì˜€ìŠµë‹ˆë‹¤.']}]}<|eot_id|>\n",
      "Extracting..\n",
      "'Query: 10ë…„ì „ ìš°ë¦¬ë°˜ ì˜¨ë„ ì•Œë ¤ì¤˜, Time: 9.927461385726929'\n",
      "{'Formalized Input': '10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì•Œë ¤ì¤˜.', 'Input Semantic Parsing': {'Temporal': {'10ë…„ ì „': '2012-09-30 00:00:00 ~ 2012-09-30 23:59:59'}, 'Spatial': {'ìš°ë¦¬ë°˜': '01_IB5'}, 'Modality': {'ì‹¤ë‚´ì˜¨ë„': 'roomtemp'}, 'Operation': ['ì§ˆë¬¸ ì‘ë‹µ']}, 'Strategy': {'Objective': '10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì¡°íšŒí•¨.', 'Expected Outputs': ['10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 24â„ƒì˜€ìŠµë‹ˆë‹¤.'], 'Step': ['ìš°ë¦¬ë°˜ì˜ 10ë…„ ì „ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•´ qr_roomtempì— ì €ì¥', 'qr_roomtempì—ì„œ ì‹¤ë‚´ì˜¨ë„ ë°˜í™˜']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2012-09-30 00:00:00' AND '2012-09-30 23:59:59'\"],'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}},'result': {'name': 'qr_roomtemp'}}, {'type': 'o', 'python_script_to_process': \"roomtemp = qr_roomtemp['roomtemp'].values[0]\",'return': ['roomtemp']}, {'type': 'r', 'templates': ['10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” {roomtemp}â„ƒì˜€ìŠµë‹ˆë‹¤.']}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\'Formalized Input\\': \\'10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì•Œë ¤ì¤˜.\\', \\'Input Semantic Parsing\\': {\\'Temporal\\': {\\'10ë…„ ì „\\': \\'2012-09-30 00:00:00 ~ 2012-09-30 23:59:59\\'}, \\'Spatial\\': {\\'ìš°ë¦¬ë°˜\\': \\'01_IB5\\'}, \\'Modality\\': {\\'ì‹¤ë‚´ì˜¨ë„\\': \\'roomtemp\\'}, \\'Operation\\': [\\'ì§ˆë¬¸ ì‘ë‹µ\\']}, \\'Strategy\\': {\\'Objective\\': \\'10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ë¥¼ ì¡°íšŒí•¨.\\', \\'Expected Outputs\\': [\\'10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” 24â„ƒì˜€ìŠµë‹ˆë‹¤.\\'], \\'Step\\': [\\'ìš°ë¦¬ë°˜ì˜ 10ë…„ ì „ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•´ qr_roomtempì— ì €ì¥\\', \\'qr_roomtempì—ì„œ ì‹¤ë‚´ì˜¨ë„ ë°˜í™˜\\']}, \\'Instruction Set\\': [{\\'type\\': \\'q\\', \\'run\\': {\\'args\\': {\\'table_name\\': \\'data_t\\', \\'columns\\': [\\'roomtemp\\'], \\'conditions\\': [\"timestamp BETWEEN \\'2012-09-30 00:00:00\\' AND \\'2012-09-30 23:59:59\\'\"],\\'subquery\\': \"idu_id = (SELECT id FROM idu_t WHERE name = \\'01_IB5\\')\"}},\\'result\\': {\\'name\\': \\'qr_roomtemp\\'}}, {\\'type\\': \\'o\\', \\'python_script_to_process\\': \"roomtemp = qr_roomtemp[\\'roomtemp\\'].values[0]\",\\'return\\': [\\'roomtemp\\']}, {\\'type\\': \\'r\\', \\'templates\\': [\\'10ë…„ ì „ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ëŠ” {roomtemp}â„ƒì˜€ìŠµë‹ˆë‹¤.\\']}]}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"ë¡¯ë°ìºìŠ¬ì˜ í˜„ì¬ ì˜¨ë„ ì•Œë ¤ì¤˜\")\n",
    "run(\"10ë…„ì „ ìš°ë¦¬ë°˜ ì˜¨ë„ ì•Œë ¤ì¤˜\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
