{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "from unsloth import FastLanguageModel, unsloth_train\n",
    "\n",
    "from transformers import TrainerCallback, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 0     # Unsloth auto supports RoPE Scaling internally!\n",
    "# dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "device = f\"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'sh2orc/Llama-3.1-Korean-8B-Instruct'\n",
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = 'Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B'\n",
    "# model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download(repo_id=model_id, local_dir=\"70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizer initialization\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     cache_dir=f\"{model_dir}/cache\",\n",
    "#     # attn_implementation=attn_implementation,\n",
    "#     local_files_only=True,\n",
    "#     device_map=\"cuda\"\n",
    "# )\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\n",
    "# #     model_id,\n",
    "# #     cache_dir=f\"{model_dir}/cache\",\n",
    "# #     local_files_only=True\n",
    "# # )\n",
    "# # if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "# pretrained_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer initialization\n",
    "\n",
    "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = torch_dtype,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_use_double_quant=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     # load_in_8bit=True,\n",
    "    #     # llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    # ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # local_files_only=True\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "\n",
    "# if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "#     pretrained_model.save_pretrained(model_dir)\n",
    "#     tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = [\n",
    "    \"woall\", # 0\n",
    "    \"ours\" # 1\n",
    "][1]\n",
    "print(f\"train_type: {train_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DATASET_DIR = Path(\"../dataset/v5-250228-multimetadata\")\n",
    "# dataset_name = \"v6-250306-optimizetoken\"\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "BASE_DATASET_DIR = Path(f\"../dataset/{dataset_name}\")\n",
    "print(f\"BASE_DATASET_DIR: {BASE_DATASET_DIR}\")\n",
    "print(list(BASE_DATASET_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# current_metadata = json.load(open(\"metadata.json\", \"r\"))\n",
    "\n",
    "\n",
    "common_prompt = open(BASE_DATASET_DIR / f\"prompt.txt\", \"r\").read()\n",
    "\n",
    "if train_type in [\"woall\"]:\n",
    "    # search <|FI|>~~<|FI|> and remove between them\n",
    "    common_prompt = re.sub(r\"\\n?<\\|Ours\\|>(.|\\n)*?<\\|Ours\\|>\", \"\", common_prompt)\n",
    "\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "print(common_prompt)\n",
    "\n",
    "# print(common_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_dirs = [d for d in BASE_DATASET_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]\n",
    "print(scenario_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_dataset(dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if \"v6\" in dataset_name:\n",
    "            if train_type in [\"woall\"]:\n",
    "                del d[\"Response\"][\"생각\"]\n",
    "        elif \"v7\" in dataset_name:\n",
    "            if train_type in [\"woall\"]:\n",
    "                del d[\"Response\"][\"Thinking\"]\n",
    "                expectations = d[\"Response\"][\"Expectations\"]\n",
    "                del d[\"Response\"][\"Expectations\"]\n",
    "                d[\"Response\"][\"Expectations\"] = expectations\n",
    "        \n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "\n",
    "        skip_tags = [\"Reason\"]\n",
    "\n",
    "        skip = False\n",
    "        for skip_tag in skip_tags:\n",
    "            if skip_tag in tags:\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "dataset_trs = []\n",
    "dataset_tss = []\n",
    "for scenario_dir in scenario_dirs:\n",
    "    dataset_trs.extend(read_dataset(scenario_dir, \"onlyq_tr.json\"))\n",
    "    dataset_tss.extend(read_dataset(scenario_dir, \"onlyq_ts.json\"))\n",
    "\n",
    "dataset_tr = Dataset.from_list(dataset_trs) # 서로 다른 메타데이터 합치면서\n",
    "dataset_ts = Dataset.from_list(dataset_tss) # Mutually exclusive한 애들은 None 됨\n",
    "\n",
    "max_seq_length = 0\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for metadata, input, response in zip(examples['Metadata'], examples['Input'], examples['Response']):\n",
    "        # global max_seq_length\n",
    "        response.replace(\"    \", \"\")\n",
    "\n",
    "        # print(metadata['current_datetime'])\n",
    "        # print(metadata['idu_mapping'])\n",
    "\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        if \"llama\" in model_id.lower():\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            user_input = {\n",
    "                \"content\": f\"Metadata:{metadata};Input:{input};\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "        elif \"gemma\" in model_id.lower():\n",
    "            user_input = {\n",
    "                \"content\": f\"{common_prompt};{metadata};{input}\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([user_input, answer])\n",
    "        \n",
    "        \n",
    "        # print(\"Answer length: \", len(response))\n",
    "        # convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        # if len(response) + 50 > max_seq_length:\n",
    "        #     max_seq_length = len(response) + len(metadata) + len(input) + 50\n",
    "            # print(response)\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    \n",
    "    # remove \\n\\nCutting Knowledge Date: BLAH BLAH \\nToday Date: BLAH BLAH\\n\\n using regex\n",
    "    # texts = [re.sub(r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', '', text) for text in texts]\n",
    "\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset_tr = dataset_tr.map(formatting_prompts_func, batched=True)\n",
    "dataset_ts = dataset_ts.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "max_seq_length = max([len(tokenizer.encode(dataset_tr[i]['text'])) for i in range(len(dataset_tr))]) + 100\n",
    "# max_seq_length += len(common_prompt)\n",
    "print(max_seq_length)\n",
    "print(dataset_tr[0])\n",
    "print(len(dataset_tr), len(dataset_ts))\n",
    "# print(f\"seq length: {len(tokenizer.encode(dataset_tr[0]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 256\n",
    "lora_alpha = lora_r * 2\n",
    "lora_repr = f\"v7_r{lora_r}_a{lora_alpha}_{train_type}_16bit_adamw16bit_0322\"\n",
    "print(lora_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    pretrained_model,\n",
    "    r=lora_r,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    # \"embed_tokens\",\n",
    "                    # \"lm_head\"\n",
    "                    ],\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None,   # No LoftQ, for standard fine-tuning\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "# del pretrained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(len(dataset_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(f\"{model_dir}/chkpts/{lora_repr}\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "per_device_train_batch_size, epochs = 25, 200 # 8\n",
    "gradient_accumulation_steps = int(np.ceil(len(dataset_tr) / per_device_train_batch_size))\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "# clear all checkpoints\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # num_train_epochs = 1,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,  # Controls the batch size per device\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # Accumulates gradients to simulate a larger batch\n",
    "    max_steps=gradient_accumulation_steps * epochs,\n",
    "    # 리소스 제약때문에 batch size를 타협해야하는 경우가 발생 -> micro batch size를 줄이고,\n",
    " \t# accumulated step을 늘려, 적절한 size로 gradient를 구해 weight update\n",
    "    # https://www.youtube.com/watch?v=ptlmj9Y9iwE\n",
    "    warmup_steps = gradient_accumulation_steps,\n",
    "    learning_rate = 1e-4,             # Sets the learning rate for optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_torch\", # adamw_torch, adafactor, prodigy\n",
    "    weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "    lr_scheduler_type = \"cosine\",  # Sets the learning rate scheduler\n",
    "    seed = 3407,                        \n",
    "    output_dir = f\"{model_dir}/chkpts/{lora_repr}\",  # Output directory for checkpoints and predictions     \n",
    "    report_to = \"none\",              # Enables Weights & Biases (W&B) logging\n",
    "    logging_steps = gradient_accumulation_steps,                # Sets frequency of logging to W&B\n",
    "    logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "    evaluation_strategy=\"steps\",  # enable evaluation during training\n",
    "    eval_steps=gradient_accumulation_steps,\n",
    "    eval_accumulation_steps=1, # 낮을수록 eval시 사용하는 메모리 줄어듦\n",
    "    save_steps=gradient_accumulation_steps,\n",
    "    save_strategy = \"steps\",               \n",
    "    # load_best_model_at_end = True,    # Loads the best model at the end\n",
    "    save_only_model = False,           # Saves entire model, not only weights\n",
    "    resume_from_checkpoint = f\"{model_dir}/chkpts/{lora_repr}\",  # Resumes training from a checkpoint\n",
    ")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     pred_ids = eval_pred.predictions\n",
    "#     print(pred_ids, type(pred_ids), pred_ids.shape)\n",
    "#     predictions = tokenizer.batch_decode(\n",
    "#         pred_ids, \n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "#     print(predictions)\n",
    "#     return {\"accuracy\": 0}\n",
    "\n",
    "# class CustomCallback(TrainerCallback):\n",
    "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "#         print(f\"Step {state.global_step}: {logs}\")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_tr,\n",
    "    eval_dataset = dataset_ts,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,        # Can make training 5x faster for short sequences.\n",
    "    args = args,\n",
    "    # compute_metrics = compute_metrics,\n",
    "    # callbacks = [CustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base model\n",
    "\n",
    "# save_dir = f\"{model_dir}/gguf\"\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# command = (\n",
    "#     f\"python ../../llama.cpp/convert_hf_to_gguf.py \"\n",
    "#     # f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "#     f\"--outfile {save_dir}/base.gguf \"              # Output file for the GGUF model\n",
    "#     f\"--outtype f16 \"                      # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "#     f\"--verbose \"                            # Optional: increase logging output\n",
    "#     f\"{model_dir}\"                      # Positional argument: path to the LoRA adapter files\n",
    "# )\n",
    "\n",
    "# print(command)\n",
    "\n",
    "# os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 56\n",
    "\n",
    "# lora_repr = \"v7_r8_a16_ours_70B\"\n",
    "checkpoint_dir = f\"{model_dir}/chkpts/{lora_repr}/checkpoint-{step}\"\n",
    "output_path = f\"{model_dir}/gguf/{lora_repr}-checkpoint-{step}.gguf\"\n",
    "lora_output_dir = f\"{model_dir}/lora_output/\"\n",
    "\n",
    "if not os.path.exists(f\"{model_dir}/gguf\"):\n",
    "    os.makedirs(f\"{model_dir}/gguf\")\n",
    "print(checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(lora_output_dir):\n",
    "    os.makedirs(lora_output_dir)\n",
    "print(checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     torch_dtype=torch_dtype,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     # attn_implementation=attn_implementation,\n",
    "    #     local_files_only=True,\n",
    "    #     device_map=\"cuda\"\n",
    "    # )\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     local_files_only=True\n",
    "    # )\n",
    "    \n",
    "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        dtype = torch_dtype,\n",
    "        attn_implementation=attn_implementation,\n",
    "        load_in_4bit = False,\n",
    "        load_in_8bit=False,\n",
    "        cache_dir=f\"{model_dir}/cache\",\n",
    "        local_files_only=True,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    # FastLanguageModel.for_inference(model)\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # merge lora model and base pretrained model\n",
    "    # model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# peft_model.save_pretrained_gguf(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, quantization_method = \"q8_0\")\n",
    "peft_model.save_pretrained_merged(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, save_method=\"merged_16bit\")\n",
    "# tokenizer.save_pretrained(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# command = (\n",
    "#     f\"python ../../llama.cpp/convert_lora_to_gguf.py \"\n",
    "#     f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "#     f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "#     f\"--outtype f16 \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "#     f\"--verbose \"                            # Optional: increase logging output\n",
    "#     f\"{checkpoint_dir}\"                      # Positional argument: path to the LoRA adapter files\n",
    "# )\n",
    "\n",
    "command = (\n",
    "    f\"python ../../llama.cpp/convert_hf_to_gguf.py \"\n",
    "    # f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "    f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "    f\"--outtype f16 \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "    f\"--verbose \"                            # Optional: increase logging output\n",
    "    f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\"                      # Positional argument: path to the LoRA adapter files\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = (\n",
    "#     f\"../../llama.cpp/build/bin/llama-quantize \"\n",
    "#     f\"{output_path} \"\n",
    "#     f\"{output_path.replace('.gguf', '-Q4_K_M.gguf')} \"                      # Positional argument: path to the LoRA adapter files\n",
    "#     f\"Q4_K_M \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"Running command:\", command)\n",
    "# os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy(\n",
    "    f\"{output_path}\",\n",
    "    f\"../../src/i2i.gguf\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -cvf - ../../src/i2i.gguf | pigz -p 128 > src/i2i.tar.gz\n",
    "\n",
    "command = (\n",
    "    f\"tar -cvf - ../../src/i2i.gguf | pigz -p 128 > ../../src/i2i.tar.gz\"\n",
    ")\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
