{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from unsloth import FastLanguageModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_type: ours\n"
     ]
    }
   ],
   "source": [
    "train_type = [\n",
    "    \"woall\", # 0\n",
    "    \"FI\", # 1\n",
    "    \"ISP\", # 2\n",
    "    \"ours\" # 3\n",
    "][0]\n",
    "print(f\"train_type: {train_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DATASET_DIR: ../dataset/v5-250228-multimetadata\n",
      "[PosixPath('../dataset/v5-250228-multimetadata/scenario3'), PosixPath('../dataset/v5-250228-multimetadata/scenario1'), PosixPath('../dataset/v5-250228-multimetadata/scenario2'), PosixPath('../dataset/v5-250228-multimetadata/prompt.txt')]\n"
     ]
    }
   ],
   "source": [
    "BASE_DATASET_DIR = Path(\"../dataset/v5-250228-multimetadata\")\n",
    "print(f\"BASE_DATASET_DIR: {BASE_DATASET_DIR}\")\n",
    "print(list(BASE_DATASET_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 0     # Unsloth auto supports RoPE Scaling internally!\n",
    "dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "device = f\"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \n",
      "ì‚¬ìš©ìì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„\n",
      "Formalized Inputìœ¼ë¡œ êµ¬ì²´í™” ë° ì •ê·œí™” í›„,\n",
      "Input Semantic Parsingê³¼ì •ì—ì„œ Metadataë¥¼ ì°¸ì¡°í•´ í†µí•´ Formalized inputì˜ Temporal, Spatial, Modalityì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ ëª…ì‹œì ìœ¼ë¡œ ì¹˜í™˜í•˜ê³ ,\n",
      "Strategyì— ì§ˆë¬¸ì˜ Objective, ìœ ì €ê°€ ë“£ê³  ì‹¶ì–´í•˜ëŠ” Expected Outputì„ ìœ ì¶”, Objectiveì™€ Expected Outputì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë¬¸ì œ í•´ê²° Stepì„ ì‘ì„±í•˜ê³ , Stepì— ë§ì¶”ì–´\n",
      "Agentì˜ Instruction setì„ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\n",
      "\n",
      "AgentëŠ” ì•„ë˜ DDL statementë¡œ êµ¬ì„±ëœ databaseì— ì ‘ê·¼í•˜ì—¬ ì¿¼ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, \n",
      "ë„ˆëŠ” \"type\"=\"q\"ì˜ instructionìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤.\n",
      "\n",
      "<DDL statement>\n",
      "CREATE TABLE IF NOT EXISTS data_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
      "    idu_id integer,\n",
      "    roomtemp double precision,\n",
      "    settemp double precision,\n",
      "    oper boolean,\n",
      "    \"timestamp\" timestamp without time zone NOT NULL\n",
      ")\n",
      "    \n",
      "CREATE TABLE IF NOT EXISTS idu_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
      "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
      "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
      "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "ì¶œë ¥ í˜•ì‹ì€ jsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«í˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.\n",
      "\n",
      "\n",
      "ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´.\n",
      "Input: \"ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜\"\n",
      "{\n",
      "\"Formalized Input\": \"ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜.\",\n",
      "\"Input Semantic Parsing\": {\n",
      "\"Temporal\": {\n",
      "\"ì˜¬í•´ ì—¬ë¦„\": \"2022-06-01 00:00:00 ~ 2022-08-31 23:59:59\"\n",
      "},\n",
      "\"Spatial\": {\n",
      "\"ìš°ë¦¬ë°˜\": \"01_IB5\"\n",
      "},\n",
      "\"Modality\": {\n",
      "\"ì‹¤ë‚´ì˜¨ë„\": \"roomtemp\"\n",
      "}\n",
      "},\n",
      "\"Strategy\": {\n",
      "\"Objective\": \"ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„° ì¤‘ ì˜¬í•´ ì—¬ë¦„ ë°ì´í„°ë§Œ ì¿¼ë¦¬ í›„ í‰ê· ê°’ ê³„ì‚°.\",\n",
      "\"Expected Outputs\": [\n",
      "\"ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” 26â„ƒ ì…ë‹ˆë‹¤.\"\n",
      "],\n",
      "\"Step\": [\n",
      "\"ì˜¬í•´ ì—¬ë¦„(2022-06-01 ~ 2022-08-31) ìš°ë¦¬ë°˜ì˜ roomtemp ì¿¼ë¦¬ í›„ qrì— ì €ì¥\",\n",
      "\"qrì—ì„œ roomtempì˜ í‰ê· ê°’ ë°˜í™˜\"\n",
      "]\n",
      "},\n",
      "\"Instruction Set\": [\n",
      "{\n",
      "\"type\": \"q\",\n",
      "\"args\": {\n",
      "\"table_name\": \"data_t\",\n",
      "\"columns\": [\n",
      "\"roomtemp\"\n",
      "],\n",
      "\"conditions\": [\n",
      "\"timestamp BETWEEN '2022-06-01 00:00:00' AND '2022-08-31 23:59:59'\"\n",
      "],\n",
      "\"subquery\": \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"\n",
      "},\n",
      "\"result_name\": \"qr\"\n",
      "},\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# current_metadata = json.load(open(\"metadata.json\", \"r\"))\n",
    "\n",
    "\n",
    "common_prompt = open(BASE_DATASET_DIR / f\"prompt.txt\", \"r\").read()\n",
    "\n",
    "if train_type in [\"woall\", \"FI\", \"ISP\"]:\n",
    "    # search <|ST|>~~<|ST|> and remove between them\n",
    "    common_prompt = re.sub(r\"\\n?<\\|ST\\|>(.|\\n)*?<\\|ST\\|>\", \"\", common_prompt)\n",
    "if train_type in [\"woall\", \"FI\"]:\n",
    "    # search <|ISP|>~~<|ISP|> and remove between them\n",
    "    common_prompt = re.sub(r\"\\n?<\\|ISP\\|>(.|\\n)*?<\\|ISP\\|>\", \"\", common_prompt)\n",
    "if train_type in [\"woall\"]:\n",
    "    # search <|FI|>~~<|FI|> and remove between them\n",
    "    common_prompt = re.sub(r\"\\n?<\\|FI\\|>(.|\\n)*?<\\|FI\\|>\", \"\", common_prompt)\n",
    "\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "print(common_prompt)\n",
    "\n",
    "# print(common_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'sh2orc/Llama-3.1-Korean-8B-Instruct'\n",
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = 'Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B'\n",
    "\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.689 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813d28826df441f2b840cfa164ea3947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer initialization\n",
    "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,  \n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    # load_in_4bit = False if not \"27B\" in model_id else True,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True,\n",
    "    #     # bnb_4bit_use_double_quant=True,\n",
    "    #     # bnb_4bit_quant_type=\"nf4\",\n",
    "    #     # bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     load_in_8bit=False if not \"27B\" in model_id else True,\n",
    "    #     llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    # ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # local_files_only=True\n",
    ")\n",
    "\n",
    "# if not os.path.exists(model_dir):\n",
    "# pretrained_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Token id: 128004 and Pad Token: <|finetune_right_pad_id|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../dataset/v5-250228-multimetadata/scenario3'), PosixPath('../dataset/v5-250228-multimetadata/scenario1'), PosixPath('../dataset/v5-250228-multimetadata/scenario2')]\n"
     ]
    }
   ],
   "source": [
    "scenario_dirs = [d for d in BASE_DATASET_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]\n",
    "print(scenario_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c60c846c02b42aaac4234e7cf40d574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b276d17e456e46809f16d90eadeb8cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "ë„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \n",
      "ì‚¬ìš©ìì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„\n",
      "Formalized Inputìœ¼ë¡œ êµ¬ì²´í™” ë° ì •ê·œí™” í›„,\n",
      "Input Semantic Parsingê³¼ì •ì—ì„œ Metadataë¥¼ ì°¸ì¡°í•´ í†µí•´ Formalized inputì˜ Temporal, Spatial, Modalityì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì„ ëª…ì‹œì ìœ¼ë¡œ ì¹˜í™˜í•˜ê³ ,\n",
      "Strategyì— ì§ˆë¬¸ì˜ Objective, ìœ ì €ê°€ ë“£ê³  ì‹¶ì–´í•˜ëŠ” Expected Outputì„ ìœ ì¶”, Objectiveì™€ Expected Outputì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë¬¸ì œ í•´ê²° Stepì„ ì‘ì„±í•˜ê³ , Stepì— ë§ì¶”ì–´\n",
      "Agentì˜ Instruction setì„ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\n",
      "\n",
      "AgentëŠ” ì•„ë˜ DDL statementë¡œ êµ¬ì„±ëœ databaseì— ì ‘ê·¼í•˜ì—¬ ì¿¼ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, \n",
      "ë„ˆëŠ” \"type\"=\"q\"ì˜ instructionìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤.\n",
      "\n",
      "<DDL statement>\n",
      "CREATE TABLE IF NOT EXISTS data_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
      "    idu_id integer,\n",
      "    roomtemp double precision,\n",
      "    settemp double precision,\n",
      "    oper boolean,\n",
      "    \"timestamp\" timestamp without time zone NOT NULL\n",
      ")\n",
      "    \n",
      "CREATE TABLE IF NOT EXISTS idu_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
      "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
      "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
      "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "ì¶œë ¥ í˜•ì‹ì€ jsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«í˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.\n",
      "\n",
      "\n",
      "ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´.\n",
      "Input: \"ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜\"\n",
      "{\n",
      "\"Formalized Input\": \"ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜.\",\n",
      "\"Input Semantic Parsing\": {\n",
      "\"Temporal\": {\n",
      "\"ì˜¬í•´ ì—¬ë¦„\": \"2022-06-01 00:00:00 ~ 2022-08-31 23:59:59\"\n",
      "},\n",
      "\"Spatial\": {\n",
      "\"ìš°ë¦¬ë°˜\": \"01_IB5\"\n",
      "},\n",
      "\"Modality\": {\n",
      "\"ì‹¤ë‚´ì˜¨ë„\": \"roomtemp\"\n",
      "}\n",
      "},\n",
      "\"Strategy\": {\n",
      "\"Objective\": \"ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ì˜¨ë„ ë°ì´í„° ì¤‘ ì˜¬í•´ ì—¬ë¦„ ë°ì´í„°ë§Œ ì¿¼ë¦¬ í›„ í‰ê· ê°’ ê³„ì‚°.\",\n",
      "\"Expected Outputs\": [\n",
      "\"ì˜¬í•´ ì—¬ë¦„ ìš°ë¦¬ë°˜ì˜ í‰ê·  ì˜¨ë„ëŠ” 26â„ƒ ì…ë‹ˆë‹¤.\"\n",
      "],\n",
      "\"Step\": [\n",
      "\"ì˜¬í•´ ì—¬ë¦„(2022-06-01 ~ 2022-08-31) ìš°ë¦¬ë°˜ì˜ roomtemp ì¿¼ë¦¬ í›„ qrì— ì €ì¥\",\n",
      "\"qrì—ì„œ roomtempì˜ í‰ê· ê°’ ë°˜í™˜\"\n",
      "]\n",
      "},\n",
      "\"Instruction Set\": [\n",
      "{\n",
      "\"type\": \"q\",\n",
      "\"args\": {\n",
      "\"table_name\": \"data_t\",\n",
      "\"columns\": [\n",
      "\"roomtemp\"\n",
      "],\n",
      "\"conditions\": [\n",
      "\"timestamp BETWEEN '2022-06-01 00:00:00' AND '2022-08-31 23:59:59'\"\n",
      "],\n",
      "\"subquery\": \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"\n",
      "},\n",
      "\"result_name\": \"qr\"\n",
      "},\n",
      "]\n",
      "}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Metadata:{'current_datetime': '2021-08-05 14:30:00', 'idu_mapping': {'01_IB5': ['ìš°ë¦¬ë°˜'], '01_IB7': ['ì˜†ë°˜'], '02_I81': ['ì•ë°˜'], '02_I84': None, '02_I85': None}, 'idu_name': '01_IB5', 'modality_mapping': {'oper': ['ì „ì›'], 'roomtemp': ['ì‹¤ë‚´ì˜¨ë„'], 'settemp': ['ì„¤ì •ì˜¨ë„']}, 'site_name': 'YongDongIllHighSchool', 'user_name': 'í™ê¸¸ë™', 'user_role': 'customer'};Input:ì–´ì œ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜;<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"Formalized Input\": \"ì–´ì œ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ ì¼í‰ê·  ì„¤ì •ì˜¨ë„ ì°¨ì´ ì•Œë ¤ì¤˜.\", \"Input Semantic Parsing\": {\"Temporal\": {\"ì–´ì œ\": \"2021-08-04 00:00:00 ~ 2021-08-04 23:59:59\"}, \"Spatial\": {\"ìš°ë¦¬ë°˜\": \"01_IB5\", \"ì˜†ë°˜\": \"01_IB7\"}, \"Modality\": {\"ì„¤ì •ì˜¨ë„\": \"settemp\"}}, \"Strategy\": {\"Objective\": \"ì–´ì œ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ ê°ê°ì˜ ì„¤ì •ì˜¨ë„ ì¿¼ë¦¬ í›„ ê° ë°˜ì˜ ì¼í‰ê·  settempì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨.\", \"Expected Outputs\": [\"ìš°ë¦¬ë°˜ì˜ ì„¤ì •ì˜¨ë„(19.3â„ƒ)ê°€ ì˜†ë°˜ì˜ ì„¤ì •ì˜¨ë„(17.0â„ƒ)ë³´ë‹¤ 2.3â„ƒ ë†’ìŠµë‹ˆë‹¤.\"], \"Step\": [\"ìš°ë¦¬ë°˜ì˜ ì–´ì œ(2021-08-04) settemp ì¿¼ë¦¬ ê²°ê³¼ qr_oursì— ì €ì¥\", \"ì˜†ë°˜ì˜ ì–´ì œ(2021-08-04) settemp ì¿¼ë¦¬ ê²°ê³¼ qr_besideì— ì €ì¥\", \"ê° ë°˜ì˜ ì¼í‰ê·  settempì™€ ì°¨ì´ ë°˜í™˜\"]}, \"Instruction Set\": [{\"type\": \"q\", \"args\": {\"table_name\": \"data_t\", \"columns\": [\"settemp\"], \"conditions\": [\"timestamp BETWEEN '2021-08-04 00:00:00' AND '2021-08-04 23:59:59'\"], \"subquery\": \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5'))\"}, \"result_name\": \"qr_ours\"}, {\"type\": \"q\", \"args\": {\"table_name\": \"data_t\", \"columns\": [\"settemp\"], \"conditions\": [\"timestamp BETWEEN '2021-08-04 00:00:00' AND '2021-08-04 23:59:59'\"], \"subquery\": \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7'))\"}, \"result_name\": \"qr_beside\"}]}<|eot_id|>\n",
      "3199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_dataset(dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if train_type in [\"woall\", \"FI\", \"ISP\"]:\n",
    "            del d[\"Response\"][\"Strategy\"]\n",
    "        \n",
    "        if train_type in [\"woall\", \"FI\"]:\n",
    "            del d[\"Response\"][\"Input Semantic Parsing\"]\n",
    "        \n",
    "        if train_type in [\"woall\"]:\n",
    "            del d[\"Response\"][\"Formalized Input\"]\n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "dataset_trs = []\n",
    "dataset_tss = []\n",
    "for scenario_dir in scenario_dirs:\n",
    "    dataset_trs.extend(read_dataset(scenario_dir, \"onlyq_tr.json\"))\n",
    "    dataset_tss.extend(read_dataset(scenario_dir, \"onlyq_ts.json\"))\n",
    "\n",
    "dataset_tr = Dataset.from_list(dataset_trs)\n",
    "dataset_ts = Dataset.from_list(dataset_tss)\n",
    "\n",
    "max_seq_length = 0\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for metadata, input, response in zip(examples['Metadata'], examples['Input'], examples['Response']):\n",
    "        global max_seq_length\n",
    "        response.replace(\"    \", \"\")\n",
    "\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        if \"llama\" in model_id.lower():\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            user_input = {\n",
    "                \"content\": f\"Metadata:{metadata};Input:{input};\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "        elif \"gemma\" in model_id.lower():\n",
    "            user_input = {\n",
    "                \"content\": f\"{common_prompt};{metadata};{input}\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([user_input, answer])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(\"Answer length: \", len(response))\n",
    "        # convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        if len(response) + 50 > max_seq_length:\n",
    "            max_seq_length = len(response) + len(metadata) + len(input) + 50\n",
    "            # print(response)\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    # remove \\n\\nCutting Knowledge Date: BLAH BLAH \\nToday Date: BLAH BLAH\\n\\n using regex\n",
    "    texts = [re.sub(r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', '', text) for text in texts]\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset_tr = dataset_tr.map(formatting_prompts_func, batched=True)\n",
    "dataset_ts = dataset_ts.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(dataset_tr[0][\"text\"])\n",
    "max_seq_length += len(common_prompt)\n",
    "print(max_seq_length)\n",
    "# print(f\"seq length: {len(tokenizer.encode(dataset_tr[0]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5_r64_a128_ours\n"
     ]
    }
   ],
   "source": [
    "lora_r = 64\n",
    "lora_alpha = 128\n",
    "lora_repr = f\"v5_r{lora_r}_a{lora_alpha}_{train_type}\"\n",
    "print(lora_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.1.8 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    pretrained_model,\n",
    "    r=lora_r,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    # \"embed_tokens\", \n",
    "                    # \"lm_head\"\n",
    "                    ],\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None   # No LoftQ, for standard fine-tuning\n",
    ")\n",
    "# del pretrained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(len(dataset_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Accumulation Steps: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7634ce886f214fe1a0306511441683b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8364c73441433689eab70c8ae1dc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/35 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "per_device_train_batch_size, epochs = 37, 70 # 8\n",
    "gradient_accumulation_steps = int(np.ceil(len(dataset_tr) / per_device_train_batch_size))\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "# clear all checkpoints\n",
    "import shutil\n",
    "shutil.rmtree(f\"{model_dir}/chkpts/{lora_repr}\", ignore_errors=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # num_train_epochs = 1,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,  # Controls the batch size per device\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # Accumulates gradients to simulate a larger batch\n",
    "    max_steps=gradient_accumulation_steps * epochs,\n",
    "    # ë¦¬ì†ŒìŠ¤ ì œì•½ë•Œë¬¸ì— batch sizeë¥¼ íƒ€í˜‘í•´ì•¼í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒ -> micro batch sizeë¥¼ ì¤„ì´ê³ ,\n",
    " \t# accumulated stepì„ ëŠ˜ë ¤, ì ì ˆí•œ sizeë¡œ gradientë¥¼ êµ¬í•´ weight update\n",
    "    # https://www.youtube.com/watch?v=ptlmj9Y9iwE\n",
    "    warmup_steps = gradient_accumulation_steps,\n",
    "    learning_rate = 1e-4,             # Sets the learning rate for optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "    lr_scheduler_type = \"cosine\",  # Sets the learning rate scheduler\n",
    "    seed = 3407,                        \n",
    "    output_dir = f\"{model_dir}/chkpts/{lora_repr}\",  # Output directory for checkpoints and predictions     \n",
    "    report_to = \"none\",              # Enables Weights & Biases (W&B) logging\n",
    "    logging_steps = gradient_accumulation_steps,                # Sets frequency of logging to W&B\n",
    "    logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "    evaluation_strategy=\"steps\",  # enable evaluation during training\n",
    "    eval_steps=gradient_accumulation_steps,\n",
    "    # eval_accumulation_steps=1, # ë‚®ì„ìˆ˜ë¡ evalì‹œ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ì¤„ì–´ë“¦\n",
    "    save_steps=gradient_accumulation_steps,\n",
    "    save_strategy = \"steps\",               \n",
    "    load_best_model_at_end = True,    # Loads the best model at the end\n",
    "    save_only_model = False           # Saves entire model, not only weights\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_tr,\n",
    "    eval_dataset = dataset_ts,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,        # Can make training 5x faster for short sequences.\n",
    "    args = args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 143 | Num Epochs = 280\n",
      "O^O/ \\_/ \\    Batch size per device = 37 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 148 | Total steps = 280\n",
      " \"-____-\"     Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 13/280 30:49 < 12:28:11, 0.01 it/s, Epoch 12/280]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.503500</td>\n",
       "      <td>1.044289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.718400</td>\n",
       "      <td>0.280769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.127242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "print(trainer_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del base_model\n",
    "# del peft_model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Specify the checkpoint directory\n",
    "checkpoint_dir = f\"{model_dir}/chkpts/{lora_repr}/checkpoint-{18}\"\n",
    "# checkpoint_dir = \"/model/Bllossom-llama-3.2-Korean-Bllossom-3B/chkpts/r1700_a1500/checkpoint-12\"\n",
    "print(checkpoint_dir)\n",
    "# Load the tokenizer (ensure it's the same tokenizer used for training)\n",
    "\n",
    "# Load the base model\n",
    "peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    dtype = dtype,\n",
    "    # max_seq_length = max_seq_length,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True,\n",
    "    #     # bnb_4bit_use_double_quant=True,\n",
    "    #     # bnb_4bit_quant_type=\"nf4\",\n",
    "    #     # bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     load_in_8bit=True,\n",
    "    #     llm_int8_enable_fp32_cpu_offload=True\n",
    "    # ),\n",
    "    device_map=device,\n",
    "    attn_implementation=attn_implementation,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "print(f\"Padding side: {tokenizer.padding_side}\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# \n",
    "\n",
    "\n",
    "# Make sure the tokenizer is ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Local saving\n",
    "    peft_model.save_pretrained(\"lora_i2i\") \n",
    "    tokenizer.save_pretrained(\"lora_i2i\")\n",
    "\n",
    "    # For merging the LoRA adapters with the base model and save the model to 16-bit precision for optimized performance with vLLM, use:\n",
    "    # # Merge to 16bit\n",
    "    peft_model.save_pretrained_merged(\"i2i_merged_16bit\", tokenizer, save_method = \"merged_16bit\",)\n",
    "    peft_model.save_pretrained_merged(\"i2i_merged_4bit\", tokenizer, save_method = \"merged_4bit_forced\",)\n",
    "    # model.push_to_hub_merged(\"<hf_username/model_name>\", tokenizer, save_method = \"merged_16bit\", token = hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "if False:\n",
    "    # del peft_model\n",
    "    torch.cuda.empty_cache()\n",
    "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"i2i_merged_16bit\",        # Trained model either locally or from huggingface\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = False,\n",
    "        attn_implementation=attn_implementation,\n",
    "        # device_map=[device],\n",
    "        local_files_only=True\n",
    "    )\n",
    "FastLanguageModel.for_inference(peft_model)  # Enable native 2x faster inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Instruction(['q', 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ê°€ ìµœê³ ì¸ ë‚ ì§œ ì•Œë ¤ì¤˜.', 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ê°€ ìµœê³ ì¸ ë‚ ì§œ', 'ê°’', Semantic(Temporal=[('ì´ë²ˆë‹¬', '2022-09-01 00:00:00 ~ 2022-09-30 23:59:59')], Spatial=['ìš°ë¦¬ë°˜'], Modality=['ì‹¤ë‚´ì˜¨ë„'], Operation=['ìµœê³ ì¸'], Target=['ë‚ ì§œ'])]), Instruction(['r', \"ì˜ˆ) 'ì´ë²ˆë‹¬ ìš°ë¦¬ë°˜ì˜ ì‹¤ë‚´ ì˜¨ë„ê°€ ê°€ì¥ ë”ì› ë˜ ë‚ ì€ 2022ë…„ 9ì›” 15ì¼ì…ë‹ˆë‹¤.'\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "\n",
    "def extract_content(text):\n",
    "    # Define the regex pattern to extract the content\n",
    "    print(text)\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def run(query):\n",
    "    start_time = time.time()\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": common_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Metadata:{current_metadata};Input:{query};\"},\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = peft_model.generate(\n",
    "        input_ids = inputs,\n",
    "        max_new_tokens = max_seq_length,\n",
    "        use_cache = True,\n",
    "        pad_token_id = tokenizer.eos_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    # print(response)\n",
    "    # print(\"Extracting..\")\n",
    "    parsed_response = extract_content(response)\n",
    "    pprint.pprint(f\"Query: {query}, Time: {time.time() - start_time}\")\n",
    "    # print(parsed_response)\n",
    "    \n",
    "    parsed_response_dict = eval(parsed_response)\n",
    "    \n",
    "    return parsed_response, parsed_response_dict\n",
    "    \n",
    "    # text_streamer = transformers.TextStreamer(\n",
    "    #     tokenizer, \n",
    "    # skip_prompt = True\n",
    "    # )\n",
    "    # _ = peft_model.generate(\n",
    "    #     input_ids = inputs, \n",
    "    #     streamer = text_streamer, \n",
    "    #     max_new_tokens = max_seq_length, \n",
    "    #     use_cache = True,\n",
    "    #     pad_token_id = tokenizer.eos_token_id,\n",
    "    #     eos_token_id = tokenizer.eos_token_id,\n",
    "    # )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run(\"ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ê³¼ ì˜†ë°˜ì˜ í‰ê·  ì˜¨ë„ì°¨ì´ ì•Œë ¤ì¤˜\")\n",
    "# parsed_response_dict = eval(result)\n",
    "\n",
    "# print(parsed_response_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "run(\"ì˜¤ëŠ˜ ìš°ë¦¬ë°˜ í‰ê· ì˜¨ë„ ì•Œë ¤ì¤˜\")\n",
    "\n",
    "run(\"ì–´ì œ ì˜†ë°˜ ì˜¨ë„ í‰ê·  ì•Œë ¤ì¤˜\")\n",
    "run(\"í˜„ì¬ ìš°ë¦¬ë°˜ ì‹¤ë‚´ì˜¨ë„ ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "run(\"ë¡¯ë°ìºìŠ¬ì˜ í˜„ì¬ ì˜¨ë„ ì•Œë ¤ì¤˜\")\n",
    "run(\"10ë…„ì „ ìš°ë¦¬ë°˜ ì˜¨ë„ ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "run(\"ì§€ê¸ˆ ì˜†ë°˜ ì—ì–´ì»¨ ìƒíƒœ ì•Œë ¤ì¤˜\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
