{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "from unsloth import FastLanguageModel, unsloth_train\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_type: ours\n"
     ]
    }
   ],
   "source": [
    "train_type = [\n",
    "    \"woall\", # 0\n",
    "    \"ours\" # 1\n",
    "][1]\n",
    "print(f\"train_type: {train_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DATASET_DIR: ../dataset/v7-250309-reduceinputanddatefunctioncall\n",
      "[PosixPath('../dataset/v7-250309-reduceinputanddatefunctioncall/scenario1'), PosixPath('../dataset/v7-250309-reduceinputanddatefunctioncall/process.ipynb'), PosixPath('../dataset/v7-250309-reduceinputanddatefunctioncall/prompt.txt')]\n"
     ]
    }
   ],
   "source": [
    "# BASE_DATASET_DIR = Path(\"../dataset/v5-250228-multimetadata\")\n",
    "# dataset_name = \"v6-250306-optimizetoken\"\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "BASE_DATASET_DIR = Path(f\"../dataset/{dataset_name}\")\n",
    "print(f\"BASE_DATASET_DIR: {BASE_DATASET_DIR}\")\n",
    "print(list(BASE_DATASET_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 0     # Unsloth auto supports RoPE Scaling internally!\n",
    "dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "device = f\"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„ˆëŠ” ìœ ì €ì˜ HVAC ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Agentì˜ ê³„íšì„ ì„¤ê³„í•˜ëŠ” ì •í™•í•˜ê³  í›Œë£¡í•œ ì¸ê³µì§€ëŠ¥ì´ë‹¤. \n",
      "ì‚¬ìš©ìì˜ ì§ˆë¬¸(Input)ì„ ë°›ì•„ Agentì˜ Instructionsë¥¼ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤.\n",
      "Thinkingì—ì„œëŠ” Metadataë¥¼ ë°”íƒ•ìœ¼ë¡œ Inputì„ specificí•œ termìœ¼ë¡œ ìˆ˜ì •í•˜ê³ , ìœ ì €ì˜ ì˜ë„ë¥¼ ì¶”ì¸¡í•˜ë©°, ë‹µë³€ ê³„íšì„ ì„¸ì›Œì•¼í•¨.\n",
      "Expectationì—ì„œëŠ” ìœ ì €ê°€ ê¸°ëŒ€í• ë§Œí•œ ë‹µë³€ì„ ì¶”ì¸¡í•´ì•¼í•¨.\n",
      "\n",
      "AgentëŠ” ì•„ë˜ DDL statementë¡œ êµ¬ì„±ëœ databaseì— ì ‘ê·¼í•˜ì—¬ ì¿¼ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, \n",
      "ë„ˆëŠ” \"type\"=\"q\"ì˜ instructionìœ¼ë¡œ ì¿¼ë¦¬ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤.\n",
      "\n",
      "<DDL statement>\n",
      "CREATE TABLE IF NOT EXISTS data_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
      "    idu_id integer,\n",
      "    roomtemp double precision,\n",
      "    settemp double precision,\n",
      "    oper boolean,\n",
      "    \"timestamp\" timestamp without time zone NOT NULL\n",
      ")\n",
      "    \n",
      "CREATE TABLE IF NOT EXISTS idu_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
      "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
      "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
      "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "ì¶œë ¥ í˜•ì‹ì€ jsoní˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë©°, eval() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´„í˜¸ë“¤ê³¼ ë”°ì˜´í‘œë“¤ì˜ ìˆœì„œì™€ ë‹«í˜ì„ ë§¤ìš° ì‹ ê²½ì¨ì„œ ì¶œë ¥í•´ì•¼í•œë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# current_metadata = json.load(open(\"metadata.json\", \"r\"))\n",
    "\n",
    "\n",
    "common_prompt = open(BASE_DATASET_DIR / f\"prompt.txt\", \"r\").read()\n",
    "\n",
    "if train_type in [\"woall\"]:\n",
    "    # search <|FI|>~~<|FI|> and remove between them\n",
    "    common_prompt = re.sub(r\"\\n?<\\|Ours\\|>(.|\\n)*?<\\|Ours\\|>\", \"\", common_prompt)\n",
    "\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "print(common_prompt)\n",
    "\n",
    "# print(common_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'sh2orc/Llama-3.1-Korean-8B-Instruct'\n",
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = 'Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B'\n",
    "\n",
    "model_dir = f\"/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.8: Fast Llama patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.689 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce86d830db0f4168ba606794f45839b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh2orc/Llama-3.1-Korean-8B-Instruct does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer initialization\n",
    "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,  \n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    # load_in_4bit = False if not \"27B\" in model_id else True,\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #     # load_in_4bit=True,\n",
    "    #     # bnb_4bit_use_double_quant=True,\n",
    "    #     # bnb_4bit_quant_type=\"nf4\",\n",
    "    #     # bnb_4bit_compute_dtype=torch_dtype\n",
    "    #     load_in_8bit=False if not \"27B\" in model_id else True,\n",
    "    #     llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    # ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # local_files_only=True\n",
    ")\n",
    "\n",
    "# if not os.path.exists(model_dir):\n",
    "# pretrained_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Token id: 128004 and Pad Token: <|finetune_right_pad_id|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('../dataset/v7-250309-reduceinputanddatefunctioncall/scenario1')]\n"
     ]
    }
   ],
   "source": [
    "scenario_dirs = [d for d in BASE_DATASET_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]\n",
    "print(scenario_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2d7f3d2af346d6aabde0c8449c3c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d8e9c89ede440ea23be3e4d5f01b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_dataset(dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if \"v6\" in dataset_name:\n",
    "            if train_type in [\"woall\"]:\n",
    "                del d[\"Response\"][\"ìƒê°\"]\n",
    "        \n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "dataset_trs = []\n",
    "dataset_tss = []\n",
    "for scenario_dir in scenario_dirs:\n",
    "    dataset_trs.extend(read_dataset(scenario_dir, \"onlyq_tr.json\"))\n",
    "    dataset_tss.extend(read_dataset(scenario_dir, \"onlyq_ts.json\"))\n",
    "\n",
    "dataset_tr = Dataset.from_list(dataset_trs) # ì„œë¡œ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„° í•©ì¹˜ë©´ì„œ\n",
    "dataset_ts = Dataset.from_list(dataset_tss) # Mutually exclusiveí•œ ì• ë“¤ì€ None ë¨\n",
    "\n",
    "max_seq_length = 0\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for metadata, input, response in zip(examples['Metadata'], examples['Input'], examples['Response']):\n",
    "        # global max_seq_length\n",
    "        response.replace(\"    \", \"\")\n",
    "\n",
    "        # print(metadata['current_datetime'])\n",
    "        # print(metadata['idu_mapping'])\n",
    "\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        if \"llama\" in model_id.lower():\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            user_input = {\n",
    "                \"content\": f\"Metadata:{metadata};Input:{input};\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "        elif \"gemma\" in model_id.lower():\n",
    "            user_input = {\n",
    "                \"content\": f\"{common_prompt};{metadata};{input}\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([user_input, answer])\n",
    "        \n",
    "        \n",
    "        # print(\"Answer length: \", len(response))\n",
    "        # convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        # if len(response) + 50 > max_seq_length:\n",
    "        #     max_seq_length = len(response) + len(metadata) + len(input) + 50\n",
    "            # print(response)\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    \n",
    "    # remove \\n\\nCutting Knowledge Date: BLAH BLAH \\nToday Date: BLAH BLAH\\n\\n using regex\n",
    "    texts = [re.sub(r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', '', text) for text in texts]\n",
    "\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset_tr = dataset_tr.map(formatting_prompts_func, batched=True)\n",
    "dataset_ts = dataset_ts.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "max_seq_length = max([len(tokenizer.encode(dataset_tr[i]['text'])) for i in range(len(dataset_tr))]) + 10\n",
    "# max_seq_length += len(common_prompt)\n",
    "print(max_seq_length)\n",
    "# print(f\"seq length: {len(tokenizer.encode(dataset_tr[0]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v7_r256_a512_ours\n"
     ]
    }
   ],
   "source": [
    "lora_r = 256\n",
    "lora_alpha = 512\n",
    "lora_repr = f\"v7_r{lora_r}_a{lora_alpha}_{train_type}\"\n",
    "print(lora_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.1.8 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    pretrained_model,\n",
    "    r=lora_r,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    # \"embed_tokens\", \n",
    "                    # \"lm_head\"\n",
    "                    ],\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None,   # No LoftQ, for standard fine-tuning\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "del pretrained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(len(dataset_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Accumulation Steps: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "per_device_train_batch_size, epochs = 10, 50 # 8\n",
    "gradient_accumulation_steps = int(np.ceil(len(dataset_tr) / per_device_train_batch_size))\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "# clear all checkpoints\n",
    "import shutil\n",
    "shutil.rmtree(f\"{model_dir}/chkpts/{lora_repr}\", ignore_errors=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # num_train_epochs = 1,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,  # Controls the batch size per device\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # Accumulates gradients to simulate a larger batch\n",
    "    max_steps=gradient_accumulation_steps * epochs,\n",
    "    # ë¦¬ì†ŒìŠ¤ ì œì•½ë•Œë¬¸ì— batch sizeë¥¼ íƒ€í˜‘í•´ì•¼í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒ -> micro batch sizeë¥¼ ì¤„ì´ê³ ,\n",
    " \t# accumulated stepì„ ëŠ˜ë ¤, ì ì ˆí•œ sizeë¡œ gradientë¥¼ êµ¬í•´ weight update\n",
    "    # https://www.youtube.com/watch?v=ptlmj9Y9iwE\n",
    "    warmup_steps = gradient_accumulation_steps,\n",
    "    learning_rate = 1e-4,             # Sets the learning rate for optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "    lr_scheduler_type = \"cosine\",  # Sets the learning rate scheduler\n",
    "    seed = 3407,                        \n",
    "    output_dir = f\"{model_dir}/chkpts/{lora_repr}\",  # Output directory for checkpoints and predictions     \n",
    "    report_to = \"none\",              # Enables Weights & Biases (W&B) logging\n",
    "    logging_steps = gradient_accumulation_steps,                # Sets frequency of logging to W&B\n",
    "    logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "    evaluation_strategy=\"steps\",  # enable evaluation during training\n",
    "    eval_steps=gradient_accumulation_steps,\n",
    "    # eval_accumulation_steps=1, # ë‚®ì„ìˆ˜ë¡ evalì‹œ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ì¤„ì–´ë“¦\n",
    "    save_steps=gradient_accumulation_steps,\n",
    "    save_strategy = \"steps\",               \n",
    "    load_best_model_at_end = True,    # Loads the best model at the end\n",
    "    save_only_model = False           # Saves entire model, not only weights\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_tr,\n",
    "    eval_dataset = dataset_ts,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,        # Can make training 5x faster for short sequences.\n",
    "    args = args,\n",
    "    # compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 13 | Num Epochs = 140\n",
      "O^O/ \\_/ \\    Batch size per device = 10 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 20 | Total steps = 140\n",
      " \"-____-\"     Number of trainable parameters = 671,088,640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/140 00:08 < 19:53, 0.11 it/s, Epoch 2/140]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.298900</td>\n",
       "      <td>1.503096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43munsloth_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer_stats)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/unsloth/trainer.py:45\u001b[0m, in \u001b[0;36munsloth_train\u001b[0;34m(trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munsloth_train\u001b[39m(trainer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:382\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:68\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "print(trainer_stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
