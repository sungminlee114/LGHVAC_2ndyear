{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "from unsloth import FastLanguageModel, unsloth_train\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 0     # Unsloth auto supports RoPE Scaling internally!\n",
    "# dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "device = f\"cuda\"\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "# QLora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'sh2orc/Llama-3.1-Korean-8B-Instruct'\n",
    "# model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "# model_id = 'Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B'\n",
    "model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "\n",
    "model_dir = f\"/workspace/LGHVAC_2ndyear/model/{model_id.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'Bllossom/llama-3-Korean-Bllossom-70B'\n",
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download(repo_id=model_id, local_dir=\"70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizer initialization\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     cache_dir=f\"{model_dir}/cache\",\n",
    "#     # attn_implementation=attn_implementation,\n",
    "#     local_files_only=True,\n",
    "#     device_map=\"cuda\"\n",
    "# )\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\n",
    "# #     model_id,\n",
    "# #     cache_dir=f\"{model_dir}/cache\",\n",
    "# #     local_files_only=True\n",
    "# # )\n",
    "# # if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "# pretrained_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastLanguageModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tokenizer initialization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pretrained_model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model_id,  \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# max_seq_length = max_seq_length,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch_dtype,\n\u001b[1;32m      6\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mBitsAndBytesConfig(\n\u001b[1;32m      9\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# bnb_4bit_use_double_quant=True,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# bnb_4bit_quant_type=\"nf4\",\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# bnb_4bit_compute_dtype=torch_dtype\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     ),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# device_map=device,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cache\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39mattn_implementation,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# local_files_only=True\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# tokenizer.truncation_side = \"left\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenizer initialization\n",
    "pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,  \n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = torch_dtype,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit=False,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=False,\n",
    "        # bnb_4bit_use_double_quant=True,\n",
    "        # bnb_4bit_quant_type=\"nf4\",\n",
    "        # bnb_4bit_compute_dtype=torch_dtype\n",
    "        load_in_8bit=False,\n",
    "        # llm_int8_enable_fp32_cpu_offload=False if not \"27B\" in model_id else True,\n",
    "    ),\n",
    "    # device_map=device,\n",
    "    cache_dir=f\"{model_dir}/cache\",\n",
    "    attn_implementation=attn_implementation,\n",
    "    # local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")\n",
    "\n",
    "# if not os.path.exists(f\"{model_dir}/config.json\"):\n",
    "pretrained_model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = [\n",
    "    \"woall\", # 0\n",
    "    \"ours\" # 1\n",
    "][1]\n",
    "print(f\"train_type: {train_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DATASET_DIR = Path(\"../dataset/v5-250228-multimetadata\")\n",
    "# dataset_name = \"v6-250306-optimizetoken\"\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "BASE_DATASET_DIR = Path(f\"../dataset/{dataset_name}\")\n",
    "print(f\"BASE_DATASET_DIR: {BASE_DATASET_DIR}\")\n",
    "print(list(BASE_DATASET_DIR.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# current_metadata = json.load(open(\"metadata.json\", \"r\"))\n",
    "\n",
    "\n",
    "common_prompt = open(BASE_DATASET_DIR / f\"prompt.txt\", \"r\").read()\n",
    "\n",
    "if train_type in [\"woall\"]:\n",
    "    # search <|FI|>~~<|FI|> and remove between them\n",
    "    common_prompt = re.sub(r\"\\n?<\\|Ours\\|>(.|\\n)*?<\\|Ours\\|>\", \"\", common_prompt)\n",
    "\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)\n",
    "print(common_prompt)\n",
    "\n",
    "# print(common_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_dirs = [d for d in BASE_DATASET_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]\n",
    "print(scenario_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_dataset(dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if \"v6\" in dataset_name:\n",
    "            if train_type in [\"woall\"]:\n",
    "                del d[\"Response\"][\"ÏÉùÍ∞Å\"]\n",
    "        \n",
    "        \n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "dataset_trs = []\n",
    "dataset_tss = []\n",
    "for scenario_dir in scenario_dirs:\n",
    "    dataset_trs.extend(read_dataset(scenario_dir, \"onlyq_tr.json\"))\n",
    "    dataset_tss.extend(read_dataset(scenario_dir, \"onlyq_ts.json\"))\n",
    "\n",
    "dataset_tr = Dataset.from_list(dataset_trs) # ÏÑúÎ°ú Îã§Î•∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ìï©ÏπòÎ©¥ÏÑú\n",
    "dataset_ts = Dataset.from_list(dataset_tss) # Mutually exclusiveÌïú Ïï†Îì§ÏùÄ None Îê®\n",
    "\n",
    "max_seq_length = 0\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = []\n",
    "    # Iterate through each item in the batch (examples are structured as lists of values)\n",
    "    for metadata, input, response in zip(examples['Metadata'], examples['Input'], examples['Response']):\n",
    "        # global max_seq_length\n",
    "        response.replace(\"    \", \"\")\n",
    "\n",
    "        # print(metadata['current_datetime'])\n",
    "        # print(metadata['idu_mapping'])\n",
    "\n",
    "        answer = {\n",
    "            \"content\": f\"{response}\",\n",
    "            \"role\": \"assistant\"\n",
    "        }\n",
    "        if \"llama\" in model_id.lower():\n",
    "            prompt = {\n",
    "                \"content\": common_prompt,\n",
    "                \"role\": \"system\"\n",
    "            }\n",
    "            user_input = {\n",
    "                \"content\": f\"Metadata:{metadata};Input:{input};\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([prompt, user_input, answer])\n",
    "        elif \"gemma\" in model_id.lower():\n",
    "            user_input = {\n",
    "                \"content\": f\"{common_prompt};{metadata};{input}\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "            convos.append([user_input, answer])\n",
    "        \n",
    "        \n",
    "        # print(\"Answer length: \", len(response))\n",
    "        # convos.append([prompt, user_input, answer])\n",
    "        \n",
    "        # if len(response) + 50 > max_seq_length:\n",
    "        #     max_seq_length = len(response) + len(metadata) + len(input) + 50\n",
    "            # print(response)\n",
    "    \n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        for convo in convos]\n",
    "    \n",
    "    # remove \\n\\nCutting Knowledge Date: BLAH BLAH \\nToday Date: BLAH BLAH\\n\\n using regex\n",
    "    texts = [re.sub(r'(\\nCutting Knowledge Date:.*?\\nToday Date:.*?\\n\\n)', '', text) for text in texts]\n",
    "\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset_tr = dataset_tr.map(formatting_prompts_func, batched=True)\n",
    "dataset_ts = dataset_ts.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "max_seq_length = max([len(tokenizer.encode(dataset_tr[i]['text'])) for i in range(len(dataset_tr))]) + 10\n",
    "# max_seq_length += len(common_prompt)\n",
    "print(max_seq_length)\n",
    "# print(f\"seq length: {len(tokenizer.encode(dataset_tr[0]['text']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "lora_repr = f\"v7_r{lora_r}_a{lora_alpha}_{train_type}\"\n",
    "print(lora_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "peft_model = FastLanguageModel.get_peft_model(\n",
    "    pretrained_model,\n",
    "    r=lora_r,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    # \"embed_tokens\", \n",
    "                    # \"lm_head\"\n",
    "                    ],\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=0.05,   # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",      # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Ideal for long context tuning\n",
    "    random_state=3407,\n",
    "    use_rslora=False,   # Disable rank-sensitive LoRA for simpler tasks\n",
    "    loftq_config=None,   # No LoftQ, for standard fine-tuning\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "del pretrained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(len(dataset_tr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "per_device_train_batch_size, epochs = 50, 100 # 8\n",
    "gradient_accumulation_steps = int(np.ceil(len(dataset_tr) / per_device_train_batch_size))\n",
    "print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "# clear all checkpoints\n",
    "import shutil\n",
    "shutil.rmtree(f\"{model_dir}/chkpts/{lora_repr}\", ignore_errors=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    # num_train_epochs = 1,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,  # Controls the batch size per device\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,  # Accumulates gradients to simulate a larger batch\n",
    "    max_steps=gradient_accumulation_steps * epochs,\n",
    "    # Î¶¨ÏÜåÏä§ Ï†úÏïΩÎïåÎ¨∏Ïóê batch sizeÎ•º ÌÉÄÌòëÌï¥ÏïºÌïòÎäî Í≤ΩÏö∞Í∞Ä Î∞úÏÉù -> micro batch sizeÎ•º Ï§ÑÏù¥Í≥†,\n",
    " \t# accumulated stepÏùÑ ÎäòÎ†§, Ï†ÅÏ†àÌïú sizeÎ°ú gradientÎ•º Íµ¨Ìï¥ weight update\n",
    "    # https://www.youtube.com/watch?v=ptlmj9Y9iwE\n",
    "    warmup_steps = gradient_accumulation_steps,\n",
    "    learning_rate = 1e-4,             # Sets the learning rate for optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "    lr_scheduler_type = \"cosine\",  # Sets the learning rate scheduler\n",
    "    seed = 3407,                        \n",
    "    output_dir = f\"{model_dir}/chkpts/{lora_repr}\",  # Output directory for checkpoints and predictions     \n",
    "    report_to = \"none\",              # Enables Weights & Biases (W&B) logging\n",
    "    logging_steps = gradient_accumulation_steps,                # Sets frequency of logging to W&B\n",
    "    logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "    evaluation_strategy=\"steps\",  # enable evaluation during training\n",
    "    eval_steps=gradient_accumulation_steps,\n",
    "    # eval_accumulation_steps=1, # ÎÇÆÏùÑÏàòÎ°ù evalÏãú ÏÇ¨Ïö©ÌïòÎäî Î©îÎ™®Î¶¨ Ï§ÑÏñ¥Îì¶\n",
    "    save_steps=gradient_accumulation_steps,\n",
    "    save_strategy = \"steps\",               \n",
    "    load_best_model_at_end = True,    # Loads the best model at the end\n",
    "    save_only_model = False,           # Saves entire model, not only weights\n",
    "    resume_from_checkpoint = f\"{model_dir}/chkpts/{lora_repr}\",  # Resumes training from a checkpoint\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset_tr,\n",
    "    eval_dataset = dataset_ts,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,        # Can make training 5x faster for short sequences.\n",
    "    args = args,\n",
    "    # compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "print(trainer_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/chkpts/v7_r8_a16_ours_70B/checkpoint-100\n",
      "/workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/chkpts/v7_r8_a16_ours_70B/checkpoint-100\n"
     ]
    }
   ],
   "source": [
    "step = 100\n",
    "\n",
    "lora_repr = \"v7_r8_a16_ours_70B\"\n",
    "checkpoint_dir = f\"{model_dir}/chkpts/{lora_repr}/checkpoint-{step}\"\n",
    "output_path = f\"{model_dir}/gguf/{lora_repr}-checkpoint-{step}.gguf\"\n",
    "lora_output_dir = f\"{model_dir}/lora_output/\"\n",
    "\n",
    "if not os.path.exists(f\"{model_dir}/gguf\"):\n",
    "    os.makedirs(f\"{model_dir}/gguf\")\n",
    "print(checkpoint_dir)\n",
    "\n",
    "if not os.path.exists(lora_output_dir):\n",
    "    os.makedirs(lora_output_dir)\n",
    "print(checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.216 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e32b4721c647b59d9f7af8774a7173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.9 patched 80 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     torch_dtype=torch_dtype,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     # attn_implementation=attn_implementation,\n",
    "    #     local_files_only=True,\n",
    "    #     device_map=\"cuda\"\n",
    "    # )\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #     checkpoint_dir,\n",
    "    #     cache_dir=f\"{model_dir}/cache\",\n",
    "    #     local_files_only=True\n",
    "    # )\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        dtype = torch_dtype,\n",
    "        attn_implementation=attn_implementation,\n",
    "        load_in_4bit = False,\n",
    "        load_in_8bit=True,\n",
    "        cache_dir=f\"{model_dir}/cache\",\n",
    "        local_files_only=True,\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "    # FastLanguageModel.for_inference(model)\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # merge lora model and base pretrained model\n",
    "    # model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
      "This might take 5 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:85: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model.save_pretrained_gguf(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, quantization_method = \"q8_0\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained_merged\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlora_output_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlora_repr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-checkpoint-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstep\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_4bit_forced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tokenizer.save_pretrained(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/save.py:1313\u001b[0m, in \u001b[0;36munsloth_save_pretrained_merged\u001b[0;34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1311\u001b[0m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1313\u001b[0m \u001b[43munsloth_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   1315\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/save.py:298\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Counteract no LoRA adapters!\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerge_and_unload\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 298\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_and_unload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/model.py:892\u001b[0m, in \u001b[0;36mLoraModel.merge_and_unload\u001b[0;34m(self, progressbar, safe_merge, adapter_names)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_and_unload\u001b[39m(\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28mself\u001b[39m, progressbar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, safe_merge: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, adapter_names: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    866\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m    867\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m    This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m    as a standalone model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unload_and_optionally_merge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_merge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_merge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_names\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/model.py:514\u001b[0m, in \u001b[0;36mLoraModel._unload_and_optionally_merge\u001b[0;34m(self, merge, progressbar, safe_merge, adapter_names)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m merge:\n\u001b[0;32m--> 514\u001b[0m         \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafe_merge\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_merge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_module(parent, target_name, target\u001b[38;5;241m.\u001b[39mget_base_layer(), target)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, ModulesToSaveWrapper):\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;66;03m# save any additional trainable modules part of `modules_to_save`\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:120\u001b[0m, in \u001b[0;36mLinear8bitLt.merge\u001b[0;34m(self, safe_merge, adapter_names)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_merge \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(w_data)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs detected in the merged weights. The adapter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactive_adapter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seems to be broken\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mInt8Params(\n\u001b[1;32m    121\u001b[0m     w_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, has_fp16_weights\u001b[38;5;241m=\u001b[39mweight\u001b[38;5;241m.\u001b[39mhas_fp16_weights\n\u001b[1;32m    122\u001b[0m )\u001b[38;5;241m.\u001b[39mto(weight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_bias[active_adapter]:\n\u001b[1;32m    124\u001b[0m     bias_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_base_layer()\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B[active_adapter]\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:659\u001b[0m, in \u001b[0;36mBaseTunerLayer.get_base_layer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# List all merged adapters\u001b[39;00m\n\u001b[1;32m    657\u001b[0m merged_adapters: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_base_layer\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m    660\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    (Recursively) get the base_layer.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    This is necessary for the case that the tuner layer wraps another tuner layer.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    666\u001b[0m     base_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# model.save_pretrained_gguf(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\", tokenizer, quantization_method = \"q8_0\")\n",
    "model.save_pretrained_merged(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}-8bit\", tokenizer, save_method=\"merged_8bit\")\n",
    "# tokenizer.save_pretrained(f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python ../../llama.cpp/convert_hf_to_gguf.py --outfile /workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/gguf/v7_r8_a16_ours_70B-checkpoint-100.gguf --outtype auto --verbose /workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/lora_output//v7_r8_a16_ours_70B-checkpoint-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: v7_r8_a16_ours_70B-checkpoint-100\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:choosing --outtype bf16 from first tensor type (torch.bfloat16)\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {8192, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00007-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.36.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.36.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.36.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.36.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.36.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.36.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.36.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.36.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.37.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.37.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.37.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.37.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.37.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.37.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00008-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.37.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.37.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.37.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.38.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.38.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.38.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.38.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.38.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.38.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.38.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.38.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.38.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.39.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.39.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.39.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.39.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.39.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.39.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.39.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.39.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.40.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.40.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.40.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.40.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.40.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.40.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.40.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.40.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.41.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.41.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.41.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.41.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.41.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.41.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.41.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.41.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.42.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.42.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.42.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.42.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.42.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.42.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.42.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.42.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.43.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.43.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.43.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.43.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.43.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00009-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.43.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.43.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.43.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.43.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.44.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.44.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.44.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.44.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.44.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.44.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.45.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.45.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.45.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.45.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.45.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.45.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.45.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.45.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.46.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.46.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.46.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.46.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.46.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.46.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.46.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.46.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.47.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.47.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.47.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.47.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.47.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.47.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.47.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.47.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.48.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.48.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.48.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.48.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.48.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.48.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.48.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.48.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.49.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.49.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.49.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00010-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.49.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.49.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.49.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.49.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.49.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.50.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.50.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.50.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.50.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.50.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.50.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.51.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.51.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.51.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.51.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.51.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.51.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.51.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.51.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.52.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.52.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.52.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.52.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.52.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.52.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.52.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.52.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.53.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.53.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.53.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.53.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.53.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.53.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.53.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.53.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.54.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.54.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.54.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.54.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.54.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.54.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.54.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.54.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00011-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.55.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.55.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.55.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.55.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.55.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.55.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.55.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.55.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.55.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.56.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.56.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.56.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.56.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.56.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.56.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.56.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.56.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.57.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.57.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.57.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.57.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.57.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.57.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.57.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.57.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.58.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.58.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.58.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.58.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.58.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.58.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.58.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.58.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.59.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.59.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.59.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.59.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.59.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.59.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.59.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.59.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.60.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.60.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.60.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.60.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.60.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.60.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00012-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.60.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.60.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.60.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.61.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.61.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.61.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.61.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.61.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.61.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.62.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.62.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.62.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.62.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.62.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.62.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.62.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.62.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.63.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.63.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.63.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.63.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.63.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.63.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.63.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.63.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.64.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.64.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.64.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.64.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.64.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.64.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.64.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.64.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.64.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.65.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.65.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.65.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.65.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.65.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.65.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.65.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.65.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.65.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.66.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.66.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.66.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.66.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.66.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00013-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.66.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.66.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.66.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.66.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.67.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.67.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.67.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.67.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.67.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.67.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.67.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.67.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.67.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.68.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.68.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.68.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.68.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.68.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.68.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.68.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.68.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.68.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.69.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.69.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.69.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.69.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.69.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.69.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.69.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.69.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.69.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.70.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.70.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.70.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.70.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.70.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.70.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.70.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.70.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.70.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.71.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.71.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.71.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.71.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.71.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.71.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.71.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.71.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.71.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.72.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.72.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.72.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.72.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00014-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:blk.72.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.72.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.72.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.72.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.72.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.73.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.73.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.73.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.73.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.73.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.73.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.73.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.73.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.73.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.74.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.74.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.74.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.74.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.74.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.74.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.74.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.74.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.74.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.75.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.75.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.75.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.75.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.75.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.75.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.75.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.75.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.75.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.76.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.76.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.76.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.76.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.76.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.76.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.76.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.76.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.76.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.77.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.77.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.77.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.77.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.77.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.77.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.77.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.77.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.77.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00015-of-00015.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {8192, 128256}\n",
      "INFO:hf-to-gguf:blk.78.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.78.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.78.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.78.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.78.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.78.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.78.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.78.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.78.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.79.attn_norm.weight,     torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.79.ffn_down.weight,      torch.int8 --> BF16, shape = {28672, 8192}\n",
      "INFO:hf-to-gguf:blk.79.ffn_gate.weight,      torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.79.ffn_up.weight,        torch.int8 --> BF16, shape = {8192, 28672}\n",
      "INFO:hf-to-gguf:blk.79.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:blk.79.attn_k.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:blk.79.attn_output.weight,   torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.79.attn_q.weight,        torch.int8 --> BF16, shape = {8192, 8192}\n",
      "INFO:hf-to-gguf:blk.79.attn_v.weight,        torch.int8 --> BF16, shape = {8192, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {8192}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 8192\n",
      "INFO:hf-to-gguf:gguf: embedding length = 8192\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 28672\n",
      "INFO:hf-to-gguf:gguf: head count = 64\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "DEBUG:hf-to-gguf:chktok: [128000, 198, 4815, 15073, 66597, 8004, 1602, 2355, 79772, 11187, 9468, 248, 222, 320, 8416, 8, 27623, 114, 102470, 9468, 234, 104, 31643, 320, 36773, 100166, 98634, 8, 26602, 227, 11410, 99, 247, 9468, 99, 247, 220, 18, 220, 1644, 220, 8765, 220, 8765, 18, 220, 8765, 1644, 220, 8765, 8765, 220, 8765, 8765, 18, 220, 8765, 8765, 1644, 220, 18, 13, 18, 220, 18, 497, 18, 220, 18, 1131, 18, 220, 21549, 222, 98629, 241, 45358, 233, 21549, 237, 45358, 224, 21549, 244, 21549, 115, 21549, 253, 45358, 223, 21549, 253, 21549, 95, 98629, 227, 76460, 223, 949, 37046, 101067, 19000, 23182, 102301, 9263, 18136, 16, 36827, 21909, 56560, 54337, 19175, 102118, 13373, 64571, 34694, 3114, 112203, 80112, 3436, 106451, 14196, 14196, 74694, 3089, 3089, 29249, 17523, 3001, 27708, 7801, 358, 3077, 1027, 364, 83, 820, 568, 596, 1070, 11, 364, 793, 499, 2771, 30, 364, 44, 539, 2771, 358, 3358, 1304, 433, 11, 364, 35, 499, 1093, 1063, 15600, 30, 1226, 6, 43712, 264, 64966, 43]\n",
      "DEBUG:hf-to-gguf:chkhsh: 0ef9807a4087ebef797fc749390439009c3b9eda9ad1a097abbe738f486c01e5\n",
      "DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'llama-bpe'\n",
      "DEBUG:hf-to-gguf:chkhsh: 0ef9807a4087ebef797fc749390439009c3b9eda9ad1a097abbe738f486c01e5\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128255\n",
      "INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/gguf/v7_r8_a16_ours_70B-checkpoint-100.gguf: n_tensors = 723, total_size = 141.1G\n",
      "Writing:  18%|‚ñà‚ñä        | 24.8G/141G [01:18<06:17, 308Mbyte/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# command = (\n",
    "#     f\"python ../../llama.cpp/convert_lora_to_gguf.py \"\n",
    "#     f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "#     f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "#     f\"--outtype f16 \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "#     f\"--verbose \"                            # Optional: increase logging output\n",
    "#     f\"{checkpoint_dir}\"                      # Positional argument: path to the LoRA adapter files\n",
    "# )\n",
    "\n",
    "command = (\n",
    "    f\"python ../../llama.cpp/convert_hf_to_gguf.py \"\n",
    "    # f\"--base {model_dir} \"              # Provide the base model config if needed\n",
    "    f\"--outfile {output_path} \"              # Output file for the GGUF model\n",
    "    f\"--outtype auto \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    "    f\"--verbose \"                            # Optional: increase logging output\n",
    "    f\"{lora_output_dir}/{lora_repr}-checkpoint-{step}\"                      # Positional argument: path to the LoRA adapter files\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: ../../llama.cpp/build/bin/llama-quantize /workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/gguf/v7_r8_a16_ours_70B-checkpoint-100.gguf /workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/gguf/v7_r8_a16_ours_70B-checkpoint-100-Q4_K_M.gguf Q4_K_M \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 4869 (2c9f833d)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/gguf/v7_r8_a16_ours_70B-checkpoint-100.gguf' to '/workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/gguf/v7_r8_a16_ours_70B-checkpoint-100-Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_quantize: failed to quantize: tensor 'blk.14.ffn_up.weight' data is not within the file bounds, model is corrupted or incomplete\n",
      "main: failed to quantize model from '/workspace/LGHVAC_2ndyear/model/Bllossom-llama-3-Korean-Bllossom-70B/gguf/v7_r8_a16_ours_70B-checkpoint-100.gguf'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = (\n",
    "    f\"../../llama.cpp/build/bin/llama-quantize \"\n",
    "    f\"{output_path} \"\n",
    "    f\"{output_path.replace('.gguf', '-Q4_K_M.gguf')} \"                      # Positional argument: path to the LoRA adapter files\n",
    "    f\"Q4_K_M \"                        # Use f16 (or choose f32, bf16, q8_0, auto as needed)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Running command:\", command)\n",
    "os.system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
