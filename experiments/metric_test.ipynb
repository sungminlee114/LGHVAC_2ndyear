{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# device_num = \"0, 1\"\n",
    "\n",
    "# # !export CUDA_VISIBLE_DEVICES=device_num\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=f\"{device_num}\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "import torch\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#PYTHONUTF8=1\n",
    "# os.environ['PYTHONUTF8'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"auto\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "# from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "# from unsloth import FastLanguageModel\n",
    "import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# bits and bytes\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using flash_attention_2 for attention computation.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 3000     # Unsloth auto supports RoPE Scaling internally!\n",
    "# dtype = None              # None for auto detection\n",
    "load_in_4bit = False      # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "# End\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "# attn_implementation = \"eager\"\n",
    "# attn_implementation = \"flash_attention_2\"\n",
    "# torch_dtype = torch.bfloat16\n",
    "print(f\"Using {attn_implementation} for attention computation.\")\n",
    "\n",
    "BASE_DIR = \"../finetuning/try_lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'site_name': 'YongDongIllHighSchool', 'user_name': '홍길동', 'user_role': 'customer', 'idu_name': '01_IB5', 'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_IB1': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도'], 'oper': ['전원']}, '말투': '존댓말', '언어': '한국어', 'current_datetime': '2022-09-30 12:00:00'}\n"
     ]
    }
   ],
   "source": [
    "common_prompt = \"\"\"\n",
    "나는 훌룡한 HVAC 관련 질문 답변을 제공하는 인공지능이다. 사용자의 질문을 받아들여서, 그에 맞는 답변을 제공하는 것이 내 임무이다. 사용자의 질문을 받아들일 때, 다음과 같은 절차를 따라야 한다.\n",
    "<출력 내용>\n",
    "1. 'Formalized Input': 사용자의 추상적 질문을 구체화 및 정규화한 결과. 다양한 형태의 질문들을 가장 핵심적이고 근본적인 질문으로 변환한 결과.\n",
    "2. 'Input Semantic Parsing': Input Semantic Parsing 결과. dict 형태로 구성되며, Temporal, Spatial, Modality, Operation을 가짐.\n",
    "3. 'Strategy': 질문을 해결하기 위한 전략을 답변 전 고민. Objective: 질문의 근본적 의도 및 답변의 목적. Expected Output: 답변의 예상 결과. Step: 답변을 위한 구체으로 쪼개진 단계.\n",
    "4. 'Instruction Set': 문제 해결을 위해 나의 실제 행동.\n",
    "\n",
    "<DDL statement>\n",
    "CREATE TABLE IF NOT EXISTS data_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
    "    idu_id integer,\n",
    "    roomtemp double precision,\n",
    "    settemp double precision,\n",
    "    oper boolean,\n",
    "    \"timestamp\" timestamp without time zone NOT NULL\n",
    ")\n",
    "    \n",
    "CREATE TABLE IF NOT EXISTS idu_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
    "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
    "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
    "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "current_metadata = json.load(open(f\"{BASE_DIR}/metadata.json\", \"r\"))\n",
    "\n",
    "print(current_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Input': '지금 옆반 에어컨 상태 알려줘', 'Reference': {'Formalized Input': '지금 옆반의 에어컨 전원 상태 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'옆반': '01_IB7'}, 'Modality': {'에어컨 전원': 'oper'}}, 'Strategy': {'Objective': '현재 옆반의 에어컨 전원 상태를 조회함.', 'Expected Outputs': ['옆반의 에어컨은 현재 켜져있습니다.'], 'Step': ['옆반의 현재 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장', 'qr_aircon_oper에서 에어컨 전원 상태 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['oper'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB7')\"}}, 'result': {'name': 'qr_aircon_oper'}}, {'type': 'o', 'python_script_to_process': \"oper_status = qr_aircon_oper['oper'].values[0]\", 'return': ['oper_status']}, {'type': 'r', 'templates': ['옆반의 에어컨은 현재 {oper_status}입니다.']}]}, 'Candidate': {'Formalized Input': '지금 옆반의 에어컨 전원 상태 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'옆반': '01_IB5'}, 'Modality': {'에어컨 전원': 'oper'}}, 'Strategy': {'Objective': '현재 옆반의 에어컨 전원 상태를 조회함.', 'Expected Outputs': ['옆반의 에어컨은 현재 켜져있습니다.', '옆반의 에어컨은 현재 꺼져있습니다.'], 'Step': ['옆반의 현재 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장', 'qr_aircon_oper에서 에어컨 전원 상태 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['oper'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_aircon_oper'}}, {'type': 'o', 'python_script_to_process': \"oper_status = qr_aircon_oper['oper'].values[0]\", 'return': ['oper_status']}, {'type': 'r', 'templates': ['옆반의 에어컨은 현재 {oper_status}입니다.', '옆반의 에어컨은 현재 {oper_status}입니다.']}]}}\n",
      "{'Input': '이번주 우리반 평균 온도 알려줘', 'Reference': {'Formalized Input': '이번주 우리반의 평균 실내온도 알려줘.', 'Input Semantic Parsing': {'Temporal': {'이번주': '2022-09-26 00:00:00 ~ 2022-10-02 23:59:59'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['평균']}, 'Strategy': {'Objective': '이번주 우리반의 실내온도 데이터를 쿼리 후 평균값 계산.', 'Expected Outputs': ['이번주 우리반의 평균 실내온도는 24℃ 입니다.'], 'Step': ['우리반의 이번주 실내온도 데이터를 쿼리결과 qr_roomtemp_week에 저장', 'qr_roomtemp_week에서 roomtemp의 평균값 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-26 00:00:00' AND '2022-10-02 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_roomtemp_week'}}, {'type': 'o', 'python_script_to_process': \"avg_roomtemp = qr_roomtemp_week['roomtemp'].mean()\", 'return': ['avg_roomtemp']}, {'type': 'r', 'templates': ['이번주 우리반의 평균 실내온도는 {avg_roomtemp}℃ 입니다.']}]}, 'Candidate': '{\"Formalized Input\": \"이번주 우리반의 평균 실내온도 알려줘.\", \"Input Semantic Parsing\": {\"Temporal\": {\"이번주\": \"2022-09-26 00:00:00 ~ 2022-10-02 23:59:59\"}, \"Spatial\": {\"우리반\": \"01_IB5\"}, \"Modality\": {\"실내온도\": \"roomtemp\"}, \"Operation\": [\"평균\"]}, \"Strategy\": {\"Objective\": \"이번주 우리반의 실내온도 데이터 중 평균값을 계산함.\", \"Expected Outputs\": [\"이번주 우리반의 평균 온도는 25℃입니다.\", \"이번주 우리반의 평균 온도는 26℃입니다.\"], \"Step\": [\"우리반의 이번주 실내온도 데이터를 쿼리해 qr_roomtemp_weekly에 저장\", \"qr_roomtemp_weekly에서 roomtemp의 평균값 계산\", \"평균값 반환\"]}, \"Instruction Set\": [{\"type\": \"q\", \"run\": {\"args\": {\"table_name\": \"data_t\", \"columns\": [\"roomtemp\"], \"conditions\": [\"timestamp\" BETWEEN \"2022-09-26 00:00:00\" AND \"2022-10-02 23:59:59\"\\'], \"subquery\": \"idu_id = (SELECT id FROM idu_t WHERE name = \\'01_IB5\\')\"}}, \"result\": {\"name\": \"qr_roomtemp_weekly\"}}, {\"type\": \"o\", \"python_script_to_process\": \"avg_roomtemp = qr_roomtemp_weekly[\\'roomtemp\\'].mean()\", \"return\": [\"avg_roomtemp\"]}, {\"type\": \"r\", \"templates\": [\"이번주 우리반의 평균 온도는 {avg_roomtemp}℃입니다.\", \"이번주 우리반의 평균 온도는 {avg_roomtemp}℃입니다.\"]}]}'}\n",
      "{'Input': '이번달 우리반 설정온도 최대값과 최소값 알려줘', 'Reference': {'Formalized Input': '이번달 우리반의 설정온도 최대값과 최소값 알려줘.', 'Input Semantic Parsing': {'Temporal': {'이번달': '2022-09-01 00:00:00 ~ 2022-09-30 23:59:59'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'설정온도': 'settemp'}, 'Operation': ['최대값', '최소값']}, 'Strategy': {'Objective': '이번달 우리반의 설정온도 데이터를 쿼리 후 최대값과 최소값 계산.', 'Expected Outputs': ['이번달 우리반의 설정온도는 최대 28℃, 최소 22℃입니다.'], 'Step': ['우리반의 이번달 설정온도 데이터를 쿼리결과 qr_settemp_month에 저장', 'qr_settemp_month에서 설정온도의 최대값과 최소값 계산', '최대값과 최소값 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['settemp'], 'conditions': [\"timestamp BETWEEN '2022-09-01 00:00:00' AND '2022-09-30 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_settemp_month'}}, {'type': 'o', 'python_script_to_process': \"max_settemp = qr_settemp_month['settemp'].max(); min_settemp = qr_settemp_month['settemp'].min()\", 'return': ['max_settemp', 'min_settemp']}, {'type': 'r', 'templates': ['이번달 우리반의 설정온도는 최대 {max_settemp}℃, 최소 {min_settemp}℃입니다.']}]}, 'Candidate': {'Formalized Input': '이번달 우리반의 설정온도 중 최고값과 최저값 알려줘.', 'Input Semantic Parsing': {'Temporal': {'이번달': '2022-09-01 00:00:00 ~ 2022-09-30 23:59:59'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'설정온도': 'settemp'}, 'Operation': ['최고값', '최저값']}, 'Strategy': {'Objective': '이번달 우리반의 설정온도 데이터 중 날짜별 최고값과 최저값을 조회함.', 'Expected Outputs': ['이번달 우리반의 설정온도는 27℃(최고)와 25℃(최저)입니다.', '이번달 우리반의 설정온도는 27℃입니다. 최고값은 27℃, 최저값은 25℃입니다.'], 'Step': ['우리반의 이번달 설정온도 데이터를 쿼리해 qr_settemp에 저장', 'qr_settemp에서 날짜별 설정온도 값들을 쿼리해 roomtemp_max와 roomtemp_min에 저장', 'roomtemp_max와 roomtemp_min를 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-01 00:00:00' AND '2022-09-30 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_settemp'}}, {'type': 'o', 'python_script_to_process': \"roomtemp_max = qr_settemp['roomtemp_max'].values[0]; roomtemp_min = qr_settemp['roomtemp_min'].values[0]\", 'return': ['roomtemp_max', 'roomtemp_min']}, {'type': 'r', 'templates': ['이번달 우리반의 설정온도는 {roomtemp_max}℃(최고)와 {roomtemp_min}℃(최저)입니다.', '이번달 우리반의 설정온도는 {roomtemp_max}℃입니다. 최고값은 {roomtemp_max}, 최저값은 {roomtemp_min}입니다.']}]}}\n",
      "{'Input': '우리반과 옆반중 더 추운곳은 어디야?', 'Reference': {'Formalized Input': '지금 우리반과 옆반 중 실내온도가 더 낮은 곳을 찾아줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'우리반': '01_IB5', '옆반': '01_IB7'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['최저']}, 'Strategy': {'Objective': '현재 우리반과 옆반의 실내온도 중 가장 낮은 온도를 가진 반을 찾고, 두 온도의 차이를 계산.', 'Expected Outputs': ['옆반(17.0℃)이 우리반(19.3℃)보다 2.3℃ 더 낮아요.'], 'Step': ['우리반의 현재 실내온도 데이터를 쿼리해 qr_roomtemp_ours에 저장', '옆반의 현재 실내온도 데이터를 쿼리해 qr_roomtemp_beside에 저장', '두 반의 실내온도를 비교하여 더 낮은 온도를 가진 반을 찾음', '각 반의 실내온도 및 차이 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5'))\"}}, 'result': {'name': 'qr_roomtemp_ours'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7'))\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'o', 'python_script_to_process': \"roomtemp_ours = qr_roomtemp_ours['roomtemp'].values[0]; roomtemp_beside = qr_roomtemp_beside['roomtemp'].values[0]; temp_diff = roomtemp_ours - roomtemp_beside\", 'result': ['roomtemp_ours', 'roomtemp_beside', 'temp_diff']}, {'type': 'r', 'templates': ['{room_with_lower_temp}({roomtemp_lower}℃)이 {room_with_higher_temp}({roomtemp_higher}℃)보다 {temp_diff}℃ 더 낮아요.']}]}, 'Candidate': {'Formalized Input': '지금 우리반과 옆반 중 더 추운 곳을 찾는 것이 내 임무입니다.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'우리반': '01_IB5', '옆반': '01_IB7'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['최저']}, 'Strategy': {'Objective': '현재 우리반과 옆반의 실내온도 중 가장 낮은 온도를 가진 반을 찾고, 두 온도의 차이를 계산.', 'Expected Outputs': ['옆반(17.0℃)이 우리반(19.3℃)보다 2.3℃ 더 낮아요.', '옆반(17.0℃)이 우리반(19.3℃)보다 2.3℃ 더 낮아요.'], 'Step': ['우리반의 현재 실내온도 데이터를 쿼리해 qr_roomtemp_ours에 저장', '옆반의 현재 실내온도 데이터를 쿼리해 qr_roomtemp_beside에 저장', '두 반의 실내온도 중 가장 낮은 온도를 가진 반을 찾음', '각 반의 실내온도와 차이 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5')\"}}, 'result': {'name': 'qr_roomtemp_ours'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7')\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'o', 'python_script_to_process': \"roomtemp_ours = qr_roomtemp_ours['roomtemp'].values[0]; roomtemp_beside = qr_roomtemp_beside['roomtemp'].values[0]; temp_diff = roomtemp_ours - roomtemp_beside\", 'result': ['roomtemp_ours', 'roomtemp_beside', 'temp_diff']}, {'type': 'r', 'templates': ['{room_with_lower_temp}({roomtemp_lower}℃)이 {room_with_higher_temp}({roomtemp_higher}℃) 보다 {temp_diff}℃ 더 낮아요.', '{room_with_lower_temp}({roomtemp_lower}℃)이 {room_with_higher_temp}({roomtemp_higher}℃) 보다 {temp_diff}℃ 더 낮아요.']}]}}\n",
      "{'Input': '에어컨 켜져있어?', 'Reference': {'Formalized Input': '지금 우리반 에어컨 전원 상태 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'에어컨 전원': 'oper'}}, 'Strategy': {'Objective': '현재 우리반의 에어컨 전원 상태를 조회함.', 'Expected Outputs': ['우리반의 에어컨은 현재 켜져있습니다.'], 'Step': ['우리반의 현재 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장', 'qr_aircon_oper에서 에어컨 전원 상태 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['oper'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_aircon_oper'}}, {'type': 'o', 'python_script_to_process': \"oper_status = qr_aircon_oper['oper'].values[0]\", 'return': ['oper_status']}, {'type': 'r', 'templates': ['우리반의 에어컨은 현재 {oper_status}입니다.']}]}, 'Candidate': {'Formalized Input': '지금 우리반 에어컨 상태 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'에어컨 전원': 'oper'}}, 'Strategy': {'Objective': '현재 우리반의 에어컨 상태를 조회함.', 'Expected Outputs': ['우리반의 에어컨은 현재 꺼져있습니다.', '우리반의 에어컨은 현재 25℃로 켜져있습니다.'], 'Step': ['우리반의 현재 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장', 'qr_aircon_oper에서 에어컨 전원 상태 반환', '에어컨 전원 상태 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['oper'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_aircon_oper'}}, {'type': 'o', 'python_script_to_process': \"oper_status = qr_aircon_oper['oper'].values[0]\", 'return': ['oper_status']}, {'type': 'r', 'templates': ['우리반의 에어컨은 현재 {oper_status}입니다.', '우리반의 에어컨은 현재 {roomtemp}℃로 {oper}입니다.']}]}}\n",
      "{'Input': '앞반 지금 전원 켜져있어?', 'Reference': {'Formalized Input': '지금 앞반의 에어컨 전원 상태 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'앞반': '02_IB1'}, 'Modality': {'에어컨 전원': 'oper'}}, 'Strategy': {'Objective': '현재 앞반의 에어컨 전원 상태를 조회함.', 'Expected Outputs': ['앞반의 에어컨은 현재 꺼져있습니다.'], 'Step': ['앞반의 현재 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장', 'qr_aircon_oper에서 에어컨 전원 상태 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['oper'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '02_IB1')\"}}, 'result': {'name': 'qr_aircon_oper'}}, {'type': 'o', 'python_script_to_process': \"oper_status = qr_aircon_oper['oper'].values[0]\", 'return': ['oper_status']}, {'type': 'r', 'templates': ['앞반의 에어컨은 현재 {oper_status}입니다.']}]}, 'Candidate': {'Formalized Input': '지금 앞반의 에어컨 전원 상태 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'앞반': '01_IB5'}, 'Modality': {'에어컨 전원': 'oper'}}, 'Strategy': {'Objective': '현재 앞반의 에어컨 전원 상태를 조회함.', 'Expected Outputs': ['앞반의 에어컨은 현재 켜져있습니다.', '앞반의 에어컨은 현재 꺼져있습니다.'], 'Step': ['현재 앞반의 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장', 'qr_aircon_oper에서 에어컨 전원 상태 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['oper'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_aircon_oper'}}, {'type': 'o', 'python_script_to_process': \"oper_status = qr_aircon_oper['oper'].values[0]\", 'return': ['oper_status']}, {'type': 'r', 'templates': ['앞반의 에어컨은 현재 {oper_status}입니다.', '앞반의 에어컨은 현재 {oper_status}입니다.']}]}}\n",
      "{'Input': '어제 전원 껐어?', 'Reference': {'Formalized Input': '어제 우리반 에어컨 전원 상태 알려줘.', 'Input Semantic Parsing': {'Temporal': {'어제': '2022-09-29 00:00:00 ~ 2022-09-29 23:59:59'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'에어컨 전원': 'oper'}}, 'Strategy': {'Objective': '어제동안 우리반의 에어컨 전원 상태를 조회함.', 'Expected Outputs': ['어제 우리반의 에어컨은 꺼져있습니다.', '어제 우리반의 에어컨은 17시부터 19시까지 켜져있었습니다.'], 'Step': ['우리반의 어제 하루 시간별 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장', 'qr_aircon_oper에서 에어컨 전원이 특정 상태(켜짐/꺼짐)인 시간대 찾기', '상태별 시간대 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['timestamp', 'oper'], 'conditions': [\"timestamp BETWEEN '2022-09-29 00:00:00' AND '2022-09-29 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_aircon_oper'}}, {'type': 'o', 'python_script_to_process': \"oper_on_time = qr_aircon_oper[qr_aircon_oper['oper'] == true]['timestamp'].dt.strftime('%H:%M').values; oper_off_time = qr_aircon_oper[qr_aircon_oper['oper'] == false]['timestamp'].dt.strftime('%H:%M').values\", 'return': ['oper_on_time', 'oper_off_time']}, {'type': 'r', 'templates': ['어제 우리반의 에어컨은 꺼져있습니다.', '어제 우리반의 에어컨은 {oper_on_time[0]}부터 {oper_off_time[0]}까지 켜져있었습니다.']}]}, 'Candidate': '{\"Formalized Input\": \"어제 우리반 에어컨 전원 상태 알려줘.\", \"Input Semantic Parsing\": {\"Temporal\": {\"어제\": \"2022-09-29 00:00:00 ~ 2022-09-29 23:59:59\"}, \"Spatial\": {\"우리반\": \"01_IB5\"}, \"Modality\": {\"에어컨 전원\": \"oper\"}}, \"Strategy\": {\"Objective\": \"어제 우리반의 에어컨 전원 상태를 조회함.\", \"Expected Outputs\": [\"어제 우리반의 에어컨은 꺼져있습니다.\", \"어제 우리반의 에어컨은 17시부터 19시까지 켜져있었습니다.\"], \"Step\": [\"우리반의 어제 시간별 에어컨 전원 상태 데이터를 쿼리해 qr_aircon_oper에 저장\", \"qr_aircon_oper에서 에어컨 전원 상태 반환\"]}, \"Instruction Set\": [{\"type\": \"q\", \"run\": {\"args\": {\"table_name\": \"data_t\", \"columns\": [\"oper\"], \"conditions\": [\"timestamp BETWEEN \\'2022-09-29 00:00:00\\' AND \\'2022-09-29 23:59:59\\'\"], \"subquery\": \"idu_id = (SELECT id FROM idu_t WHERE name = \\'01_IB5\\')\"}}, \"result\": {\"name\": \"qr_aircon_oper\"}}, {\"type\": \"o\", \"python_script_to_process\": \"oper_status = qr_aircon_oper[\\'oper\\'].values\", \"return\": [\"oper_status\"]}, {\"type\": \"r\", \"templates\": [\"어제 우리반의 에어컨은 꺼져있습니다.\\', \\'어제 우리반의 에어컨은 17시부터 19시까지 켜져있었습니다.\\']}]}'}\n",
      "{'Input': '지금 옆반 온도랑 우리반 온도 알려줘', 'Reference': {'Formalized Input': '지금 옆반과 우리반의 실내온도 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'옆반': '01_IB7', '우리반': '01_IB5'}, 'Modality': {'실내온도': 'roomtemp'}}, 'Strategy': {'Objective': '현재 옆반과 우리반의 실내온도를 조회함.', 'Expected Outputs': ['옆반의 실내온도는 25℃, 우리반의 실내온도는 27℃입니다.'], 'Step': ['옆반의 현재 실내온도 데이터를 쿼리해 qr_roomtemp_beside에 저장', '우리반의 현재 실내온도 데이터를 쿼리해 qr_roomtemp_ours에 저장', '두 반의 실내온도 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7'))\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5'))\"}}, 'result': {'name': 'qr_roomtemp_ours'}}, {'type': 'o', 'python_script_to_process': \"roomtemp_beside = qr_roomtemp_beside['roomtemp'].values[0]; roomtemp_ours = qr_roomtemp_ours['roomtemp'].values[0]\", 'return': ['roomtemp_beside', 'roomtemp_ours']}, {'type': 'r', 'templates': ['옆반의 실내온도는 {roomtemp_beside}℃, 우리반의 실내온도는 {roomtemp_ours}℃입니다.']}]}, 'Candidate': {'Formalized Input': '지금 옆반과 우리반의 현재 온도 알려줘.', 'Input Semantic Parsing': {'Temporal': {'지금': '2022-09-30 12:00:00'}, 'Spatial': {'옆반': '01_IB7', '우리반': '01_IB5'}, 'Modality': {'온도': 'roomtemp'}}, 'Strategy': {'Objective': '현재 옆반과 우리반의 온도 데이터를 쿼리 후, 각각의 온도 반환', 'Expected Outputs': ['옆반의 현재 온도는 25℃, 우리반의 현재 온도는 27℃입니다.', '지금 옆반의 온도는 옆반이므로 25℃입니다., 우리반의 온도는 실내이므로 27℃입니다.'], 'Step': ['옆반의 현재 온도 데이터 쿼리결과 qr_roomtemp_beside에 저장', '우리반의 현재 온도 데이터 쿼리결과 qr_roomtemp_ours에 저장', '두 반의 온도 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7'))\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-09-30 12:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5'))\"}}, 'result': {'name': 'qr_roomtemp_ours'}}, {'type': 'o', 'python_script_to_process': \"roomtemp_beside = qr_roomtemp_beside['roomtemp'].values[0]; roomtemp_ours = qr_roomtemp_ours['roomtemp'].values[0]\", 'return': ['roomtemp_beside', 'roomtemp_ours']}, {'type': 'r', 'templates': ['옆반의 현재 온도는 {roomtemp_beside}℃, 우리반의 현재 온도는 {roomtemp_ours}℃입니다.', '지금 옆반의 온도는 {roomtemp_beside}℃입니다., 우리반의 온도는 {roomtemp_ours}℃입니다.']}]}}\n",
      "{'Input': '이번주 우리반과 옆반 온도 변화 추이 비교해줘', 'Reference': {'Formalized Input': '이번주 우리반과 옆반의 실내온도 변화 추이 비교해줘.', 'Input Semantic Parsing': {'Temporal': {'이번주': '2022-09-26 00:00:00 ~ 2022-10-02 23:59:59'}, 'Spatial': {'우리반': '01_IB5', '옆반': '01_IB7'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['변화 추이', '비교']}, 'Strategy': {'Objective': '이번주 우리반과 옆반의 실내온도 데이터를 쿼리 후, 각 반의 변화 추이를 시각화하고 주요 통계 정보를 비교함.', 'Expected Outputs': ['(그래프) X축: 날짜, Y축: 실내온도, 두 반의 변화 추이 그래프', '이번주 우리반의 평균 실내온도는 24℃, 최저 22℃, 최고 26℃입니다.', '이번주 옆반의 평균 실내온도는 25℃, 최저 23℃, 최고 28℃입니다.'], 'Step': ['우리반과 옆반의 이번주 실내온도 데이터를 각각 쿼리해 qr_roomtemp_ours와 qr_roomtemp_beside에 저장', '두 반의 데이터를 시각화하여 변화 추이 비교 그래프 생성', '각 반의 평균, 최저, 최고값 계산', '그래프와 통계적 비교 정보 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['timestamp', 'roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-26 00:00:00' AND '2022-10-02 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_roomtemp_ours'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['timestamp', 'roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-26 00:00:00' AND '2022-10-02 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB7')\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'o', 'python_script_to_process': \"avg_ours = qr_roomtemp_ours['roomtemp'].mean(); min_ours = qr_roomtemp_ours['roomtemp'].min(); max_ours = qr_roomtemp_ours['roomtemp'].max(); avg_beside = qr_roomtemp_beside['roomtemp'].mean(); min_beside = qr_roomtemp_beside['roomtemp'].min(); max_beside = qr_roomtemp_beside['roomtemp'].max(); merged_data = pd.merge(qr_roomtemp_ours, qr_roomtemp_beside, on='timestamp', suffixes=('_ours', '_beside')); merged_data.plot(x='timestamp', y=['roomtemp_ours', 'roomtemp_beside'])\", 'return': ['avg_ours', 'min_ours', 'max_ours', 'avg_beside', 'min_beside', 'max_beside']}, {'type': 'r', 'templates': ['(그래프) X축: 날짜, Y축: 실내온도, 우리반과 옆반의 변화 추이 비교 그래프', '이번주 우리반의 평균 실내온도는 {avg_ours}℃, 최저 {min_ours}℃, 최고 {max_ours}℃입니다.', '이번주 옆반의 평균 실내온도는 {avg_beside}℃, 최저 {min_beside}℃, 최고 {max_beside}℃입니다.']}]}, 'Candidate': {'Formalized Input': '이번주 우리반과 옆반의-room온도 변화 추이를 비교해줘.', 'Input Semantic Parsing': {'Temporal': {'이번주': '2022-09-26 00:00:00 ~ 2022-10-02 23:59:59'}, 'Spatial': {'우리반': '01_IB5', '옆반': '01_IB7'}, 'Modality': {'ROOM온도': 'roomtemp'}, 'Operation': ['변화 추이의 차이']}, 'Strategy': {'Objective': '이번주 우리반과 옆반 각각의 ROOM온도 데이터를 쿼리 후, 두 값의 차이와 평균 값을 계산함.', 'Expected Outputs': ['(우리반) vs (옆반) - 1℃', '(우리반)의 평균 온도는 25℃, (옆반)의 평균 온도는 26℃입니다.', '이번주 우리반(25℃)이 옆반(26℃)보다 1℃ 낮아요.', '이번주 우리반(25℃)이 옆반(26℃)보다 1℃ 더 낮아요.'], 'Step': ['우리반의 이번주 ROOM온도 데이터 쿼리결과 qr_roomtemp_ours_weekly에 저장', '옆반의 이번주 ROOM온도 데이터 쿼리결과 qr_roomtemp_beside_weekly에 저장', '각 반의 ROOM온도 데이터를 시각화하여 변화 추이 시각화', '각 반의 ROOM온도 데이터의 평균값 계산', '각 반의 평균온도와 다른값 및 차이 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-26 00:00:00' AND '2022-10-02 23:59:59'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5'))\"}}, 'result': {'name': 'qr_roomtemp_ours_weekly'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2022-09-26 00:00:00' AND '2022-10-02 23:59:59'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7'))\"}}, 'result': {'name': 'qr_roomtemp_beside_weekly'}}, {'type': 'o', 'python_script_to_process': \"roomtemp_ours_weekly = qr_roomtemp_ours_weekly['roomtemp'].values; roomtemp_beside_weekly = qr_roomtemp_beside_weekly['roomtemp'].values; roomtemp_diff = roomtemp_ours_weekly - roomtemp_beside_weekly; roomtemp_avg_ours = roomtemp_ours_weekly.mean(); roomtemp_avg_beside = roomtemp_beside_weekly.mean(); roomtemp_diff_avg = (roomtemp_diff).mean();\", 'return': ['roomtemp_diff', 'roomtemp_diff_avg', 'roomtemp_avg_ours', 'roomtemp_avg_beside']}, {'type': 'r', 'templates': ['(우리반) vs (옆반) - {roomtemp_diff_avg}℃', '({roomtemp_avg_ours}) vs ({roomtemp_avg_beside}) - {roomtemp_diff_avg}℃', '이번주 우리반({roomtemp_avg_ours})이 옆반({roomtemp_avg_beside})보다 {roomtemp_diff_avg}℃ 더 낮아요.', '이번주 우리반({roomtemp_avg_ours})이 옆반({roomtemp_avg_beside})보다 {roomtemp_diff_avg}℃ 더 낮아요.']}]}}\n",
      "{'Input': 'Why is our classroom so cold', 'Reference': {'Formalized Input': '우리반의 실내온도가 낮은 이유를 분석해줘.', 'Input Semantic Parsing': {'Temporal': {'현재': '2022-09-30 12:00:00'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['이유 분석']}, 'Strategy': {'Objective': '우리반의 실내온도가 낮은 이유를 분석하기 위해 설정온도, 실내온도, 에어컨 작동 상태 데이터를 수집하고 종합적으로 비교 분석함.', 'Expected Outputs': ['우리반의 실내온도가 낮은 이유는 설정온도가 18℃로 너무 낮기 때문입니다.', '지금 우리반이 추운 이유는 에어컨이 계속 켜져 있기 때문입니다.', '우리반이 추운 이유는 에어컨이 켜진지 5분밖에 안 되었기 때문입니다.'], 'Step': ['우리반의 최근 데이터를 쿼리해 qr_ours_recent에 저장', '데이터를 분석하여 실내온도가 낮은 원인 식별', '원인을 텍스트로 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp', 'settemp', 'oper'], 'conditions': [\"timestamp BETWEEN '2022-09-30 00:00:00' AND '2022-09-30 12:00:00'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_ours_recent'}}, {'type': 'o', 'python_script_to_process': \"recent_settemps = qr_ours_recent['settemp']; recent_roomtemps = qr_ours_recent['roomtemp']; recent_opers = qr_ours_recent['oper']\", 'return': ['recent_settemps', 'recent_roomtemps', 'recent_opers']}, {'type': 'r', 'templates': ['우리반의 실내온도가 낮은 이유는 설정온도가 {recent_settemps.values[-1]}℃로 너무 낮기 때문입니다.', '지금 우리반이 추운 이유는 에어컨이 {recent_opers.values[-1]} 상태이기 때문입니다.', '우리반이 추운 이유는 에어컨이 켜진지 {latest_oper_on_time}밖에 안 되었기 때문입니다.']}]}, 'Candidate': {'Formalized Input': 'Our classroom temperature is higher than the set temperature.', 'Input Semantic Parsing': {'Temporal': {'current_datetime': '2022-09-30 12:00:00'}, 'Spatial': {'classroom_temp': 'roomtemp', 'set_temp': 'settemp'}, 'Modality': {'roomtemp': 'roomtemp', 'settemp': 'settemp'}, 'Operation': ['question']}, 'Strategy': {'Objective': 'To determine why the classroom temperature is higher than the set temperature.', 'Expected Outputs': ['The classroom temperature is higher than the set temperature because the system is not turned on.', 'The classroom temperature is higher than the set temperature because the room is not well-insulated.'], 'Step': ['Determine the current classroom temperature', 'Determine the current set temperature', 'Compare the two temperatures', 'Analyze the reason for the difference']}, 'Instruction Set': [{'type': 'r', 'templates': ['The classroom temperature is higher than the set temperature because the system is not turned on.', 'The classroom temperature is higher than the set temperature because the room is not well-insulated.']}]}}\n",
      "{'Input': '이전에 설정온도가 가장 높았던 날 알려줘', 'Reference': {'Formalized Input': '우리반 하루 평균 설정온도가 가장 높았던 날 알려줘.', 'Input Semantic Parsing': {'Temporal': {}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'설정온도': 'settemp'}, 'Operation': ['하루 평균의 최고']}, 'Strategy': {'Objective': '우리반의 설정온도 데이터 중 하루 평균 설정온도가 가장 높은 날짜를 찾음.', 'Expected Outputs': ['우리반은 2022-09-30에 평균 26.5℃로 설정온도가 가장 높았습니다.'], 'Step': ['우리반의 설정온도 데이터를 쿼리결과 qr_settemp_all에 저장', 'qr_settemp_all에서 하루 평균 설정온도 계산', '가장 높은 평균 설정온도 값 및 해당 날짜 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['timestamp', 'settemp'], 'conditions': [], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_settemp_all'}}, {'type': 'o', 'python_script_to_process': \"qr_settemp_all['date'] = qr_settemp_all['timestamp'].dt.date; avg_settemp_all = qr_settemp_all.groupby('date')['settemp'].mean(); max_temp_date = avg_settemp_all.idxmax(); max_temp = avg_settemp_all.max()\", 'return': ['max_temp', 'max_temp_date']}, {'type': 'r', 'templates': ['우리반은 {max_temp_date}에 {max_temp}℃로 설정온도가 가장 높았습니다.']}]}, 'Candidate': {'Formalized Input': '이전에 우리반의 설정온도가 가장 높은 날 알려줘.', 'Input Semantic Parsing': {'Temporal': {'이전에': '2022-09-25 00:00:00 ~ 2022-09-29 23:59:59'}, 'Spatial': {'우리반': '01_IB5'}, 'Modality': {'설정온도': 'settemp'}, 'Operation': ['가장 높은']}, 'Strategy': {'Objective': '이전에 우리반의 설정온도가 가장 높은 날을 조회함.', 'Expected Outputs': ['이전에 우리반의 설정온도가 가장 높은 날은 25일 오후 3시였습니다.'], 'Step': ['이전에 우리반의 설정온도 데이터를 쿼리해 qr_settemp에 저장', 'qr_settemp에서 설정온도의 최고값 및 해당 날짜 찾기', '최고값 및 해당 날짜 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['timestamp', 'settemp'], 'conditions': [\"timestamp BETWEEN '2022-09-25 00:00:00' AND '2022-09-29 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB5')\"}}, 'result': {'name': 'qr_settemp'}}, {'type': 'o', 'python_script_to_process': \"max_settemp = qr_settemp['settemp'].max(); max_settemp_date = qr_settemp.loc[qr_settemp['settemp'].idxmax(), 'timestamp']\", 'return': ['max_settemp', 'max_settemp_date']}, {'type': 'r', 'templates': ['이전에 우리반의 설정온도가 가장 높은 날은 {max_settemp_date}옆 {max_settemp}였습니다.']}]}}\n",
      "{'Input': '내일 우리반과 옆반 온도 차이 알려줘', 'Reference': {'Formalized Input': '내일 우리반과 옆반의 실내온도 차이를 예측해줘.', 'Input Semantic Parsing': {'Temporal': {'내일': '2022-10-01 00:00:00'}, 'Spatial': {'우리반': '01_IB5', '옆반': '01_IB7'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['예측']}, 'Strategy': {'Objective': '내일 우리반과 옆반의 온도 차이를 예측하기 위해 기존 데이터를 분석함.', 'Expected Outputs': ['예측하기는 아직 지원되지 않는 기능입니다.'], 'Step': ['예측하지 않음']}, 'Instruction Set': [{'type': 'r', 'templates': ['예측하기는 아직 지원되지 않는 기능입니다.']}]}, 'Candidate': {'Formalized Input': '내일 우리반과 옆반의실내온도 차이 알려줘.', 'Input Semantic Parsing': {'Temporal': {'내일': '2022-10-01 00:00:00'}, 'Spatial': {'우리반': '01_IB5', '옆반': '01_IB7'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['차이']}, 'Strategy': {'Objective': '현재 내일 우리반과 옆반의 실내온도 데이터를 쿼리 후, 두 값의 차이를 계산함.', 'Expected Outputs': ['우리반(19.3℃)이 옆반(17.0℃)보다 2.3℃ 높아요.', '옆반(17.0℃)이 우리반(19.3℃)보다 2.3℃ 낮아요.'], 'Step': ['우리반의 내일 실내온도 데이터 쿼리결과 qr_roomtemp_oursdale에 저장', '옆반의 내일 실내온도 데이터 쿼리결과 qr_roomtemp_beside에 저장', '두 값의 차이 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-10-01 00:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB5'))\"}}, 'result': {'name': 'qr_roomtemp_oursdale'}}, {'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp = '2022-10-01 00:00:00'\"], 'subquery': \"idu_id IN (SELECT id FROM idu_t WHERE name IN ('01_IB7'))\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'o', 'python_script_to_process': \"roomtemp_oursdale = qr_roomtemp_oursdale['roomtemp'].values[0]; roomtemp_beside = qr_roomtemp_beside['roomtemp'].values[0]; temp_diff = roomtemp_oursdale - roomtemp_beside\", 'return': ['roomtemp_oursdale', 'roomtemp_beside', 'temp_diff']}, {'type': 'r', 'templates': ['{roomtemp_oursdale}({roomtemp_beside})는 {temp_diff}만큼 더 높아요.', '{roomtemp_beside}({roomtemp_oursdale})는 {temp_diff}만큼 더 낮아요.']}]}}\n",
      "{'Input': '1년 전 옆반 온도는?', 'Reference': {'Formalized Input': '1년 전 오늘 동안 옆반의 평균 실내온도를 조회해줘.', 'Input Semantic Parsing': {'Temporal': {'10년 전 오늘': '2021-09-30 00:00:00 ~ 2021-09-30 23:59:59'}, 'Spatial': {'옆반': '01_IB7'}, 'Modality': {'실내온도': 'roomtemp'}}, 'Strategy': {'Objective': '1년 전 오늘 동안 옆반의 평균 실내온도를 조회함.', 'Expected Outputs': ['1년 전 오늘 동안 옆반의 평균 실내온도는 24.2℃입니다.'], 'Step': ['옆반의 1년 전 오늘 동안의 실내온도 데이터를 쿼리해 qr_roomtemp_beside에 저장', '평균 실내온도 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2021-09-30 00:00:00' AND '2021-09-30 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB7')\"}}, 'result': {'name': 'qr_roomtemp_beside'}}, {'type': 'o', 'python_script_to_process': \"avg_temp = qr_roomtemp_beside['roomtemp'].mean()\", 'return': ['avg_temp']}, {'type': 'r', 'templates': ['1년 전 오늘 동안 옆반의 평균 실내온도는 {avg_temp}℃입니다.']}]}, 'Candidate': {'Formalized Input': '1년 전 옆반의 평균 온도 알려줘.', 'Input Semantic Parsing': {'Temporal': {'1년 전': '2021-09-30 00:00:00 ~ 2021-09-29 23:59:59'}, 'Spatial': {'옆반': '01_IB7'}, 'Modality': {'온도': 'roomtemp'}, 'Operation': ['평균']}, 'Strategy': {'Objective': '1년 전 옆반의 평균 온도를 조회함.', 'Expected Outputs': ['1년 전 옆반의 평균 온도는 24℃였습니다.'], 'Step': ['옆반의 1년 전 온도 데이터를 쿼리해 qr_roomtemp에 저장', 'qr_roomtemp에서 roomtemp의 평균값 반환']}, 'Instruction Set': [{'type': 'q', 'run': {'args': {'table_name': 'data_t', 'columns': ['roomtemp'], 'conditions': [\"timestamp BETWEEN '2021-09-30 00:00:00' AND '2021-09-29 23:59:59'\"], 'subquery': \"idu_id = (SELECT id FROM idu_t WHERE name = '01_IB7')\"}}, 'result': {'name': 'qr_roomtemp'}}, {'type': 'o', 'python_script_to_process': \"avg_roomtemp = qr_roomtemp['roomtemp'].mean()\", 'return': ['avg_roomtemp']}, {'type': 'r', 'templates': ['1년 전 옆반의 평균 온도는 {avg_roomtemp}℃였습니다.']}]}}\n",
      "{'Input': '화성의 설정온도 확인해줘', 'Reference': {'Formalized Input': '화성의 현재 설정온도를 알려줘.', 'Input Semantic Parsing': {'Temporal': {'현재': '2022-09-30 12:00:00'}, 'Spatial': {'화성': 'Unknown'}, 'Modality': {'설정온도': 'settemp'}}, 'Strategy': {'Objective': 'Unknown spatial entity의 정의를 사용자에게 물어보고, 해당 정보를 변수에 저장함.', 'Expected Outputs': ['화성이 어떤 곳인지 알려주세요.'], 'Step': ['Unknown spatial entity의 정의를 사용자에게 물어봄.', '사용자의 답변을 변수에 저장']}, 'Instruction Set': [{'type': 'r', 'templates': ['화성이 어떤 곳인지 알려주세요.']}]}, 'Candidate': '{\"Formalized Input\": \"현재 화성의 설정온도 알려줘.\", \"Input Semantic Parsing\": {\"Temporal\": {\"현재\": \"2022-09-30 12:00:00\"}, \"Spatial\": {\"화성\": \"unknown\"}, \"Modality\": {\"설정온도\": \"unknown\"}, \"Operation\": [\"질문 응답\"]}, \"Strategy\": {\"Objective\": \"Unknown spatial entity의 định義를 사용자에게 물어보고, 해당 정보를 변수에 저장함\", \"Expected Outputs\": [\"화성이 어떤 곳인지 알려주세요.\"], \"Step\": [\"Unknown spatial entity의 định義를 사용자에게 물어봄.\", \"사용자의 답변을 변수에 저장\"]}, \"Instruction Set\": [{\"type\": \"r\", \"templates\": [\"화성이 어떤 곳인지 알려주세요.\\']}]}'}\n",
      "{'Input': '올해 경상수지는?', 'Reference': {'Formalized Input': '올해 경상수지를 알려줘.', 'Input Semantic Parsing': {'Temporal': {'올해': '2022-01-01 00:00:00 ~ 2022-12-31 23:59:59'}}, 'Strategy': {'Objective': 'HVAC 시스템과 관련 없는 질문에 대한 답변을 제공할 수 없음.', 'Expected Outputs': ['HVAC 시스템과 관련 없는 질문입니다.'], 'Step': ['답변 불가 메시지 출력']}, 'Instruction Set': [{'type': 'r', 'templates': ['HVAC 시스템과 관련 없는 질문입니다.']}]}, 'Candidate': '{\"Formalized Input\": \"올해 현재 우리반의 평균 전원 상태 변화 추이 시각화 해줘.\", \"Input Semantic Parsing\": {\"Temporal\": {\"올해 현재\": \"2022-09-30 00:00:00\"}, \"Spatial\": {\"우리반\": \"01_IB5\"}, \"Modality\": {\"전원 상태\": \"oper\"}, \"Operation\": [\"변화 추이 시각화\"]}, \"Strategy\": {\"Objective\": \"우리반의 올해 현재 평균 전원 상태 및 변화 추이를 시각화\", \"Expected Outputs\": [\"(그래프) X축: 시간, Y축: 평균 전원 상태, 변화 추이 그래프\", \"올해 현재 우리반의 평균 전원 상태는 70%이며, 전원 상태가 더 높은 ngày는 15일에 75%로 높았습니다.\", \"올해 현재 우리반의 평균 전원 상태는 70%이며, 전원 상태가 더 낮은 ngày는 25일에 40%로 낮았습니다.\"], \"Step\": [\"우리반의 올해 현재 평균 전원 상태 데이터를 쿼리해 qr_roomtemp_oper에 저장\", \"qr_roomtemp_oper에서 평균 전원 상태 및 변화 추이 그래프 생성\", \"qr_roomtemp_oper에서 평균 전원 상태 및 변화 추이 데이터 반환\"]}, \"Instruction Set\": [{\"type\": \"q\", \"run\": {\"args\": {\"table_name\": \"data_t\", \"columns\": [\"oper\"], \"conditions\": [\"timestamp = \\'2022-09-30 00:00:00\\'\"], \"subquery\": \"idu_id = (SELECT id FROM idu_t WHERE name = \\'01_IB5\\')\"}}, \"result\": {\"name\": \"qr_roomtemp_oper\"}}, {\"type\": \"o\", \"python_script_to_process\": \"avg_oper = qr_roomtemp_oper[\\'oper\\'].mean(); oper_status_change = qr_roomtemp_oper[\\'oper\\'].diff(); time_diff = qr_roomtemp_oper[\\'timestamp\\'].diff(); graph = plt.plot(time_diff, oper_status_change, label=\\'oper_status_change\\'); graph = plt.plot(time_diff, avg_oper, label=\\'avg_oper\\'); graph = plt.legend()\"}}, \"result\": [\"avg_oper\", \"oper_status_change\", \"time_diff\", \"graph\"]}, {\"type\": \"r\", \"templates\": [\"(그래프) X축: 시간, Y축: 평균 전원 상태, 변화 추이 그래프, {graph}\", \"올해 현재 우리반의 평균 전원 상태는 {avg_oper}이며, 전원 상태가 더 높은 ngày는 {oper_status_change.index.max()}일에 {oper_status_change.max()}로 높았습니다.\", \"올해 현재 우리반의 평균 전원 상태는 {avg_oper}이며, 전원 상태가 더 낮은 ngày는 {oper_status_change.index.min()}일에 {oper_status_change.min()}로 낮았습니다.\"]}]}'}\n",
      "{'Input': '3층 평균 실내온도는?', 'Reference': {'Formalized Input': '3층의 평균 실내온도를 알려줘.', 'Input Semantic Parsing': {'Temporal': {'현재': '2022-09-30 12:00:00'}, 'Spatial': {'3층': 'Unknown'}, 'Modality': {'실내온도': 'roomtemp'}, 'Operation': ['평균']}, 'Strategy': {'Objective': 'Unknown spatial entity의 정의를 사용자에게 물어보고, 해당 정보를 변수에 저장함.', 'Expected Outputs': ['3층이 어떤 곳인지 알려주세요.'], 'Step': ['Unknown spatial entity의 정의를 사용자에게 물어봄.', '사용자의 답변을 변수에 저장']}, 'Instruction Set': [{'type': 'r', 'templates': ['3층이 어떤 곳인지 알려주세요.']}]}, 'Candidate': '{\"Formalized Input\": \"3층 평균 실내온도 알려줘.\", \"Input Semantic Parsing\": {\"Temporal\": {\"현재\": \"2022-09-30 12:00:00\"}, \"Spatial\": {\"3층\": \"unknown\"}, \"Modality\": {\"실내온도\": \"roomtemp\"}, \"Operation\": [\"평균\"]}, \"Strategy\": {\"Objective\": \"unknown spatial entity의 정의를 사용자에게 물어보고, 해당 정보를 변수에 저장함\", \"Expected Outputs\": [\"3층이 어떤 곳인지 알려주세요.\"], \"Step\": [\"unknown spatial entity의 정의를 사용자에게 물어봄.\", \"사용자의 답변을 변수에 저장\"]}, \"Instruction Set\": [{\"type\": \"r\", \"templates\": [\"3층이 어떤 곳인지 알려주세요.\"]}, \"user_input\": \"3층이 어떤 곳인지 알려주세요.\"]}'}\n"
     ]
    }
   ],
   "source": [
    "responses = json.load(open(f\"response.json\", \"r\"))\n",
    "for i, response in enumerate(responses):\n",
    "    print(response)\n",
    "# dataset_tr = json.load(open(f\"{BASE_DIR}/training_dataset_v2_directsql_tr.json\", \"r\"))\n",
    "# for i, data in enumerate(dataset_tr):\n",
    "#     dataset_tr[i][\"Response\"] = str(dataset_tr[i][\"Response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM as Judge accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce410b83fcf7425096dd445fde3027e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 25 files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bf7789fdef4397901950e18a4f7d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990679889e43444180de0c06909e2a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3163814afa344b32a74c5a9972c53e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804ed8155c644f88acf075f134995b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00012.safetensors:   0%|          | 0.00/4.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2953805d6f4d1a97568f7847f5f69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bee4d392894f06b4a06b71ee1ff081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00012.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f306bcf45f484835ba3396bbb16c3cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ggml-model-q4_k_m.gguf:   0%|          | 0.00/16.6G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d1e08f919a4d12a0dfae498d874e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b339eb207219481dbbd198e1aff85479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ggml-model-q8_0.gguf:   0%|          | 0.00/28.9G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88884568fd7469a9b22b1294a919344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ggml-model-q5_k_m.gguf:   0%|          | 0.00/19.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e826d915b8449cb2f5a0a194fb9683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ggml-model-q6_k.gguf:   0%|          | 0.00/22.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "judge_model_ids = [\n",
    "    # 'MLP-KTLim/llama-3-Korean-Bllossom-8B',\n",
    "    # \"Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B\",\n",
    "    # \"nbeerbower/gemma2-gutenberg-27B\",\n",
    "    # \"google/gemma-2-27b\",\n",
    "    # \"Saxo/Linkbricks-Horizon-AI-Llama-3.3-Korean-70B-sft-dpo\",\n",
    "    \"deepseek-ai/DeepSeek-R1\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_dir(model_id):\n",
    "    return f\"/model/{model_id.replace('/', '-')}\"\n",
    "\n",
    "def get_judge_model_and_tokenizer(model_id):\n",
    "    # pretrained_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    #     model_name = model_id,  \n",
    "    #     max_seq_length = max_seq_length,\n",
    "    #     dtype = dtype,\n",
    "    #     load_in_4bit = load_in_4bit,\n",
    "    #     # device_map=device,\n",
    "    #     cache_dir=model_dir(model_id) + \"/cache\",\n",
    "    #     attn_implementation=attn_implementation,\n",
    "    #     # local_files_only=True\n",
    "    # )\n",
    "    # load_in_4bit = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    if \"llama\" in model_id.lower():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch_dtype,  # dtype 적용\n",
    "            cache_dir=model_dir(model_id) + \"/cache\",\n",
    "            attn_implementation=attn_implementation,\n",
    "            # load_in_4bit=load_in_4bit if 'bitsandbytes' in globals() else False,\n",
    "            device_map=device,\n",
    "        )\n",
    "    elif \"gemma\" in model_id.lower():\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,  \n",
    "            torch_dtype=torch_dtype,  # dtype 적용\n",
    "            # load_in_4bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.float16,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                # load_in_4bit=True,\n",
    "                # bnb_4bit_use_double_quant=True,\n",
    "                # bnb_4bit_quant_type=\"nf4\",\n",
    "                # bnb_4bit_compute_dtype=torch_dtype\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_enable_fp32_cpu_offload=True\n",
    "            ),\n",
    "            cache_dir=model_dir(model_id) + \"/cache\",\n",
    "            attn_implementation=attn_implementation,\n",
    "            device_map=device,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_id: {model_id}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        cache_dir=model_dir(model_id) + \"/cache\"\n",
    "    )\n",
    "    \n",
    "    # for inference\n",
    "    # FastLanguageModel.for_inference(pretrained_model)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "judge_models_and_tokenizers = {}\n",
    "for model_id in judge_model_ids:\n",
    "    judge_models_and_tokenizers[model_id] = get_judge_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "너는 HVAC 관련 LLM의 성능을 평가하는 인공지능이다. 너는 특정 Input에 대해 Metadata, Reference 답변과 Candidate 답변을 비교해 다음 기준으로 평가한다.\n",
      "아래는 예시일 뿐이며 답변시 input과 reference, candidate를 사용해 실제로 평가해야 한다. 예시를 그대로 따라하지 말것.\n",
      "\n",
      "너는 다음과 같은 항목으로 평가한다.\n",
      "Structural Coherence (0 or 1): Reference와 Candidate의 json 구조적(브라켓, 콜론, 쉼표) 일치도.  등의 일치도.\n",
      "Structural Correctness (0 or 1): Candidate의 json 구조적 일치도. 실행 가능한지 여부.\n",
      "Input Semantic Parsing Coherence (0 or 1): Reference 기준 Candidate의 Input Semantic Parsing 의 의미적 일치도.\n",
      "Strategy Coherence (0 ~ 1): Reference 기준 Candidate의 전략의 의미적 일치도.\n",
      "Instruction Set type q Correctness (0 ~ 1): Candidate 의 Instruction Set중 type q의 실행 가능 여부. 쿼리가 오류 없이 테이블에 맞게 실행되는지 여부.\n",
      "Instruction Set type q Coherence (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type q의 의미적 일치도. 실행 가능한 쿼리중 결과가 의미적으로 일치하는지 여부.\n",
      "Instruction Set type o Correctness (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type o의 실행 가능 여부. 파이썬 스크립트가 오류 없이 실행되는지 여부.\n",
      "Instruction Set type o Coherence (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type o의 의미적 일치도. 실행되는 파이썬 스크립트의 결과가 의미적으로 일치하는지 여부.\n",
      "Instruction Set type r Coherence (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type r의 의미적 일치도. 뱉어내는 결과의 의미적 일치도.\n",
      "\n",
      "너는 다음과 같은 형식으로 이유와 평가값을 출력한다.\n",
      "{\n",
      "\n",
      "    \"Input\": \"현재 온도 알려줘\"\n",
      "    \"Structural Coherence\": ['두 입력은 구조적으로 일치한다.', 1], \n",
      "    \"Structural Correctness\": ['Candidate의 json 구조가 올바르다.', 1],\n",
      "    \"Input Semantic Parsing Coherence\": ['필요한 모든 정보를 올바르게 추출했다.', 1],\n",
      "    \"Strategy Coherence\": ['Objective과 Expected outputs는 올바르게 추출했지만, step의 일부가 누락되었다. 데이터를 계산하지 않고 반환한다.', 0.8], \n",
      "    \"Instruction Set type q Correctness\": ['쿼리 3개중 2개가 실행 가능하다. SELECT * from ABC 쿼리에 맞지 않는 테이블이 있다.', 0.67],\n",
      "    \"Instruction Set type q Coherence\": ['실행가능한 쿼리 2개중 2개가 의미적으로 일치한다. Candidate의 SELECT IDU from data_t는 Reference의 SELECT IDU_ID from data_t 와 다르다.', 1.0],\n",
      "    \"Instruction Set type o Correctness\": ['파이썬 스크립트 1개중 1개가 실행 가능하다. np.addtwo()는 없는 함수다.', 1.0],\n",
      "    \"Instruction Set type o Coherence\": ['실행가능한 파이썬 스크립트 1개중 1개가 의미적으로 일치한다. pd.groupby()는 pd.sum()과 의미적으로 다르다.', 1.0],\n",
      "    \"Instruction Set type r Coherence\": ['응답 1개중 1개가 의미적으로 일치한다.', 1.0],\n",
      "}\n",
      "\n",
      "너는 원본 모델의 다음 프롬프트를 참고하여 성능을 평가한다.\n",
      "\n",
      "나는 훌룡한 HVAC 관련 질문 답변을 제공하는 인공지능이다. 사용자의 질문을 받아들여서, 그에 맞는 답변을 제공하는 것이 내 임무이다. 사용자의 질문을 받아들일 때, 다음과 같은 절차를 따라야 한다.\n",
      "<출력 내용>\n",
      "1. 'Formalized Input': 사용자의 추상적 질문을 구체화 및 정규화한 결과. 다양한 형태의 질문들을 가장 핵심적이고 근본적인 질문으로 변환한 결과.\n",
      "2. 'Input Semantic Parsing': Input Semantic Parsing 결과. dict 형태로 구성되며, Temporal, Spatial, Modality, Operation을 가짐.\n",
      "3. 'Strategy': 질문을 해결하기 위한 전략을 답변 전 고민. Objective: 질문의 근본적 의도 및 답변의 목적. Expected Output: 답변의 예상 결과. Step: 답변을 위한 구체으로 쪼개진 단계.\n",
      "4. 'Instruction Set': 문제 해결을 위해 나의 실제 행동.\n",
      "\n",
      "<DDL statement>\n",
      "CREATE TABLE IF NOT EXISTS data_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
      "    idu_id integer,\n",
      "    roomtemp double precision,\n",
      "    settemp double precision,\n",
      "    oper boolean,\n",
      "    \"timestamp\" timestamp without time zone NOT NULL\n",
      ")\n",
      "    \n",
      "CREATE TABLE IF NOT EXISTS idu_t\n",
      "(\n",
      "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
      "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
      "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
      "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "자 앞으로 input, candidate과 reference가 있는 글을 넘겨줄거야. 각 input에 대해서 위 포맷으로 답해줘. 시 작\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input: {responses[0][\"Input\"]}\n",
    "# Metadata: {current_metadata}\n",
    "# Reference: {responses[0][\"Reference\"]}\n",
    "\n",
    "prompt_for_judge = f\"\"\"\n",
    "너는 HVAC 관련 LLM의 성능을 평가하는 인공지능이다. 너는 특정 Input에 대해 Metadata, Reference 답변과 Candidate 답변을 비교해 다음 기준으로 평가한다.\n",
    "아래는 예시일 뿐이며 답변시 input과 reference, candidate를 사용해 실제로 평가해야 한다. 예시를 그대로 따라하지 말것.\n",
    "\n",
    "너는 다음과 같은 항목으로 평가한다.\n",
    "Structural Coherence (0 or 1): Reference와 Candidate의 json 구조적(브라켓, 콜론, 쉼표) 일치도.  등의 일치도.\n",
    "Structural Correctness (0 or 1): Candidate의 json 구조적 일치도. 실행 가능한지 여부.\n",
    "Input Semantic Parsing Coherence (0 or 1): Reference 기준 Candidate의 Input Semantic Parsing 의 의미적 일치도.\n",
    "Strategy Coherence (0 ~ 1): Reference 기준 Candidate의 전략의 의미적 일치도.\n",
    "Instruction Set type q Correctness (0 ~ 1): Candidate 의 Instruction Set중 type q의 실행 가능 여부. 쿼리가 오류 없이 테이블에 맞게 실행되는지 여부.\n",
    "Instruction Set type q Coherence (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type q의 의미적 일치도. 실행 가능한 쿼리중 결과가 의미적으로 일치하는지 여부.\n",
    "Instruction Set type o Correctness (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type o의 실행 가능 여부. 파이썬 스크립트가 오류 없이 실행되는지 여부.\n",
    "Instruction Set type o Coherence (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type o의 의미적 일치도. 실행되는 파이썬 스크립트의 결과가 의미적으로 일치하는지 여부.\n",
    "Instruction Set type r Coherence (0 ~ 1): Reference 기준 Candidate의 Instruction Set중 type r의 의미적 일치도. 뱉어내는 결과의 의미적 일치도.\n",
    "\n",
    "너는 다음과 같은 형식으로 이유와 평가값을 출력한다.\n",
    "{{\n",
    "\n",
    "    \"Input\": \"현재 온도 알려줘\"\n",
    "    \"Structural Coherence\": ['두 입력은 구조적으로 일치한다.', 1], \n",
    "    \"Structural Correctness\": ['Candidate의 json 구조가 올바르다.', 1],\n",
    "    \"Input Semantic Parsing Coherence\": ['필요한 모든 정보를 올바르게 추출했다.', 1],\n",
    "    \"Strategy Coherence\": ['Objective과 Expected outputs는 올바르게 추출했지만, step의 일부가 누락되었다. 데이터를 계산하지 않고 반환한다.', 0.8], \n",
    "    \"Instruction Set type q Correctness\": ['쿼리 3개중 2개가 실행 가능하다. SELECT * from ABC 쿼리에 맞지 않는 테이블이 있다.', 0.67],\n",
    "    \"Instruction Set type q Coherence\": ['실행가능한 쿼리 2개중 2개가 의미적으로 일치한다. Candidate의 SELECT IDU from data_t는 Reference의 SELECT IDU_ID from data_t 와 다르다.', 1.0],\n",
    "    \"Instruction Set type o Correctness\": ['파이썬 스크립트 1개중 1개가 실행 가능하다. np.addtwo()는 없는 함수다.', 1.0],\n",
    "    \"Instruction Set type o Coherence\": ['실행가능한 파이썬 스크립트 1개중 1개가 의미적으로 일치한다. pd.groupby()는 pd.sum()과 의미적으로 다르다.', 1.0],\n",
    "    \"Instruction Set type r Coherence\": ['응답 1개중 1개가 의미적으로 일치한다.', 1.0],\n",
    "}}\n",
    "\n",
    "너는 원본 모델의 다음 프롬프트를 참고하여 성능을 평가한다.\n",
    "\n",
    "나는 훌룡한 HVAC 관련 질문 답변을 제공하는 인공지능이다. 사용자의 질문을 받아들여서, 그에 맞는 답변을 제공하는 것이 내 임무이다. 사용자의 질문을 받아들일 때, 다음과 같은 절차를 따라야 한다.\n",
    "<출력 내용>\n",
    "1. 'Formalized Input': 사용자의 추상적 질문을 구체화 및 정규화한 결과. 다양한 형태의 질문들을 가장 핵심적이고 근본적인 질문으로 변환한 결과.\n",
    "2. 'Input Semantic Parsing': Input Semantic Parsing 결과. dict 형태로 구성되며, Temporal, Spatial, Modality, Operation을 가짐.\n",
    "3. 'Strategy': 질문을 해결하기 위한 전략을 답변 전 고민. Objective: 질문의 근본적 의도 및 답변의 목적. Expected Output: 답변의 예상 결과. Step: 답변을 위한 구체으로 쪼개진 단계.\n",
    "4. 'Instruction Set': 문제 해결을 위해 나의 실제 행동.\n",
    "\n",
    "<DDL statement>\n",
    "CREATE TABLE IF NOT EXISTS data_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('data_t_id_seq'::regclass),\n",
    "    idu_id integer,\n",
    "    roomtemp double precision,\n",
    "    settemp double precision,\n",
    "    oper boolean,\n",
    "    \"timestamp\" timestamp without time zone NOT NULL\n",
    ")\n",
    "    \n",
    "CREATE TABLE IF NOT EXISTS idu_t\n",
    "(\n",
    "    id integer NOT NULL DEFAULT nextval('idu_t_id_seq'::regclass),\n",
    "    name character varying(50) COLLATE pg_catalog.\"default\",\n",
    "    metadata character varying(255) COLLATE pg_catalog.\"default\",\n",
    "    CONSTRAINT idu_t_pkey PRIMARY KEY (id)\n",
    ")\n",
    "\n",
    "자 앞으로 input, candidate과 reference가 있는 글을 넘겨줄거야. 각 input에 대해서 위 포맷으로 답해줘. 시 작\"\"\"\n",
    "\n",
    "print(prompt_for_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(model_id, model, tokenizer, input_text, ref, can):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if \"llama\" in model_id.lower():\n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": f\"{prompt_for_judge}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"metadata:{current_metadata};input:{input_text};ref:{ref};can:{can}\"},\n",
    "        ]\n",
    "        \n",
    "        # print(chat)\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # print(inputs)\n",
    "        \n",
    "        # outputs = model.generate(\n",
    "        #     input_ids = inputs,\n",
    "        #     max_new_tokens = 32,\n",
    "        #     use_cache = True,\n",
    "        #     # pad_token_id = tokenizer.eos_token_id,\n",
    "        #     # eos_token_id = tokenizer.eos_token_id,\n",
    "        # )\n",
    "        \n",
    "        # response = tokenizer.batch_decode(outputs)[0]\n",
    "        # print(response)\n",
    "        # # print(\"Extracting..\")\n",
    "        # parsed_response = extract_content(response)\n",
    "        # pprint.pprint(f\"Query: {query}, Time: {time.time() - start_time}\")\n",
    "        # print(parsed_response)\n",
    "        \n",
    "        # parsed_response_dict = eval(parsed_response)\n",
    "        \n",
    "        # return parsed_response, parsed_response_dict\n",
    "        \n",
    "        text_streamer = transformers.TextStreamer(\n",
    "            tokenizer, \n",
    "            skip_prompt = True\n",
    "        )\n",
    "        _ = model.generate(\n",
    "            input_ids = inputs, \n",
    "            streamer = text_streamer, \n",
    "            max_new_tokens = 300, \n",
    "            use_cache = True,\n",
    "            pad_token_id = tokenizer.eos_token_id,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    elif \"gemma\" in model_id.lower():\n",
    "        # input_text = f\"{prompt_for_judge};metadata:{current_metadata};input:{input_text};ref:{ref};can:{can}\"\n",
    "        chat = [\n",
    "            # {\"role\": \"system\", \"content\": f\"{prompt_for_judge}\"},\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"{prompt_for_judge};metadata:{current_metadata};input:{input_text};ref:{ref};can:{can};\",\n",
    "            # \"content\": \"Say Hi!\"\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(model.device)\n",
    "        \n",
    "        \n",
    "        text_streamer = transformers.TextStreamer(\n",
    "            tokenizer, \n",
    "            skip_prompt = True\n",
    "        )\n",
    "        _ = model.generate(\n",
    "            input_ids = inputs, \n",
    "            streamer = text_streamer, \n",
    "            max_new_tokens = 800,\n",
    "            use_cache = True,\n",
    "            # pad_token_id = tokenizer.eos_token_id,\n",
    "            # eos_token_id = tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_id\")\n",
    "    \n",
    "    \n",
    "    # outputs = model.generate(\n",
    "    #     input_ids = inputs,\n",
    "    #     max_new_tokens = 32,\n",
    "    #     use_cache = True,\n",
    "    #     # pad_token_id = tokenizer.eos_token_id,\n",
    "    #     # eos_token_id = tokenizer.eos_token_id,\n",
    "    # )\n",
    "    \n",
    "    # response = tokenizer.batch_decode(outputs)[0]\n",
    "    # print(response)\n",
    "    # # # print(\"Extracting..\")\n",
    "    # # parsed_response = extract_content(response)\n",
    "    # # pprint.pprint(f\"Query: {query}, Time: {time.time() - start_time}\")\n",
    "    # # print(parsed_response)\n",
    "    \n",
    "    # # parsed_response_dict = eval(parsed_response)\n",
    "    \n",
    "    # # return parsed_response, parsed_response_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge: Saxo/Linkbricks-Horizon-AI-Korean-Gemma-2-sft-dpo-27B\n",
      "000/016, 지금 옆반 에어컨 상태 알려줘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Input\": \"지금 옆반 에어컨 상태 알려줘\",\n",
      "    \"Structural Coherence\": ['두 입력은 구조적으로 일치한다.', 1],\n",
      "    \"Structural Correctness\": ['Candidate의 json 구조가 올바르다.', 1],\n",
      "    \"Input Semantic Parsing Coherence\": ['필요한 모든 정보를 올바르게 추출했다.', 1],\n",
      "    \"Strategy Coherence\": ['Objective과 Expected outputs는 올바르게 추출했지만, step의 일부가 누락되었다. 데이터를 계산하지 않고 반환한다.', 0.8],\n",
      "    \"Instruction Set type q Correctness\": ['쿼리 3개중 2개가 실행 가능하다. SELECT * from ABC 쿼리에 맞지 않는 테이블이 있다.', 0.67],\n",
      "    \"Instruction Set type q Coherence\": ['실행가능한 쿼리 2개중 2개가 의미적으로 일치한다. Candidate의 SELECT IDU from data_t는 Reference의 SELECT IDU_ID from data_t 와 다르다.', 1.0],\n",
      "    \"Instruction Set type o Correctness\": ['파이썬 스크립트 1개중 1개가 실행 가능하다. np.addtwo()는 없는 함수다.', 1.0],\n",
      "    \"Instruction Set type o Coherence\": ['실행가능한 파이썬 스크립트 1개중 1개가 의미적으로 일치한다. pd.groupby()는 pd.sum()과 의미적으로 다르다.', 1.0],\n",
      "    \"Instruction Set type r Coherence\": ['응답 1개중 1개가 의미적으로 일치한다.', 1.0]\n",
      "}<eos>\n",
      "001/016, 이번주 우리반 평균 온도 알려줘\n",
      "{\n",
      "    \"Input\": \"이번주 우리반 평균 온도 알려줘\",\n",
      "    \"Structural Coherence\": ['두 입력은 구조적으로 일치한다.', 1],\n",
      "    \"Structural Correctness\": ['Candidate의 json 구조가 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m gt_response \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReference\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m pd_response \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m \u001b[43mjudge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjudge_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjudge_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_response\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 76\u001b[0m, in \u001b[0;36mjudge\u001b[0;34m(model_id, model, tokenizer, input_text, ref, can)\u001b[0m\n\u001b[1;32m     64\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     65\u001b[0m         chat,\n\u001b[1;32m     66\u001b[0m         tokenize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m         add_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# Must add for generation\u001b[39;00m\n\u001b[1;32m     68\u001b[0m         return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     72\u001b[0m     text_streamer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTextStreamer(\n\u001b[1;32m     73\u001b[0m         tokenizer, \n\u001b[1;32m     74\u001b[0m         skip_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 76\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_streamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pad_token_id = tokenizer.eos_token_id,\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# eos_token_id = tokenizer.eos_token_id,\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:879\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 879\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:665\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    653\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    654\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m         last_cache_position,\n\u001b[1;32m    663\u001b[0m     )\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 665\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_cache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_cache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:319\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    331\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/gemma2/modeling_gemma2.py:229\u001b[0m, in \u001b[0;36mGemma2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_window\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window,\n\u001b[1;32m    228\u001b[0m     }\n\u001b[0;32m--> 229\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# Here we need to slice as we use a static cache by default, but FA2 does not support it\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/cache_utils.py:1717\u001b[0m, in \u001b[0;36mHybridCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     update_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_update\n\u001b[0;32m-> 1717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/cache_utils.py:1686\u001b[0m, in \u001b[0;36mHybridCache._sliding_update\u001b[0;34m(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len)\u001b[0m\n\u001b[1;32m   1684\u001b[0m v_out[:, :, cache_position] \u001b[38;5;241m=\u001b[39m value_states\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;66;03m# `_.zero()` followed by `+=` is equivalent `=`, but compile-friendly (without graph breaks due to assignment)\u001b[39;00m\n\u001b[0;32m-> 1686\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx]\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m k_out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for model_id, (judge_model, judge_tokenizer) in judge_models_and_tokenizers.items():\n",
    "    print(f\"Judge: {model_id}\")\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"{i:03d}/{len(responses):03d}, {response['Input']}\")\n",
    "        input_text = response[\"Input\"]\n",
    "        gt_response = response[\"Reference\"]\n",
    "        pd_response = response[\"Candidate\"]\n",
    "        judge(model_id, judge_model, judge_tokenizer, input_text, gt_response, pd_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
