{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "attn_implementation: flash_attention_2, torch_dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "print(f\"attn_implementation: {attn_implementation}, torch_dtype: {torch_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sh2orc-Llama-3.1-Korean-8B-Instruct, Config: v7_r256_a512_ours_16bit_adamw16bit_0322/checkpoint-56\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def read_dataset(train_type, dir, path):\n",
    "    # the file is originally json-list format\n",
    "    # we want every first-level elements to be a string itself\n",
    "    # for example, [{\"Hi\": \"a'b'\"}, {\"Hi\": \"c'd'\"}] -> [\"\"\"{\"Hi\": \"a'b'\"}\"\"\", \"\"\"{\"Hi\": \"c'd'\"}\"\"\"]\n",
    "    \n",
    "    metadata = json.load(open(dir / \"metadata.json\", \"r\"))\n",
    "\n",
    "    path = dir / path\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    result = []\n",
    "    for d in data:\n",
    "        if \"v5\" in dir.name:\n",
    "            if train_type in [\"woall\", \"FI\", \"ISP\"]:\n",
    "                del d[\"Response\"][\"Strategy\"]\n",
    "            \n",
    "            if train_type in [\"woall\", \"FI\"]:\n",
    "                del d[\"Response\"][\"Input Semantic Parsing\"]\n",
    "            \n",
    "            if train_type in [\"woall\"]:\n",
    "                del d[\"Response\"][\"Formalized Input\"]\n",
    "        elif \"v6\" in dir.name:\n",
    "            if train_type in [\"woall\"]:\n",
    "                del d[\"Response\"][\"생각\"]\n",
    "        \n",
    "        tags = d[\"Tags\"][\"Style\"]\n",
    "\n",
    "        skip_tags = [\"Graph\", \"Reason\"]\n",
    "\n",
    "        skip = False\n",
    "        for skip_tag in skip_tags:\n",
    "            if skip_tag in tags:\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        result.append({\"Metadata\": metadata, \"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)})\n",
    "    # result = [{\"Input\": d[\"Input\"], \"Response\": json.dumps(d[\"Response\"], ensure_ascii=False)} for d in data]\n",
    "    # print(f\"Read {len(result)} examples from {path}\")\n",
    "    # print(f\"Type of result: {type(result)}\")\n",
    "    # print(f\"Type of result[0]: {type(result[0])}\")\n",
    "    # print(f\"Type of result[0]['Input']: {type(result[0]['Input'])}\")\n",
    "    # print(f\"Type of result[0]['Response']: {type(result[0]['Response'])}\")\n",
    "    return result\n",
    "\n",
    "# Configuration\n",
    "dataset_name = \"v6-250306-optimizetoken\"\n",
    "dataset_name = \"v7-250309-reduceinputanddatefunctioncall\"\n",
    "BASE_DIR = Path(f\"../finetuning/dataset/{dataset_name}\")\n",
    "\n",
    "\n",
    "train_type = [\n",
    "    \"woall\", # 0\n",
    "    \"FI\", # 1\n",
    "    \"ISP\", # 2\n",
    "    \"ours\" # 3\n",
    "][3]\n",
    "\n",
    "if train_type == \"woall\":\n",
    "    model_name, tr_config = \\\n",
    "        \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "        \"r128_a256_woall/checkpoint-60\"\n",
    "    \n",
    "    model_name, tr_config = \\\n",
    "        \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "        \"v5_r64_a128_woall/checkpoint-72\"\n",
    "\n",
    "    model_name, tr_config = \\\n",
    "        \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "        \"v5_r32_a64_woall/checkpoint-70\"\n",
    "    \n",
    "    model_name, tr_config = \\\n",
    "        \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "        \"v7_r256_a512_woall_16bit_adamw16bit_0322/checkpoint-60\"\n",
    "\n",
    "elif train_type == \"FI\":\n",
    "    # model_name, tr_config = \\\n",
    "    #     \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "    #     f\"r256_a512_FI/checkpoint-57\"\n",
    "    model_name, tr_config = \\\n",
    "        \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "        f\"v5_r256_a512_FI/checkpoint-43\"\n",
    "    pass\n",
    "\n",
    "elif train_type == \"ISP\":\n",
    "    # model_name, tr_config = \\\n",
    "    #     \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "    #     f\"r256_a512_ISP/checkpoint-104\"\n",
    "    pass\n",
    "\n",
    "elif train_type == \"ours\":\n",
    "    # model_name, tr_config = \\\n",
    "    #     \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "    #     \"r128_a256_ours/checkpoint-51\"\n",
    "    \n",
    "    model_name, tr_config = \\\n",
    "        \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "        \"v7_r256_a512_ours_4bit_0322/checkpoint-37\"\n",
    "    \n",
    "    model_name, tr_config = \\\n",
    "        \"Bllossom-llama-3-Korean-Bllossom-70B\", \\\n",
    "        \"v7_r32_a64_ours_4bit_0322/checkpoint-67\"\n",
    "\n",
    "    model_name, tr_config = \\\n",
    "        \"Bllossom-llama-3-Korean-Bllossom-70B\", \\\n",
    "        \"v7_r64_a128_ours_4bit_0322/checkpoint-38\"\n",
    "\n",
    "    model_name, tr_config = \\\n",
    "        \"sh2orc-Llama-3.1-Korean-8B-Instruct\", \\\n",
    "        \"v7_r256_a512_ours_16bit_adamw16bit_0322/checkpoint-56\"\n",
    "print(f\"Model: {model_name}, Config: {tr_config}\")\n",
    "\n",
    "model_dir = Path(f\"/model/{model_name}\")\n",
    "checkpoint_dir = Path(f\"{model_dir}/chkpts/{tr_config}\")\n",
    "cache_dir = Path(f\"{model_dir}/cache\")\n",
    "\n",
    "# Verify paths exist\n",
    "if not checkpoint_dir.exists():\n",
    "    raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "if not BASE_DIR.exists():\n",
    "    raise ValueError(f\"Base directory {BASE_DIR} does not exist\")\n",
    "\n",
    "dataset = []\n",
    "for scenario_dir in [d for d in BASE_DIR.iterdir() if d.is_dir() and \"scenario\" in d.name and \"metadata.json\" in [f.name for f in d.iterdir()]]:\n",
    "    data = read_dataset(train_type, scenario_dir, \"onlyq_ts.json\")\n",
    "    for i, d in enumerate(data):\n",
    "        data[i][\"Scenario\"] = scenario_dir.name\n",
    "    dataset.extend(data)\n",
    "    \n",
    "\n",
    "common_prompt = open(BASE_DIR / F\"prompt.txt\", \"r\").read()\n",
    "\n",
    "if \"v5\" in BASE_DIR.name:\n",
    "    if train_type in [\"woall\", \"FI\", \"ISP\"]:\n",
    "        # search <|ST|>~~<|ST|> and remove between them\n",
    "        common_prompt = re.sub(r\"\\n?<\\|ST\\|>(.|\\n)*?<\\|ST\\|>\", \"\", common_prompt)\n",
    "    if train_type in [\"woall\", \"FI\"]:\n",
    "        # search <|ISP|>~~<|ISP|> and remove between them\n",
    "        common_prompt = re.sub(r\"\\n?<\\|ISP\\|>(.|\\n)*?<\\|ISP\\|>\", \"\", common_prompt)\n",
    "    if train_type in [\"woall\"]:\n",
    "        # search <|FI|>~~<|FI|> and remove between them\n",
    "        common_prompt = re.sub(r\"\\n?<\\|FI\\|>(.|\\n)*?<\\|FI\\|>\", \"\", common_prompt)\n",
    "\n",
    "elif \"v6\" in BASE_DIR.name or \"v7\" in BASE_DIR.name:\n",
    "    if train_type in [\"woall\"]:\n",
    "        # search <|FI|>~~<|FI|> and remove between them\n",
    "        common_prompt = re.sub(r\"\\n?<\\|Ours\\|>(.|\\n)*?<\\|Ours\\|>\", \"\", common_prompt)\n",
    "\n",
    "# remove all <||>\n",
    "common_prompt = re.sub(r\"<\\|.*?\\|>\", \"\", common_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(text: str) -> str:\n",
    "    \"\"\"Extract content from model output.\"\"\"\n",
    "    if \"start_header_id\" in text:\n",
    "        pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\"\n",
    "    elif \"start_of_turn\" in text:\n",
    "        pattern = r\"<start_of_turn>model\\n(.*?)<eos>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnslothInference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        checkpoint_dir: str,\n",
    "        cache_dir: str,\n",
    "        max_new_tokens: int = 3500,\n",
    "        attn_implementation: str = attn_implementation,\n",
    "    ):\n",
    "        \n",
    "\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.attn_implementation = attn_implementation\n",
    "\n",
    "        # Verify model files exist\n",
    "        if not self.checkpoint_dir.exists():\n",
    "            raise ValueError(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "        # if not (self.checkpoint_dir / \"config.json\").exists():\n",
    "        #     raise ValueError(f\"config.json not found in {checkpoint_dir}\")\n",
    "        \n",
    "        # Set torch dtype based on GPU capability\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.model, self.tokenizer = self.setup_model()\n",
    "\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize model and tokenizer for the given rank.\"\"\"\n",
    "        from unsloth import FastLanguageModel\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            self.checkpoint_dir.as_posix(),\n",
    "            dtype = self.torch_dtype,\n",
    "            load_in_4bit = False,\n",
    "            load_in_8bit = False,\n",
    "            # quantization_config=BitsAndBytesConfig(\n",
    "            #     load_in_4bit=True,\n",
    "            #     bnb_4bit_use_double_quant=True,\n",
    "            #     bnb_4bit_quant_type=\"nf4\",\n",
    "            #     bnb_4bit_compute_dtype=torch_dtype,\n",
    "            #     # load_in_8bit=True,\n",
    "            #     # llm_int8_enable_fp32_cpu_offload=True\n",
    "            # ),\n",
    "            attn_implementation=self.attn_implementation,\n",
    "            cache_dir=self.cache_dir.as_posix(),\n",
    "            local_files_only=True,\n",
    "            device_map=\"cuda\",\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model)\n",
    "    \n",
    "        tokenizer.padding_side = \"left\"\n",
    "        return model, tokenizer\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        data: Dict,\n",
    "        common_prompt: str,\n",
    "    ) -> str:\n",
    "        try:\n",
    "            \n",
    "            model, tokenizer = self.model, self.tokenizer\n",
    "            start_time = time.time()\n",
    "            metadata, input = data[\"Metadata\"], data[\"Input\"]\n",
    "            if \"llama\" in model.config.architectures[0].lower():\n",
    "                chat = [\n",
    "                    {\"role\": \"system\", \"content\": common_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Metadata:{metadata};Input:{input};\"},\n",
    "                ]\n",
    "            elif \"gemma\" in model.config.architectures[0].lower():\n",
    "                chat = [\n",
    "                    {\"role\": \"user\", \"content\": f\"{common_prompt};{json.dumps(metadata)};{input}\"},\n",
    "                ]\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model architecture: {model.config.architectures[0]}\")\n",
    "            \n",
    "            chat = tokenizer.apply_chat_template(\n",
    "                chat,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "\n",
    "            # 배치 추론 실행\n",
    "            outputs = model.generate(\n",
    "                input_ids=chat,\n",
    "                # attention_mask=attention_mask,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=False  # 결정론적 생성\n",
    "            )\n",
    "            \n",
    "            # 결과 디코딩 및 파싱\n",
    "            response = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "            parsed = extract_content(response)\n",
    "\n",
    "            if parsed is None:\n",
    "                raise ValueError(f\"Failed to extract content from response: {response}\")\n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"Elapsed time: {end_time - start_time:.2f}s\")\n",
    "            return parsed\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in infer: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "        \n",
    "    def run(\n",
    "        self,\n",
    "        dataset: List[Dict],\n",
    "        common_prompt: str\n",
    "    ):\n",
    "        \"\"\"Run inference in batches.\"\"\"\n",
    "            \n",
    "        # Setup model and tokenizer\n",
    "        \n",
    "        \n",
    "        # 토크나이저에 패딩 토큰 설정\n",
    "        # if tokenizer.pad_token is None:\n",
    "        #     tokenizer.pad_token = tokenizer.eos_token\n",
    "        # print(tokenizer.pad_token, tokenizer.eos_token)\n",
    "        with tqdm(total=len(dataset)) as pbar:\n",
    "            for data_idx in range(len(dataset)):\n",
    "                data = dataset[data_idx]          \n",
    "                \n",
    "                # 배치 처리\n",
    "                response = self.infer(\n",
    "                    data, common_prompt\n",
    "                )\n",
    "                \n",
    "                print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLamaInference(UnslothInference):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gguf_path: str,\n",
    "        binary_path: str = \"/workspace/llama-cpp/build/bin/main\",\n",
    "        max_new_tokens: int = 100,\n",
    "        temperature: float = 0.0,\n",
    "        top_p: float = 1.0,\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instead of loading a model via Hugging Face, this version sets up llama.cpp inference.\n",
    "        It assumes that the GGUF model file is named 'model.gguf' and is located inside checkpoint_dir.\n",
    "        \"\"\"\n",
    "        self.gguf_path = gguf_path\n",
    "        self.binary_path = binary_path\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.seed = seed\n",
    "\n",
    "        # Assume the GGUF model file is named \"model.gguf\" inside checkpoint_dir\n",
    "        if not os.path.exists(self.gguf_path):\n",
    "            raise ValueError(f\"GGUF model file {self.model_path} does not exist\")\n",
    "\n",
    "    def infer(self, data: str, common_prompt:str) -> str:\n",
    "        \"\"\"\n",
    "        Run inference by invoking the llama.cpp binary.\n",
    "        Constructs a command line with the given prompt and generation parameters,\n",
    "        and returns the generated text.\n",
    "        \"\"\"\n",
    "        user_input = f\"Metadata:{data['Metadata']};Input:{data['Input']};\"\n",
    "\n",
    "        # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "        command = [\n",
    "            str(self.binary_path),\n",
    "            \"-m\", str(self.gguf_path),\n",
    "            # \"-sys\", str(common_prompt),\n",
    "            \"--system-prompt-file\", \"prompt.temp\",\n",
    "            # \"-p\", str(user_input),\n",
    "            # \"--chat-template\", \"llama3\",\n",
    "\n",
    "            \"-n\", str(self.max_new_tokens),\n",
    "            \"-c\", str(len(common_prompt) + len(user_input)+100),\n",
    "            \"--threads\", str(os.cpu_count()),\n",
    "            \"-ngl\", str(33),\n",
    "            # \"-no-cnv\",\n",
    "            # \"-st\",\n",
    "            \"--temp\", str(self.temperature),\n",
    "            \"--top_p\", str(self.top_p),\n",
    "            # \"--no-warmup\",\n",
    "            \"--seed\", str(self.seed)\n",
    "        ]\n",
    "        # Run the command and capture stdout\n",
    "        def read_output(process):\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                print(line, end='')  # 명령어의 출력 실시간 표시\n",
    "            process.stdout.close()\n",
    "\n",
    "        print(\" \".join(command))\n",
    "        process = subprocess.Popen(\n",
    "            command,\n",
    "            shell=True,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1\n",
    "        )\n",
    "\n",
    "        # 출력을 실시간으로 읽는 스레드\n",
    "        thread = threading.Thread(target=read_output, args=(process,))\n",
    "        thread.start()\n",
    "\n",
    "        # 사용자의 입력을 받아서 명령어에 전달\n",
    "        try:\n",
    "            while process.poll() is None:  # 프로세스가 살아있는 동안\n",
    "                process.stdin.flush()  # 즉시 전송\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted by user\")\n",
    "        finally:\n",
    "            process.stdin.close()\n",
    "            process.wait()\n",
    "            thread.join()\n",
    "\n",
    "    # def run(\n",
    "    #     self,\n",
    "    #     dataset: List[Dict],\n",
    "    #     common_prompt: str, \n",
    "    #     output_file: str\n",
    "    # ):\n",
    "    #     # 배치 처리\n",
    "    #     with tqdm(total=len(dataset)) as pbar:\n",
    "    #         for data in dataset:\n",
    "\n",
    "    #             response = self.infer(common_prompt, data)\n",
    "                    \n",
    "    #             if response is not None:\n",
    "    #                 try:\n",
    "    #                     response = eval(response)\n",
    "    #                 except Exception as e:\n",
    "    #                     print(f\"Error in eval: {str(e)}\")\n",
    "                    \n",
    "    #                 result = {\n",
    "    #                     \"Input\": data[\"Input\"],\n",
    "    #                     \"Scenario\": data[\"Scenario\"],\n",
    "    #                     \"Metadata\": data[\"Metadata\"],\n",
    "    #                     \"Candidate\": response,\n",
    "    #                 }\n",
    "                    \n",
    "    #                 with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    #                     f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "    #             else:\n",
    "    #                 print(f\"Error in response for sample {data}\")\n",
    "            \n",
    "    #         pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_type = [\n",
    "    \"unsloth\", # 0\n",
    "    \"llama\" # 1\n",
    "][1]\n",
    "\n",
    "if inference_type == \"unsloth\":\n",
    "    inference = UnslothInference(\n",
    "        checkpoint_dir=str(checkpoint_dir),\n",
    "        cache_dir=str(cache_dir),\n",
    "        max_new_tokens = 3500,\n",
    "    )\n",
    "else:\n",
    "    gguf_path = Path(f\"../src/i2i.gguf\")\n",
    "    if not gguf_path.exists():\n",
    "        raise ValueError(f\"GGUF model file {gguf_path} does not exist\")\n",
    "    \n",
    "    with open(\"prompt.temp\", \"w\") as f:\n",
    "        f.write(common_prompt)\n",
    "    \n",
    "    inference = LLamaInference(\n",
    "        gguf_path=gguf_path,\n",
    "        binary_path = \"../llama.cpp/build/bin/llama-cli\",\n",
    "        max_new_tokens = 500,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../llama.cpp/build/bin/llama-cli -m ../src/i2i.gguf --system-prompt-file prompt.temp -p Metadata:{'site_name': 'YongDongIllHighSchool', 'user_name': '홍길동', 'user_role': 'customer', 'idu_name': '01_IB5', 'idu_mapping': {'01_IB5': ['우리반'], '01_IB7': ['옆반'], '02_I81': ['앞반']}, 'modality_mapping': {'roomtemp': ['실내온도'], 'settemp': ['설정온도'], 'oper': ['전원']}, 'current_datetime': '2022-09-30 12:00:00'};Input:오늘 아침과 저녁의 온도차이는 얼마나 돼?; -n 500 -c 786 --threads 32 -ngl 33 -st --temp 0.0 --top_p 1.0 --seed 42\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 4 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\n",
      "  Device 1: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\n",
      "  Device 2: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\n",
      "  Device 3: NVIDIA RTX A6000, compute capability 8.6, VMM: yes\n",
      "build: 4943 (18b663d8) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA RTX A6000) - 48280 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA1 (NVIDIA RTX A6000) - 48280 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA2 (NVIDIA RTX A6000) - 48280 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA3 (NVIDIA RTX A6000) - 48280 MiB free\n",
      "gguf_init_from_file: failed to open GGUF file 'models/7B/ggml-model-f16.gguf'\n",
      "llama_model_load: error loading model: llama_model_loader: failed to load model from models/7B/ggml-model-f16.gguf\n",
      "\n",
      "llama_model_load_from_file_impl: failed to load model\n",
      "common_init_from_params: failed to load model 'models/7B/ggml-model-f16.gguf'\n",
      "main: error: unable to load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference.run(\n",
    "    dataset=dataset[:1],\n",
    "    common_prompt=common_prompt\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
